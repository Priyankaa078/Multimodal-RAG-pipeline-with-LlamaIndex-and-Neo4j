{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4005fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from llama_index.multi_modal_llms.gemini import GeminiMultiModal \n",
    "from llama_index.embeddings.gemini import GeminiEmbedding \n",
    "from llama_index.llms.gemini import Gemini  \n",
    "import tiktoken\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9a126a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/3473sp494_18260h13nl3c5h0000gn/T/ipykernel_98501/106996373.py:4: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  embed_model = GeminiEmbedding(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# Set up Google embedding\n",
    "embed_model = GeminiEmbedding(\n",
    "    model_name=\"models/embedding-001\", \n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Configure LlamaIndex to use Google embeddings\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"\"\n",
    "NEO4J_USERNAME = \"\"\n",
    "NEO4J_PASSWORD = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84803a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, Document\n",
    "from llama_index.core.schema import ImageDocument\n",
    "from llama_index.core.node_parser import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02b51117",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/priyanka./Documents/Multimodal RAG pipeline with LlamaIndex and Neo4j/articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5e1631e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    # Find the required section\n",
    "    content_section = soup.find(\"section\", {\"data-field\": \"body\", \"class\": \"e-content\"})\n",
    "\n",
    "    if not content_section:\n",
    "        return \"Section not found.\"\n",
    "\n",
    "    sections = []\n",
    "    current_section = {\"header\": \"\", \"content\": \"\", \"source\": file_path.split(\"/\")[-1]}\n",
    "    images = []\n",
    "    header_found = False\n",
    "\n",
    "    for element in content_section.find_all(recursive=True):\n",
    "        if element.name in [\"h1\", \"h2\", \"h3\", \"h4\"]:\n",
    "            if header_found and (current_section[\"content\"].strip()):\n",
    "                sections.append(current_section)\n",
    "            current_section = {\n",
    "                \"header\": element.get_text(),\n",
    "                \"content\": \"\",\n",
    "                \"source\": file_path.split(\"/\")[-1],\n",
    "            }\n",
    "            header_found = True\n",
    "        elif header_found:\n",
    "            if element.name == \"pre\":\n",
    "                current_section[\"content\"] += f\"```{element.get_text().strip()}```\\n\"\n",
    "            elif element.name == \"img\":\n",
    "                img_src = element.get(\"src\")\n",
    "                img_caption = element.find_next(\"figcaption\")\n",
    "                caption_text = img_caption.get_text().strip() if img_caption else \"\"\n",
    "                images.append(ImageDocument(image_url=img_src))\n",
    "            elif element.name in [\"p\", \"span\", \"a\"]:\n",
    "                current_section[\"content\"] += element.get_text().strip() + \"\\n\"\n",
    "\n",
    "    if current_section[\"content\"].strip():\n",
    "        sections.append(current_section)\n",
    "\n",
    "    return images, sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "70a10606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text document count: 252\n",
      "Image document count: 328\n"
     ]
    }
   ],
   "source": [
    "all_documents = []\n",
    "all_images = []\n",
    "\n",
    "# Directory to search in (current working directory)\n",
    "directory = os.getcwd()\n",
    "\n",
    "# Walking through the directory\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".html\"):\n",
    "            # Update the file path to be relative to the current directory\n",
    "            images, documents = process_html_file(os.path.join(root, file))\n",
    "            all_documents.extend(documents)\n",
    "            all_images.extend(images)\n",
    "\n",
    "text_docs = [Document(text=el.pop(\"content\"), metadata=el) for el in all_documents]\n",
    "print(f\"Text document count: {len(text_docs)}\") \n",
    "print(f\"Image document count: {len(all_images)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8d1466a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/3473sp494_18260h13nl3c5h0000gn/T/ipykernel_99147/3668649631.py:2: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  gemini_llm = GeminiMultiModal(\n",
      "/var/folders/bs/3473sp494_18260h13nl3c5h0000gn/T/ipykernel_99147/3668649631.py:8: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  gemini_embedding = GeminiEmbedding(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google Gemini models\n",
    "gemini_llm = GeminiMultiModal(\n",
    "    model_name=\"models/gemini-pro-vision\", \n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Initialize Gemini embedding model\n",
    "gemini_embedding = GeminiEmbedding(\n",
    "    model_name=\"models/embedding-001\",\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5b4bebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 326 text nodes and 328 image nodes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "node_parser = SimpleNodeParser.from_defaults()\n",
    "\n",
    "# Parse text documents into nodes\n",
    "text_nodes = node_parser.get_nodes_from_documents(text_docs)\n",
    "image_nodes = node_parser.get_nodes_from_documents(all_images)\n",
    "\n",
    "print(f\"Created {len(text_nodes)} text nodes and {len(image_nodes)} image nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cd831faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.neo4jvector import Neo4jVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "26c8d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_store = Neo4jVectorStore(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=\"text_collection\",\n",
    "    node_label=\"Chunk\",\n",
    "    embedding_dimension=768, \n",
    "    embed_model=gemini_embedding\n",
    ")\n",
    "image_store = Neo4jVectorStore(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=\"image_collection\",\n",
    "    node_label=\"Image\",\n",
    "    embedding_dimension=768,  \n",
    "    embed_model=gemini_embedding\n",
    "\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=text_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "89a51fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.multi_modal import MultiModalVectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ba522",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    text_docs + all_images, \n",
    "    storage_context=storage_context, \n",
    "    image_vector_store=image_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed870445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.multi_modal_llms.gemini import GeminiMultiModal\n",
    "\n",
    "Settings.llm = GooglePaLM(api_key=\"your_google_api_key\")\n",
    "Settings.embed_model = GoogleUniversalSentenceEncoderEmbedding()\n",
    "\n",
    "# Configure ALL components to use Google (to avoid OpenAI defaults)\n",
    "Settings.llm = Gemini(model=\"gemini-pro-vision\")\n",
    "Settings.embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
    "\n",
    "# Use Gemini for multimodal\n",
    "gemini_mm_llm = GeminiMultiModal(\n",
    "    model=\"gemini-1.5-pro\",  # Latest model that supports multimodal\n",
    "    max_new_tokens=1500\n",
    ")\n",
    "\n",
    "qa_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "qa_tmpl = PromptTemplate(qa_tmpl_str)\n",
    "\n",
    "# Create query engine\n",
    "from llama_index.core.query_engine import SimpleMultiModalQueryEngine\n",
    "\n",
    "query_engine = SimpleMultiModalQueryEngine(\n",
    "    retriever=index.as_retriever(),\n",
    "    multi_modal_llm=gemini_mm_llm,\n",
    "    text_qa_template=qa_tmpl\n",
    ")\n",
    "\n",
    "query_str = \"How do vector RAG application work?\"\n",
    "response = query_engine.query(query_str)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(image_urls):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(25, 15))\n",
    "    for img_url in image_urls:\n",
    "        try:\n",
    "            response = requests.get(img_url)\n",
    "            response.raise_for_status()  # Raise an error for bad status codes\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "\n",
    "            plt.subplot(1, 3, images_shown + 1)  # Layout adjusted for 3 images\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 4:  # Break after displaying 3 images\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_url}: {e}\")\n",
    "\n",
    "plot_images([n.node.image_url for n in response.metadata[\"image_nodes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7998e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
