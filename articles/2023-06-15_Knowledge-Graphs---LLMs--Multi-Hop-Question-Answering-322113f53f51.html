<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering</h1>
</header>
<section data-field="subtitle" class="p-summary">
Retrieve information that spans across multiple documents
</section>
<section data-field="body" class="e-content">
<section name="31de" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="3d28" id="3d28" class="graf graf--h3 graf--leading graf--title">Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering</h3><h4 name="0ef2" id="0ef2" class="graf graf--h4 graf-after--h3 graf--subtitle">Retrieve information that spans across multiple documents</h4><p name="a93f" id="a93f" class="graf graf--p graf-after--h4"><em class="markup--em markup--p-em">This is the third blog post of Neo4j’s NaLLM project. We started this project to explore, develop, and showcase practical uses of these </em><a href="https://neo4j.com/generativeai/?utm_campaign=gen-ai&amp;utm_content=medium&amp;utm_source=blog" data-href="https://neo4j.com/generativeai/?utm_campaign=gen-ai&amp;utm_content=medium&amp;utm_source=blog" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">LLMs in conjunction with Neo4j</em></a><em class="markup--em markup--p-em">. As part of this project, we will construct and publicly display demonstrations in a </em><a href="https://github.com/neo4j/NaLLM" data-href="https://github.com/neo4j/NaLLM" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">GitHub repository</em></a><em class="markup--em markup--p-em">, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in blog posts. You can see the previous two blog posts here:</em></p><ul class="postList"><li name="58bc" id="58bc" class="graf graf--li graf-after--p"><a href="https://medium.com/neo4j/harnessing-large-language-models-with-neo4j-306ccbdd2867" data-href="https://medium.com/neo4j/harnessing-large-language-models-with-neo4j-306ccbdd2867" class="markup--anchor markup--li-anchor" target="_blank">Harnessing LLMs With Neo4j</a></li><li name="891d" id="891d" class="graf graf--li graf-after--li"><a href="https://medium.com/neo4j/knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation-30e875d63a35" data-href="https://medium.com/neo4j/knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation-30e875d63a35" class="markup--anchor markup--li-anchor" target="_blank">Fine-Tuning vs Retrieval-Augmented Generation</a></li></ul><figure name="f333" id="f333" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*LXd72ildC0F_xKNQkXqv1w.png" data-width="1024" data-height="1024" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*LXd72ildC0F_xKNQkXqv1w.png"><figcaption class="imageCaption">Midjourney’s idea of an investigative board.</figcaption></figure><p name="091c" id="091c" class="graf graf--p graf-after--figure">In the<a href="https://medium.com/neo4j/knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation-30e875d63a35" data-href="https://medium.com/neo4j/knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation-30e875d63a35" class="markup--anchor markup--p-anchor" target="_blank"> previous blog post</a>, we learned about the retrieval-augmented approach to overcome the limitations of Large Language Models (LLMs), such as hallucinations and limited knowledge. The idea behind the retrieval-augmented approach is to reference external data at question time and feed it to an LLM to enhance its ability to generate accurate and relevant answers.</p><figure name="197a" id="197a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zydD2GKzjpEyvL-d_cP0vA.png" data-width="1196" data-height="597" src="https://cdn-images-1.medium.com/max/800/1*zydD2GKzjpEyvL-d_cP0vA.png"><figcaption class="imageCaption">Retrieval-augmented approach to LLM applications. Image by author.</figcaption></figure><p name="b7da" id="b7da" class="graf graf--p graf-after--figure">When a user asks a question, an intelligent search tool looks for relevant information in the provided Knowledge bases. For example, you might have encountered instances of searching for relevant information within PDFs or a company’s documentation. Most of those examples use vector similarity search to identify which chunks of text might contain relevant data to answer the user’s question accurately. The implementation is relatively straightforward.</p><figure name="4cc8" id="4cc8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*oMLZ5s8OHftzqPEVreTd_g.png" data-width="1063" data-height="647" src="https://cdn-images-1.medium.com/max/800/1*oMLZ5s8OHftzqPEVreTd_g.png"><figcaption class="imageCaption">RAG applications using vector similarity search. Image by author.</figcaption></figure><p name="77ec" id="77ec" class="graf graf--p graf-after--figure">The PDFs or the documentation are first split into multiple chunks of text. Some different strategies include how large the text chunks should be and if there should be any overlap between them. In the next step, vector representations of text chunks are generated by using any of the available text embedding models. That is all the preprocessing needed to perform a vector similarity search at query time. The only step left is to encode the user input as a vector at query time and use cosine or any other similarity to compare the distance between the user input and the embedded text chunks. Most frequently, you will see that the top three most similar documents are returned to provide the context to the LLM to enhance its capability to generate accurate answers. This approach works fairly well when the vector search can produce relevant chunks of text.</p><p name="b1e9" id="b1e9" class="graf graf--p graf-after--p">However, simple vector similarity search might not be sufficient when the LLM needs information from multiple documents or even just multiple chunks to generate an answer.</p><p name="28e7" id="28e7" class="graf graf--p graf-after--p">For example, consider the following question:</p><p name="16a1" id="16a1" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Did any of the former OpenAI employees start their own company?</em></p><p name="72aa" id="72aa" class="graf graf--p graf-after--p">If you think about it, this question can be broken down into two questions.</p><ul class="postList"><li name="eb11" id="eb11" class="graf graf--li graf-after--p">Who are the former employees of OpenAI?</li><li name="96e6" id="96e6" class="graf graf--li graf-after--li">Did any of them start their own company?</li></ul><figure name="9458" id="9458" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*rFlThhafYxQ-Q2z2rfJyQA.png" data-width="446" data-height="396" src="https://cdn-images-1.medium.com/max/800/1*rFlThhafYxQ-Q2z2rfJyQA.png"><figcaption class="imageCaption">Information spanning across multiple documents. Image by author.</figcaption></figure><p name="990f" id="990f" class="graf graf--p graf-after--figure">Answering these types of questions is a <strong class="markup--strong markup--p-strong">multi-hop question-answering task</strong>, where a single question can be broken down into multiple sub-questions and can require numerous documents to be provided to the LLM to generate an accurate answer.</p><p name="5d02" id="5d02" class="graf graf--p graf-after--p">The above-mentioned workflow of simply chunking and embeddings documents in a database and then using plain vector similarity search might struggle with multi-hop questions due to:</p><ul class="postList"><li name="981b" id="981b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Repeated information in top N documents</strong>: The provided documents are not guaranteed to contain complementary and complete information needed to answer a question. For example, the top three similar documents might all mention that <em class="markup--em markup--li-em">Shariq</em> worked at <em class="markup--em markup--li-em">OpenAI</em> and possibly founded a company while completely ignoring all the other former employees that became founders</li><li name="dc5a" id="dc5a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Missing reference information: </strong>Depending on the chunk sizes, you might lose the reference to the entities in the documents. This can be partially solved by chunk overlaps. However, there are also examples where the references point to another document, so some sort of co-reference resolution or other preprocessing would be needed.</li><li name="6868" id="6868" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Hard to define ideal N number of retrieved documents</strong>: Some questions require more documents to be provided to an LLM to accurately answer the question, while in other situations, a large number of provided documents would only increase the noise (and cost).</li></ul><figure name="006c" id="006c" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*vrvcRRrsVrtAK8L_zye5Rg.png" data-width="905" data-height="565" src="https://cdn-images-1.medium.com/max/800/1*vrvcRRrsVrtAK8L_zye5Rg.png"><figcaption class="imageCaption">An example where the similarity search might return some duplicated information, while other relevant information could be ignored due to a low K number of retrieved information or embedding distance. Image by the author.</figcaption></figure><p name="8e56" id="8e56" class="graf graf--p graf-after--figure">Therefore, a plain vector similarity search might struggle with multi-hop questions. However, we can employ multiple strategies to attempt to answer multi-hop questions requiring information from various documents.</p><h4 name="0b07" id="0b07" class="graf graf--h4 graf-after--p">Knowledge Graph as Condensed Information Storage</h4><p name="c4a9" id="c4a9" class="graf graf--p graf-after--h4">If you are paying close attention to the LLM space, you might have come across the idea of using various techniques to condense information for it to be more easily accessible during query time. For example, you <a href="https://medium.com/google-cloud/how-to-use-llms-to-generate-concise-summaries-of-text-a04966659ed" data-href="https://medium.com/google-cloud/how-to-use-llms-to-generate-concise-summaries-of-text-a04966659ed" class="markup--anchor markup--p-anchor" target="_blank">could use an LLM to provide a summary of documents</a> and then embed and store the summaries instead of the actual documents. Using this approach, you could remove a lot of noise, get better results, and worry less about prompt token space.</p><p name="ad90" id="ad90" class="graf graf--p graf-after--p">Interestingly, you could conduct the contextual summarization at ingestion or <a href="https://blog.langchain.dev/improving-document-retrieval-with-contextual-compression/" data-href="https://blog.langchain.dev/improving-document-retrieval-with-contextual-compression/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">perform it during the query time</a>. Contextual compression during query time is interesting as the context is picked that is relevant to the provided question, so it is a bit more guided. However, the heavier the workload during the query time, the worse the expected user latency will be. Therefore, it is recommended to move as much of the workload to ingestion time as possible to improve latency and avoid other runtime issues.</p><p name="c5fb" id="c5fb" class="graf graf--p graf-after--p">The same approach can be applied to <a href="https://python.langchain.com/en/latest/modules/memory/types/summary_buffer.html" data-href="https://python.langchain.com/en/latest/modules/memory/types/summary_buffer.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">summarize conversation history</a> to avoid running into <em class="markup--em markup--p-em">token limit problems</em>.</p><p name="fabc" id="fabc" class="graf graf--p graf-after--p">I haven’t seen any articles about combining and summarizing multiple documents as a single record. The problem is probably that there are too many combinations of documents that we could merge and summarize. Therefore, it is perhaps <em class="markup--em markup--p-em">too costly to process all the combinations</em> of documents at ingestion time. <br>However, a knowledge graph can help here too.</p><p name="d03b" id="d03b" class="graf graf--p graf-after--p">The process of extracting structured information in the form of entities and relationships from unstructured text has been around for some time and is better known as <a href="https://towardsdatascience.com/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754" data-href="https://towardsdatascience.com/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">the information extraction pipeline</a>. The beauty of combining an information extraction pipeline with knowledge graphs is that you can process each document individually, and the information from different records gets connected when the knowledge graph is constructed or enriched.</p><figure name="ad55" id="ad55" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*N-TVTbRffy_VQ0DPcx0JKg.png" data-width="879" data-height="681" src="https://cdn-images-1.medium.com/max/800/1*N-TVTbRffy_VQ0DPcx0JKg.png"><figcaption class="imageCaption">Extracting entities and relationships from text to construct a knowledge graph. Image by author.</figcaption></figure><p name="574b" id="574b" class="graf graf--p graf-after--figure">The knowledge graph used nodes and relationships to represent data. In this example, the first document provided the information that <em class="markup--em markup--p-em">Dario</em> and <em class="markup--em markup--p-em">Daniela</em> used to work at <em class="markup--em markup--p-em">OpenAI</em>, while the second document offered information about their <em class="markup--em markup--p-em">Anthropic</em> startup. Each record was processed individually, yet <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">the knowledge graph representation connects the data</em></strong> and makes it easy to answer questions spanning across multiple documents.</p><p name="8389" id="8389" class="graf graf--p graf-after--p">Most of the newer approaches using LLMs to answer multi-hop questions we encountered focus on solving the task at query time. However, we believe that many multi-hop question-answering issues can be solved by preprocessing data before ingestion and connecting it in a knowledge graph. The information extraction pipeline can be <a href="https://medium.com/neo4j/creating-a-knowledge-graph-from-video-transcripts-with-gpt-4-52d7c7b9f32c" data-href="https://medium.com/neo4j/creating-a-knowledge-graph-from-video-transcripts-with-gpt-4-52d7c7b9f32c" class="markup--anchor markup--p-anchor" target="_blank">performed using LLMs</a> or <a href="https://medium.com/neo4j/monitoring-the-cryptocurrency-space-with-nlp-and-knowledge-graphs-92a1cfaebd1a" data-href="https://medium.com/neo4j/monitoring-the-cryptocurrency-space-with-nlp-and-knowledge-graphs-92a1cfaebd1a" class="markup--anchor markup--p-anchor" target="_blank">custom text domain models</a>.</p><p name="98d8" id="98d8" class="graf graf--p graf-after--p">In order to retrieve information from the knowledge graph at query time, we have to construct an appropriate Cypher statement. Luckily, LLMs are pretty good at translating natural language to Cypher graph-query language.</p><figure name="0d4a" id="0d4a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mkYvs8_TmzLhUUI1CShNfw.png" data-width="1195" data-height="597" src="https://cdn-images-1.medium.com/max/800/1*mkYvs8_TmzLhUUI1CShNfw.png"><figcaption class="imageCaption">Using knowledge graphs as part of retrieval-augmented LLM applications. Image by author.</figcaption></figure><p name="5f00" id="5f00" class="graf graf--p graf-after--figure">In this example, the smart search uses an LLM to generate an appropriate Cypher statement to retrieve relevant information from a knowledge graph. The relevant information is then passed to another LLM call, which uses the original question and the provided information to generate an answer. In practice, you could use different LLMs for generating Cypher statements and answers or use various prompts on a single LLM.</p><h3 name="f461" id="f461" class="graf graf--h3 graf-after--p">Combining Graph and Textual Data</h3><p name="af6e" id="af6e" class="graf graf--p graf-after--h3">Sometimes, you might want to combine textual and graph data to find relevant information. For example, consider the following question:</p><p name="b382" id="b382" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">What is the latest news about Prosper Robotics founders?</em></p><p name="4263" id="4263" class="graf graf--p graf-after--p">In this example, you might want to identify the Prosper Robotics founders using the knowledge graph structure and retrieve the latest articles mentioning them.</p><figure name="77db" id="77db" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*J9LkK_WuH5z00hLJi97_Hw.png" data-width="665" data-height="721" src="https://cdn-images-1.medium.com/max/800/1*J9LkK_WuH5z00hLJi97_Hw.png"><figcaption class="imageCaption">Knowledge graph with explicit links between structured information and unstructured text. Image by author.</figcaption></figure><p name="6928" id="6928" class="graf graf--p graf-after--figure">To answer the question about the latest news about Prosper Robotics founders, you would start from the Prosper Robotics node, traverse to its founders, and then retrieve the latest articles mentioning them.</p><p name="4960" id="4960" class="graf graf--p graf-after--p">A knowledge graph can be used to represent structured information about entities and their relationships, as well as unstructured text as node properties. Additionally, you could employ natural language techniques like named entity recognition to connect unstructured information to relevant entities in the knowledge graph, as shown with the <strong class="markup--strong markup--p-strong">MENTIONS</strong> relationship.</p><p name="ffd3" id="ffd3" class="graf graf--p graf-after--p">We believe that the future of retrieval-augmented generation applications is utilizing both structured and unstructured information to generate accurate answers. Therefore, a knowledge graph is a perfect solution because you can store both structured and unstructured data and connect them with explicit relationships, making information more accessible and easier to find.</p><figure name="0ab4" id="0ab4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AH05dvGA_7db_EMySc9AAw.png" data-width="1195" data-height="607" src="https://cdn-images-1.medium.com/max/800/1*AH05dvGA_7db_EMySc9AAw.png"><figcaption class="imageCaption">Using Cypher and vector similarity search to retrieve relevant information from a knowledge graph. Image by author.</figcaption></figure><p name="4ead" id="4ead" class="graf graf--p graf-after--figure">When the knowledge graph contains structured and unstructured data, the smart search tool could utilize Cypher queries or vector similarity search to retrieve relevant information. In some cases, you could also use a combination of the two. For example, you could start with a Cypher query to identify relevant documents and then use vector similarity search to find specific information within those documents.</p><h3 name="0e1b" id="0e1b" class="graf graf--h3 graf-after--p">Using Knowledge Graphs in Chain-of-Thought Flow</h3><p name="946e" id="946e" class="graf graf--p graf-after--h3">Another very exciting development around LLMs is the so-called <a href="https://cobusgreyling.medium.com/chain-of-thought-prompting-llm-reasoning-147a6cdb312b" data-href="https://cobusgreyling.medium.com/chain-of-thought-prompting-llm-reasoning-147a6cdb312b" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">chain-of-thought question answering</a>, especially with <a href="https://python.langchain.com/en/latest/modules/agents.html" data-href="https://python.langchain.com/en/latest/modules/agents.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LLM agents</a>. The idea behind LLM agents is that they can decompose questions into multiple steps, define a plan, and use any of the provided tools. In most cases, the agent tools are APIs or knowledge bases that the agent can access to retrieve additional information. Let’s again consider the following question:</p><p name="b768" id="b768" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">What is the latest news about Prosper Robotics founders?</em></p><figure name="7f12" id="7f12" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*xPSKLXVQUyoOhzv1AYDszA.png" data-width="649" data-height="704" src="https://cdn-images-1.medium.com/max/800/1*xPSKLXVQUyoOhzv1AYDszA.png"><figcaption class="imageCaption">No explicit links between knowledge graph entities and unstructured text. Image by author.</figcaption></figure><p name="a861" id="a861" class="graf graf--p graf-after--figure">Suppose you don’t have explicit connections between articles and entities they mention. The articles and entities could even be in separate databases. In this case, an LLM agent using chain-of-thought flow would be very helpful. First, the agent would decompose the question into sub-questions.</p><ul class="postList"><li name="176d" id="176d" class="graf graf--li graf-after--p">Who are the founders of Prosper Robotics?</li><li name="b6f2" id="b6f2" class="graf graf--li graf-after--li">What is the latest news about them?</li></ul><p name="2e2d" id="2e2d" class="graf graf--p graf-after--li">Now, an agent could decide which tool to use. Suppose we provide it with a knowledge graph access that it can use to retrieve structured information. Therefore, an agent could choose to retrieve the information about the founders of Prosper Robotics from a knowledge graph. As we already know, the founder of Prosper Robotics is Shariq Hashme. Now that the first question was answered, the agent could rewrite the second subquestion as:</p><ul class="postList"><li name="5d80" id="5d80" class="graf graf--li graf-after--p">What is the latest news about Shariq Hashme?</li></ul><p name="d6dd" id="d6dd" class="graf graf--p graf-after--li">The agent could use any of the available tools to answer the subsequent question. The tools can range from knowledge graphs, document or vector databases, various APIs, and more. Having access to structured information allows LLM applications to perform various analytics workflows where aggregation, filtering, or sorting is required. Consider the following questions:</p><ul class="postList"><li name="8fac" id="8fac" class="graf graf--li graf-after--p">Which company with a solo founder has the highest valuation?</li><li name="b25c" id="b25c" class="graf graf--li graf-after--li">Who founded the most companies?</li></ul><p name="6e6a" id="6e6a" class="graf graf--p graf-after--li">Plain vector similarity search can struggle with these types of analytical questions since it searches through unstructured text data, making it hard to sort or aggregate data. Therefore, a combination of structured and unstructured data is probably the future of retrieval-augmented LLM applications. Additionally, as we have seen, knowledge graphs are also ideal for representing connected information and, consequently, multi-hop queries.</p><p name="aa1f" id="aa1f" class="graf graf--p graf-after--p">While the chain-of-thought is a fascinating development around LLMs as it shows how an LLM can reason, it is not the most user-friendly as the response latency can be high due to multiple LLM calls. However, we are still very excited to understand more about incorporating knowledge graphs into chain-of-thought flows for various use cases.</p><h3 name="de37" id="de37" class="graf graf--h3 graf-after--p">Summary</h3><p name="0201" id="0201" class="graf graf--p graf-after--h3">Retrieval-augmented generation applications often require retrieving information from multiple sources to generate accurate answers. While textual summarization can be challenging, representing information in a graph format can offer several advantages.</p><p name="03ab" id="03ab" class="graf graf--p graf-after--p">By processing each document separately and connecting them in a knowledge graph, we can construct a structured representation of the information. This approach allows for easier traversal and navigation through interconnected documents, enabling multi-hop reasoning to answer complex queries. Furthermore, constructing the knowledge graph during the ingestion phase reduces the workload during query time, resulting in improved latency.</p><p name="919b" id="919b" class="graf graf--p graf-after--p">Another advantage of using a knowledge graph is its ability to store both structured and unstructured information. This flexibility makes a <a href="https://neo4j.com/generativeai/?utm_campaign=gen-ai&amp;utm_content=medium&amp;utm_source=blog" data-href="https://neo4j.com/generativeai/?utm_campaign=gen-ai&amp;utm_content=medium&amp;utm_source=blog" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">knowledge graphs suitable</a> for a wide range of language model (LLM) applications, as it can handle various data types and relationships between entities. The graph structure provides a visual representation of the knowledge, facilitating transparency and interpretability for both developers and users.</p><p name="3639" id="3639" class="graf graf--p graf-after--p">Overall, leveraging knowledge graphs in retrieval-augmented generation applications offers benefits such as improved query efficiency, multi-hop reasoning capabilities, and support for structured and unstructured information.</p><p name="37d7" id="37d7" class="graf graf--p graf-after--p">Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our <a href="https://github.com/neo4j/NaLLM" data-href="https://github.com/neo4j/NaLLM" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repository</a>.</p><div name="3987" id="3987" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/neo4j/NaLLM" data-href="https://github.com/neo4j/NaLLM" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/neo4j/NaLLM"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - neo4j/NaLLM: Repository for the NaLLM project</strong><br><em class="markup--em markup--mixtapeEmbed-em">Repository for the NaLLM project. Contribute to neo4j/NaLLM development by creating an account on GitHub.</em>github.com</a><a href="https://github.com/neo4j/NaLLM" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="3989afc1af776637bf832a322583c7bd" data-thumbnail-img-id="0*GMzTUNgPZXFKO-lo" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*GMzTUNgPZXFKO-lo);"></a></div><p name="6d2c" id="6d2c" class="graf graf--p graf-after--mixtapeEmbed graf--trailing"><em class="markup--em markup--p-em">/ The NaLLM Project Core Team at Neo4j: Jon Harris, Noah Mayerhofer, </em><a href="https://medium.com/u/e347f94807a8" data-href="https://medium.com/u/e347f94807a8" data-anchor-type="2" data-user-id="e347f94807a8" data-action-value="e347f94807a8" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank"><em class="markup--em markup--p-em">Oskar Hane</em></a>, <a href="https://medium.com/u/57f13c0ea39a" data-href="https://medium.com/u/57f13c0ea39a" data-anchor-type="2" data-user-id="57f13c0ea39a" data-action-value="57f13c0ea39a" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Tomaz Bratanic</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/322113f53f51"><time class="dt-published" datetime="2023-06-15T06:39:50.451Z">June 15, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/knowledge-graphs-llms-multi-hop-question-answering-322113f53f51" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>