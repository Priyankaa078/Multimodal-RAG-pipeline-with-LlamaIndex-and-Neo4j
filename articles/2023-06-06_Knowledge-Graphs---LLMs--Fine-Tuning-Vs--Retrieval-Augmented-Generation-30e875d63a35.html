<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Knowledge Graphs &amp; LLMs: Fine-Tuning Vs. Retrieval-Augmented Generation</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Knowledge Graphs &amp; LLMs: Fine-Tuning Vs. Retrieval-Augmented Generation</h1>
</header>
<section data-field="subtitle" class="p-summary">
What are the limitations of LLMs, and how to overcome them
</section>
<section data-field="body" class="e-content">
<section name="debf" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f653" id="f653" class="graf graf--h3 graf--leading graf--title">Knowledge Graphs &amp; LLMs: Fine-Tuning Vs. Retrieval-Augmented Generation</h3><h4 name="27f5" id="27f5" class="graf graf--h4 graf-after--h3 graf--subtitle">What are the limitations of LLMs, and how to overcome them</h4><figure name="38fe" id="38fe" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*gMyohWeGauh6BZvbqsXAgw.png" data-width="1024" data-height="1024" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*gMyohWeGauh6BZvbqsXAgw.png"><figcaption class="imageCaption">Midjourney’s idea of a knowledge graph chatbot.</figcaption></figure><p name="f13a" id="f13a" class="graf graf--p graf-after--figure"><em class="markup--em markup--p-em">This is the second blog post of Neo4j’s NaLLM project. We started this project to explore, develop, and showcase </em><a href="https://neo4j.com/generativeai/?utm_campaign=gen-ai&amp;utm_content=medium&amp;utm_source=blog" data-href="https://neo4j.com/generativeai/?utm_campaign=gen-ai&amp;utm_content=medium&amp;utm_source=blog" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">practical uses of these LLMs in conjunction with Neo4j</em></a><em class="markup--em markup--p-em">. As part of this project, we will construct and publicly display demonstrations in a </em><a href="https://github.com/neo4j/NaLLM" data-href="https://github.com/neo4j/NaLLM" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener noopener noopener noopener" target="_blank"><em class="markup--em markup--p-em">GitHub repository</em></a><em class="markup--em markup--p-em">, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can find the first and third blog posts here:</em></p><ul class="postList"><li name="8baf" id="8baf" class="graf graf--li graf-after--p"><a href="https://medium.com/neo4j/harnessing-large-language-models-with-neo4j-306ccbdd2867" data-href="https://medium.com/neo4j/harnessing-large-language-models-with-neo4j-306ccbdd2867" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Harnessing LLMs With Neo4j</a></li><li name="f039" id="f039" class="graf graf--li graf-after--li"><a href="https://medium.com/neo4j/knowledge-graphs-llms-multi-hop-question-answering-322113f53f51" data-href="https://medium.com/neo4j/knowledge-graphs-llms-multi-hop-question-answering-322113f53f51" class="markup--anchor markup--li-anchor" target="_blank">Multi-Hop Question Answering</a></li></ul><div name="65e2" id="65e2" class="graf graf--mixtapeEmbed graf-after--li"><a href="https://medium.com/neo4j/harnessing-large-language-models-with-neo4j-306ccbdd2867" data-href="https://medium.com/neo4j/harnessing-large-language-models-with-neo4j-306ccbdd2867" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/neo4j/harnessing-large-language-models-with-neo4j-306ccbdd2867"><strong class="markup--strong markup--mixtapeEmbed-strong">Harnessing Large Language Models with Neo4j</strong><br><em class="markup--em markup--mixtapeEmbed-em">Episode 1 — Exploring Real-World Use Cases</em>medium.com</a><a href="https://medium.com/neo4j/harnessing-large-language-models-with-neo4j-306ccbdd2867" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="f5f4e698e120afbc54bc6f81420afaaf" data-thumbnail-img-id="1*c_LYTNDI8UKMLwUt9ASI8g.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*c_LYTNDI8UKMLwUt9ASI8g.png);"></a></div><p name="0b13" id="0b13" class="graf graf--p graf-after--mixtapeEmbed">The first wave of hype for Large Language Models (LLMs) came from ChatGPT and similar web-based chatbots, where the models are so good at understanding and generating text that it shocked people, myself included.</p><p name="a6d4" id="a6d4" class="graf graf--p graf-after--p">Many of us logged in and tested its ability to write haikus, motivational letters, or email responses. What became quickly apparent is that LLMs are not only good at generating creative context but also at solving typical natural language processing and other tasks.</p><p name="b222" id="b222" class="graf graf--p graf-after--p">Shortly after the LLM hype started, people started considering integrating it into their applications. Unfortunately, if you simply develop a wrapper around an LLM API, there is a high chance your application will not be successful as it doesn’t provide additional value.</p><p name="fd72" id="fd72" class="graf graf--p graf-after--p">One major problem of LLMs is the so-called <strong class="markup--strong markup--p-strong">knowledge cutoff</strong>. The knowledge cutoff term indicates that LLMs are <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">unaware of any events that happened after their training</em></strong>. For example, if you ask ChatGPT about an event in 2023, you will get the following response.</p><figure name="4275" id="4275" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*W3w9G6BIkk8nAxN-xzS6sg.png" data-width="789" data-height="288" src="https://cdn-images-1.medium.com/max/800/1*W3w9G6BIkk8nAxN-xzS6sg.png"><figcaption class="imageCaption">ChatGPT’s knowledge cutoff date. Image by author.</figcaption></figure><p name="0ea2" id="0ea2" class="graf graf--p graf-after--figure">The same problem will occur if you ask an LLM about <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">any event not present</em></strong> in its training dataset. While the knowledge cutoff date is relevant for any publicly available information, the LLM doesn’t have any knowledge about <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">private or confidential information</em></strong> that might be available even before the knowledge cutoff date.</p><p name="6b66" id="6b66" class="graf graf--p graf-after--p">For example, most companies have some confidential information that they don’t share publicly but might be interested in having a custom LLM that could answer those questions. On the other hand, a lot of the publicly available information that the LLM is aware of might be already outdated.</p><blockquote name="f117" id="f117" class="graf graf--blockquote graf-after--p">Therefore, updating and expanding the knowledge of an LLM is highly relevant today.</blockquote><p name="197e" id="197e" class="graf graf--p graf-after--blockquote">Another problem with LLMs is that they are trained to produce realistic-sounding text, which <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">might not be accurate</em></strong>. Some invalid information is more challenging to spot than others. Especially for missing data, it is very probable that the LLM will make up an answer that sounds convincing but is nonetheless wrong instead of admitting that it lacks the base facts in its training.</p><p name="9e49" id="9e49" class="graf graf--p graf-after--p">For example, research or court citations might be easier to verify. A week ago, a <a href="https://edition.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html" data-href="https://edition.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">lawyer got in trouble for blindly believing the court citations ChatGPT produced</a>.</p><div name="c88b" id="c88b" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://edition.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html" data-href="https://edition.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://edition.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html"><strong class="markup--strong markup--mixtapeEmbed-strong">Lawyer apologizes for fake court citations from ChatGPT | CNN Business</strong><br><em class="markup--em markup--mixtapeEmbed-em">The meteoric rise of ChatGPT is shaking up multiple industries - including law. A lawyer for a man suing Avianca…</em>edition.cnn.com</a><a href="https://edition.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="bd28b228773a74c9cd5f446a16ab46f6" data-thumbnail-img-id="0*mQ2wodtDRqxSBwZG" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*mQ2wodtDRqxSBwZG);"></a></div><p name="e407" id="e407" class="graf graf--p graf-after--mixtapeEmbed">I have also noticed that LLMs will consistently <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">produce assertive, yet false information about any sort of IDs</em></strong> like the WikiData or other identification numbers.</p><figure name="a6da" id="a6da" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4--0hKMFWhDlWfVuIS_uMQ.jpeg" data-width="627" data-height="352" src="https://cdn-images-1.medium.com/max/800/1*4--0hKMFWhDlWfVuIS_uMQ.jpeg"><figcaption class="imageCaption">ChatGPT’s hallucinations. Image by author.</figcaption></figure><p name="6ae0" id="6ae0" class="graf graf--p graf-after--figure">Since the response by ChatGPT is assertive, you might expect it to be accurate. However, the given WikiData id points to a farm in England. Therefore, you have to be very careful not to blindly believe everything that LLMs produce. Verifying answers or producing more accurate results from LLMs is another big problem that needs to be solved.</p><p name="286e" id="286e" class="graf graf--p graf-after--p">Of course, LLMs have other problems, like bias, prompt injection, and others. However, we will not talk about them here. Instead, in this blog post, we will present and focus on the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">concepts of fine-tuning and retrieval-augmented LLMs</em></strong> and evaluate their pros and cons.</p><h3 name="6e13" id="6e13" class="graf graf--h3 graf-after--p">Supervised Fine-Tuning of an LLM</h3><p name="d234" id="d234" class="graf graf--p graf-after--h3">Explaining how LLMs are trained is beyond the scope of this blog post. Instead, you can watch this <a href="https://www.youtube.com/watch?v=bZQun8Y4L2A" data-href="https://www.youtube.com/watch?v=bZQun8Y4L2A" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">incredible video by Andrej Karpathy to catch up on LLMs</a> and learn about the different phases of LLM training.</p><figure name="e9d3" id="e9d3" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/bZQun8Y4L2A?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><blockquote name="739d" id="739d" class="graf graf--blockquote graf-after--figure">By fine-tuning an LLM, we refer to the <strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">supervised training phase</em></strong>, during which you provide additional question-answer pairs to optimize the performance of the Large Language Model (LLM).</blockquote><p name="2392" id="2392" class="graf graf--p graf-after--blockquote">Additionally, we have identified two different use cases for fine-tuning an LLM.</p><p name="7559" id="7559" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">One use case</strong> is fine-tuning a model to update and <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">expand its internal knowledge</em></strong>. <br>In contrast, the <strong class="markup--strong markup--p-strong">other use case</strong> is focused on fine-tuning a model <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">for a specific</em></strong> task like text summarization or translating natural language to database queries.</p><p name="0796" id="0796" class="graf graf--p graf-after--p">First, we will talk about the first use case, where we use fine-tuning techniques to update and expand the internal knowledge of an LLM.</p><figure name="08cb" id="08cb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LTfNOqZQB_M42KyvttjSyw.png" data-width="951" data-height="479" src="https://cdn-images-1.medium.com/max/800/1*LTfNOqZQB_M42KyvttjSyw.png"><figcaption class="imageCaption">Supervised fine-tuning flow. Image by author. Icons from <a href="https://www.flaticon.com/" data-href="https://www.flaticon.com/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Flaticons</a>.</figcaption></figure><p name="3db1" id="3db1" class="graf graf--p graf-after--figure">Usually, you want to avoid pre-training an LLM as the <strong class="markup--strong markup--p-strong">cost</strong> can be upwards of hundreds of thousands and even millions of dollars. A base LLM is pre-trained using a gigantic corpus of text corpus, frequently in the billions or even trillions of tokens.</p><p name="889c" id="889c" class="graf graf--p graf-after--p">While the <strong class="markup--strong markup--p-strong">number of parameters</strong> of an LLM is vital, it is not the only parameter you should consider when selecting a base LLM. Besides the <strong class="markup--strong markup--p-strong">license</strong>, you should also consider the <strong class="markup--strong markup--p-strong">bias</strong> and <strong class="markup--strong markup--p-strong">toxicity</strong> of the pre-training dataset and the base LLM.</p><p name="8646" id="8646" class="graf graf--p graf-after--p">After you have selected the base LLM, you can start the next step of fine-tuning it. The fine-tuning step is relatively cheap regarding computation cost due to available techniques like the <a href="https://huggingface.co/blog/lora" data-href="https://huggingface.co/blog/lora" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LoRa</a> and <a href="https://arxiv.org/abs/2305.14314" data-href="https://arxiv.org/abs/2305.14314" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">QLoRA</a>.</p><div name="5ea5" id="5ea5" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://huggingface.co/blog/lora" data-href="https://huggingface.co/blog/lora" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://huggingface.co/blog/lora"><strong class="markup--strong markup--mixtapeEmbed-strong">Using LoRA for Efficient Stable Diffusion Fine-Tuning</strong><br><em class="markup--em markup--mixtapeEmbed-em">LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal…</em>huggingface.co</a><a href="https://huggingface.co/blog/lora" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="74effa9b80f63fc599b40377e0d32d5a" data-thumbnail-img-id="0*ns8drxID-sn8siKA" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*ns8drxID-sn8siKA);"></a></div><p name="8c77" id="8c77" class="graf graf--p graf-after--mixtapeEmbed">However, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">constructing a training dataset</em></strong> is more complex and can get expensive. If you can not afford a dedicated team of annotators, it seems that the trend is to <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">use an LLM to construct a training dataset</em></strong> to fine-tune your desired LLM (this is really meta).</p><p name="fe18" id="fe18" class="graf graf--p graf-after--p">For example,<a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" data-href="https://crfm.stanford.edu/2023/03/13/alpaca.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> Stanford’s Alpaca training dataset was created using OpenAI’s LLMs</a>. The cost to produce 52 thousand training instructions was about 500 dollars, which is relatively cheap.</p><div name="a2e7" id="a2e7" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" data-href="https://crfm.stanford.edu/2023/03/13/alpaca.html" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://crfm.stanford.edu/2023/03/13/alpaca.html"><strong class="markup--strong markup--mixtapeEmbed-strong">Stanford CRFM</strong><br><em class="markup--em markup--mixtapeEmbed-em">We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our…</em>crfm.stanford.edu</a><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="a40e76cb68c6777bc815d513a9b9c9e6" data-thumbnail-img-id="0*91cfT7OSoor-2UIt" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*91cfT7OSoor-2UIt);"></a></div><p name="cfd4" id="cfd4" class="graf graf--p graf-after--mixtapeEmbed">On the other hand, the <a href="https://lmsys.org/blog/2023-03-30-vicuna/" data-href="https://lmsys.org/blog/2023-03-30-vicuna/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Vicuna model was fine-tuned by using the ChatGPT conversations users posted on ShareGPT.com</a>.</p><div name="4982" id="4982" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://lmsys.org/blog/2023-03-30-vicuna/" data-href="https://lmsys.org/blog/2023-03-30-vicuna/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://lmsys.org/blog/2023-03-30-vicuna/"><strong class="markup--strong markup--mixtapeEmbed-strong">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality | LMSYS Org</strong><br><em class="markup--em markup--mixtapeEmbed-em">by: The Vicuna Team, Mar 30, 2023 We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on…</em>lmsys.org</a><a href="https://lmsys.org/blog/2023-03-30-vicuna/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="e6a04e77b90913736d4f992759fe8977" data-thumbnail-img-id="0*M66LPHV-AyYcHXph" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*M66LPHV-AyYcHXph);"></a></div><p name="e916" id="e916" class="graf graf--p graf-after--mixtapeEmbed">There is also a relatively <a href="https://github.com/h2oai/h2o-wizardlm" data-href="https://github.com/h2oai/h2o-wizardlm" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">fresh project by H2O called WizardLM</a>, which is designed to turn documents into question-answer pairs that can be used to fine-tune an LLM.</p><p name="80e3" id="80e3" class="graf graf--p graf-after--p">We haven’t found any recent articles describing how to use a knowledge graph to prepare good question-answer pairs that can be used to fine-tune an LLM.</p><p name="b03a" id="b03a" class="graf graf--p graf-after--p">This is an area that we plan to explore during the NaLLM project. We have some ideas for utilizing LLMs to construct question-answer pairs from a knowledge graph context.</p><p name="aeb3" id="aeb3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">However, there are a lot of unknowns at the moment.</strong><br>For example, can you provide <em class="markup--em markup--p-em">two different answers</em> to the same question, and the LLM then somehow combines them in its internal knowledge store?</p><p name="c8e9" id="c8e9" class="graf graf--p graf-after--p">Another consideration is that some information in a knowledge graph is not relevant without considering its relationships. Therefore, do we have to pre-define relevant queries, or is there a more generic way to go about it? Or can we use the node-relationship-node patterns representing subject-predicate-object expressions to generate relevant pairs?</p><p name="b36f" id="b36f" class="graf graf--p graf-after--p">These are some of the questions we aim to answer in upcoming blog posts.</p><p name="6e67" id="6e67" class="graf graf--p graf-after--p">Imagine that you somehow managed to produce a training dataset containing question-answer pairs based on the information stored in your knowledge graph. As a result, the LLM now includes updated knowledge.</p><p name="c34c" id="c34c" class="graf graf--p graf-after--p">However, fine-tuning the model didn’t solve the knowledge cutoffs problem since it only pushed the knowledge cutoff to a later date.</p><p name="67af" id="67af" class="graf graf--p graf-after--p">Therefore, we recommend updating the internal knowledge of an LLM through fine-tuning techniques <em class="markup--em markup--p-em">only for slowly changing </em>or updating data. For example, you could use a fine-tuned model to provide tourist information.</p><p name="21ee" id="21ee" class="graf graf--p graf-after--p">However, you would run into troubles the second you would want to include special time-dependent (real-time) or personalized promotions in the responses. Similarly, fine-tuned models are not ideal for analytical workflows where you would ask how many new customers the company gained over the last week.</p><p name="3fef" id="3fef" class="graf graf--p graf-after--p">At the moment, <em class="markup--em markup--p-em">fine-tuning approaches can help mitigate hallucinations</em> but cannot completely eliminate them. One problem is that the LLMs <em class="markup--em markup--p-em">do not cite their sources</em> when providing answers. Therefore, you have no idea if the answer came from pre-training data, fine-tuning dataset, or was made up by the LLM. Additionally, there might be another possible falsehood source if you use an LLM to create the fine-tuning dataset.</p><p name="3e1d" id="3e1d" class="graf graf--p graf-after--p">Lastly, a fine-tuned model cannot automatically provide different responses <em class="markup--em markup--p-em">depending on the user</em> making the questions. Likewise, there is no concept of access restrictions, meaning that anybody interacting with the LLM has access to all of its information.</p><h3 name="9461" id="9461" class="graf graf--h3 graf-after--p">Retrieval-Augmented Generation</h3><p name="562b" id="562b" class="graf graf--p graf-after--h3">Large language models perform remarkably well in natural language applications like</p><ul class="postList"><li name="f078" id="f078" class="graf graf--li graf-after--p">Text summarization,</li><li name="fa73" id="fa73" class="graf graf--li graf-after--li">Extracting relevant information,</li><li name="0de2" id="0de2" class="graf graf--li graf-after--li">Disambiguation of entities</li><li name="f96d" id="f96d" class="graf graf--li graf-after--li">Translating from one language to another, or even</li><li name="3fc2" id="3fc2" class="graf graf--li graf-after--li">Converting natural language into database queries or scripting code.</li></ul><p name="0476" id="0476" class="graf graf--p graf-after--li">Moreover, previously NLP models were most often domain and task-specific, meaning that you would most likely need to train a custom natural language model depending on your use case and domain. However, thanks to the generalization capabilities of LLMs, a single model can be applied to solve various collections of tasks.</p><p name="c500" id="c500" class="graf graf--p graf-after--p">We have observed quite a strong trend in using retrieval-augmented LLMs, where instead of using LLMs to access its internal knowledge, you use the LLM <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">as a natural language interface</em></strong> to your company’s or private information.</p><figure name="a9ec" id="a9ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zydD2GKzjpEyvL-d_cP0vA.png" data-width="1196" data-height="597" src="https://cdn-images-1.medium.com/max/800/1*zydD2GKzjpEyvL-d_cP0vA.png"><figcaption class="imageCaption">Retrieval-augmented generation. Image by author. Icons from <a href="https://www.flaticon.com/" data-href="https://www.flaticon.com/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Flaticons</a>.</figcaption></figure><p name="6151" id="6151" class="graf graf--p graf-after--figure">The retrieval augmented approach uses the LLM to generate an answer based on the additionally provided relevant documents from your data source.</p><p name="15a9" id="15a9" class="graf graf--p graf-after--p">Therefore, you don’t rely on internal knowledge of the LLM to produce answers. Instead, the LLM is used only for extracting relevant information from documents you passed in and summarizing it.</p><div name="2396" id="2396" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://openai.com/blog/chatgpt-plugins" data-href="https://openai.com/blog/chatgpt-plugins" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://openai.com/blog/chatgpt-plugins"><strong class="markup--strong markup--mixtapeEmbed-strong">ChatGPT plugins</strong><br>openai.com</a><a href="https://openai.com/blog/chatgpt-plugins" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="5766639df7c574edca723e54e87afc81" data-thumbnail-img-id="0*8woYd_vCM6G6BSuB" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*8woYd_vCM6G6BSuB);"></a></div><p name="e781" id="e781" class="graf graf--p graf-after--mixtapeEmbed">For example, the <a href="https://openai.com/blog/chatgpt-plugins" data-href="https://openai.com/blog/chatgpt-plugins" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ChatGPT plugins</a> can be thought of as a retrieval-augmented approach to LLM applications. The ChatGPT interface with a browsing plugin enabled allows the LLM to search the internet to access up-to-date information and use it to construct the final answer.</p><figure name="4aa6" id="4aa6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*SMlbz7k53YgNSACRfkCXJg.png" data-width="897" data-height="802" src="https://cdn-images-1.medium.com/max/800/1*SMlbz7k53YgNSACRfkCXJg.png"><figcaption class="imageCaption">ChatGPT with browsing plugin. Image by author.</figcaption></figure><p name="fe21" id="fe21" class="graf graf--p graf-after--figure">In this example, ChatGPT was able to answer who won the Oscar for various categories in 2023. But, remember, the cutoff knowledge date for ChatGPT is 2021, so it couldn’t know who won the 2023 Oscars from its internal knowledge. Therefore, it accessed external information through the browsing plugin, which allowed it to answer the question with up-to-date information. Those plugins present an integrated augmentation mechanism inside the OpenAI platform.</p><p name="d88d" id="d88d" class="graf graf--p graf-after--p">If you have been watching the LLM space, you might have heard of the <a href="https://python.langchain.com/en/latest/index.html" data-href="https://python.langchain.com/en/latest/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LangChain library</a>.</p><div name="f89c" id="f89c" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c" data-href="https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c"><strong class="markup--strong markup--mixtapeEmbed-strong">Getting Started with LangChain: A Beginner’s Guide to Building LLM-Powered Applications</strong><br><em class="markup--em markup--mixtapeEmbed-em">A LangChain tutorial to build anything with large language models in Python</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="9ca598be079ca31ff6370a2dfb7a12b6" data-thumbnail-img-id="1*4C54ZxHRM1dOlAvlvoEJZg@2x.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*4C54ZxHRM1dOlAvlvoEJZg@2x.jpeg);"></a></div><p name="3af9" id="3af9" class="graf graf--p graf-after--mixtapeEmbed">The LangChain library can be used to allow LLMs to access real-time information from various sources like Google Search, vector databases, or knowledge graphs. For example, LangChain has added a <a href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" data-href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Cypher Search chain</a>, which converts natural language questions into a Cypher statement, uses it to retrieve information from the Neo4j database, and constructs a final answer based on the provided information.</p><div name="08aa" id="08aa" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" data-href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5"><strong class="markup--strong markup--mixtapeEmbed-strong">LangChain has added Cypher Search</strong><br><em class="markup--em markup--mixtapeEmbed-em">With the LangChain library, you can conveniently generate Cypher queries, enabling an efficient retrieval of…</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="11954adfcb52116bf7355acaad5e5df6" data-thumbnail-img-id="1*_mlv2MG9NEoznGg5hoHzYA.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*_mlv2MG9NEoznGg5hoHzYA.png);"></a></div><p name="76e1" id="76e1" class="graf graf--p graf-after--mixtapeEmbed">With the Cypher Search chain, an LLM is not only used to construct a final answer but also to translate a natural language question into a Cypher query.</p><figure name="fb66" id="fb66" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RsB4TVvro3oj7KKGpu_ohA.png" data-width="700" data-height="420" src="https://cdn-images-1.medium.com/max/800/1*RsB4TVvro3oj7KKGpu_ohA.png"><figcaption class="imageCaption">Cypher search in LangChain. Image by author.</figcaption></figure><p name="9997" id="9997" class="graf graf--p graf-after--figure">Another popular library for retrieval-augmented LLM workflows is <a href="https://gpt-index.readthedocs.io/en/latest/index.html" data-href="https://gpt-index.readthedocs.io/en/latest/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LlamaIndex (GPT Index)</a>. LlamaIndex is a comprehensive data framework aimed at enhancing the performance of Large Language Models (LLMs) by enabling them to leverage private or custom data.</p><div name="a94f" id="a94f" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://medium.com/llamaindex-blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595" data-href="https://medium.com/llamaindex-blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/llamaindex-blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"><strong class="markup--strong markup--mixtapeEmbed-strong">LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)</strong><br><em class="markup--em markup--mixtapeEmbed-em">Overview</em>medium.com</a><a href="https://medium.com/llamaindex-blog/llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595" class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="467433570639835bb98677ca027849c3"></a></div><p name="834e" id="834e" class="graf graf--p graf-after--mixtapeEmbed">Firstly, LlamaIndex offers data connectors that facilitate the ingestion of a variety of data sources and formats, encompassing everything from APIs, PDFs, and documents to SQL or graph data.</p><p name="201a" id="201a" class="graf graf--p graf-after--p">This feature allows for an effortless integration of existing data into the LLM. Secondly, it provides efficient mechanisms to structure the ingested data using indices and graphs, ensuring the data is suitably arranged for use with LLMs. In addition, it includes an advanced retrieval and query interface, which enables users to input an LLM prompt and receive back a context-retrieved, knowledge-augmented output.</p><p name="0787" id="0787" class="graf graf--p graf-after--p">The idea behind retrieval-augmented LLM applications like ChatGPT Plugins and LangChain is to avoid relying on internal LLM knowledge only to generate answers. Instead, LLMs are used to solve tasks like <a href="https://neo4j.com/generativeai/?utm_campaign=gen-ai&amp;utm_content=medium&amp;utm_source=blog" data-href="https://neo4j.com/generativeai/?utm_campaign=gen-ai&amp;utm_content=medium&amp;utm_source=blog" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">constructing database queries from natural language</a> and constructing answers based on externally provided information or by utilizing plugins/agents for retrieval.</p><p name="cda6" id="cda6" class="graf graf--p graf-after--p">The retrieval-augmented approach has some clear advantages over the fine-tuning approach:</p><ul class="postList"><li name="d4c7" id="d4c7" class="graf graf--li graf-after--p">The answer can cite its sources of information, which allows you to validate the information and potentially change or update the underlying information based on requirements</li><li name="ab55" id="ab55" class="graf graf--li graf-after--li">Hallucinations are more unlikely to occur as you don’t rely on the internal knowledge of an LLM to answer the question and only use information that is provided in the relevant documents</li><li name="b402" id="b402" class="graf graf--li graf-after--li">Changing, updating, and maintaining the underlying information the LLM uses is easier as you transform the problem from LLM maintenance to a database maintenance, querying and context construction problem</li><li name="ada3" id="ada3" class="graf graf--li graf-after--li">Answers can be personalized based on the user context, or their access permission</li></ul><p name="01b3" id="01b3" class="graf graf--p graf-after--li">On the other hand, you should consider the following limitations when using the retrieval-augmented approach:</p><ul class="postList"><li name="84f4" id="84f4" class="graf graf--li graf-after--p">The answers are only as good as the smart search tool</li><li name="3e46" id="3e46" class="graf graf--li graf-after--li">The application needs access to your specific knowledge base, either that be a database or other data stores</li><li name="f7bf" id="f7bf" class="graf graf--li graf-after--li">Completely disregarding the internal knowledge of the language model limits the number of questions that can be answered</li><li name="96b4" id="96b4" class="graf graf--li graf-after--li">Sometimes LLMs fail to follow instructions, so there is a risk that the context might be ignored or hallucinations occur if no relevant answer data is found in the context.</li></ul><h4 name="61fc" id="61fc" class="graf graf--h4 graf-after--li">Summary</h4><p name="82a8" id="82a8" class="graf graf--p graf-after--h4">This blog post delves into the limitations of Large Language Models (LLMs), such as</p><ul class="postList"><li name="2c11" id="2c11" class="graf graf--li graf-after--p">Knowledge cutoff,</li><li name="aafb" id="aafb" class="graf graf--li graf-after--li">Hallucinations, and</li><li name="fc94" id="fc94" class="graf graf--li graf-after--li">The lack of user customization.</li></ul><p name="baaa" id="baaa" class="graf graf--p graf-after--li">To overcome these, we explored two concepts, namely, fine-tuning and retrieval-augmented use of LLMs.</p><p name="a5ff" id="a5ff" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Fine-tuning an LLM</em></strong> involves the supervised training phase, where question-answer pairs are provided to optimize the performance of the LLM. This can be used to update and expand the LLM’s internal knowledge or fine-tune it for a specific task. However, fine-tuning fails to solve the knowledge cutoff issue as it simply pushes the cutoff to a later date. It also cannot fully eliminate hallucinations. Therefore, we recommend using the fine-tuning approach for slowly changing datasets where some hallucinations are allowed. Since fine-tuning LLMs is relatively new, we are eager to learn more about fine-tuning approaches and best practices.</p><p name="9509" id="9509" class="graf graf--p graf-after--p">The second approach to overcome the limitations of LLMs is the so-called <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">retrieval-augmented generation</em></strong>, where the LLM serves as a natural language interface to access external information, thereby not relying only on its internal knowledge to produce answers. Advantages of the retrieval-augmented approach include source-citing, negligible hallucinations, ease of changing and updating information, and personalization. <br>However, it relies heavily on the intelligent search tool to retrieve relevant information and requires access to the user’s knowledge base. Furthermore, it can only answer queries provided it has the information required to address the question.</p><p name="4e93" id="4e93" class="graf graf--p graf-after--p">Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository.</p><div name="9e99" id="9e99" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/neo4j/NaLLM" data-href="https://github.com/neo4j/NaLLM" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/neo4j/NaLLM"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - neo4j/NaLLM: Repository for the NaLLM project</strong><br><em class="markup--em markup--mixtapeEmbed-em">Welcome to the NaLLM project repository, where we are exploring and demonstrating the synergies between Neo4j and Large…</em>github.com</a><a href="https://github.com/neo4j/NaLLM" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="3989afc1af776637bf832a322583c7bd" data-thumbnail-img-id="0*bBT6F1IFkEpWVxPg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*bBT6F1IFkEpWVxPg);"></a></div><p name="f5ac" id="f5ac" class="graf graf--p graf-after--mixtapeEmbed"><strong class="markup--strong markup--p-strong">Read the next blog in the series:</strong> <a href="https://medium.com/neo4j/knowledge-graphs-llms-multi-hop-question-answering-322113f53f51" data-href="https://medium.com/neo4j/knowledge-graphs-llms-multi-hop-question-answering-322113f53f51" class="markup--anchor markup--p-anchor" target="_blank">Multi-Hop Question Answering</a></p><p name="a926" id="a926" class="graf graf--p graf-after--p graf--trailing"><em class="markup--em markup--p-em">/ The NaLLM Project Core Team at Neo4j: Jon Harris, Noah Mayerhofer, </em><a href="https://medium.com/u/e347f94807a8" data-href="https://medium.com/u/e347f94807a8" data-anchor-type="2" data-user-id="e347f94807a8" data-action-value="e347f94807a8" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank"><em class="markup--em markup--p-em">Oskar Hane</em></a>, <a href="https://medium.com/u/57f13c0ea39a" data-href="https://medium.com/u/57f13c0ea39a" data-anchor-type="2" data-user-id="57f13c0ea39a" data-action-value="57f13c0ea39a" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Tomaz Bratanic</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/30e875d63a35"><time class="dt-published" datetime="2023-06-06T09:35:13.722Z">June 6, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation-30e875d63a35" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>