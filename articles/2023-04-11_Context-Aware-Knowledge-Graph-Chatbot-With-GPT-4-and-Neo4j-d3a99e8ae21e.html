<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Context-Aware Knowledge Graph Chatbot With GPT-4 and Neo4j</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Context-Aware Knowledge Graph Chatbot With GPT-4 and Neo4j</h1>
</header>
<section data-field="subtitle" class="p-summary">
Learn how to implement a chatbot that bases its answers on the information retrieved from a graph database.
</section>
<section data-field="body" class="e-content">
<section name="269c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="f6b4" id="f6b4" class="graf graf--h3 graf--leading graf--title">Context-Aware Knowledge Graph Chatbot With GPT-4 and Neo4j</h3><h4 name="0df4" id="0df4" class="graf graf--h4 graf-after--h3 graf--subtitle"><em class="markup--em markup--h4-em">Learn how to implement a chatbot that bases its answers on the information retrieved from a graph database</em></h4><p name="c7cf" id="c7cf" class="graf graf--p graf-after--h4">Not so long ago, OpenAI added Chat API, which is optimized for generating conversations. The main difference between Chat and the older Completion APIs is that the Chat API allows specifying the conversation history. While the Completion API could be used for single-turn tasks like text generation, summarization, and similar, the Chat API is designed more for conversational tasks like chatbots or customer support. The ability to provide a dialogue history or the context of the conversation gives the model a better ability to answer additional or follow-up questions.</p><p name="3a52" id="3a52" class="graf graf--p graf-after--p">Just a month ago, I wrote a post about how <a href="https://medium.com/neo4j/knowledge-graph-based-chatbot-with-gpt-3-and-neo4j-c4ebbd325ed" data-href="https://medium.com/neo4j/knowledge-graph-based-chatbot-with-gpt-3-and-neo4j-c4ebbd325ed" class="markup--anchor markup--p-anchor" target="_blank">I implemented a knowledge-graph-based chatbot</a>. There, I used a Completion API, specifically text-davinci-003 model, to generate Cypher statements based on user prompts. However, since the Completion API is unaware of the conversation history, we must be very specific in the prompts about the entities we are interested in. For example, we cannot use pronouns like it, they, or she in follow-up questions as the model has no context to what they refer to.</p><p name="edef" id="edef" class="graf graf--p graf-after--p">However, we can solve this problem by using the Chat API. Therefore, I have decided to try out the new GPT-4 as well as the GPT-3.5-turbo models and evaluate how they could be used for a chatbot that fetches its information from a knowledge graph.</p><h4 name="45a7" id="45a7" class="graf graf--h4 graf-after--p">Knowledge Graph-Based Chatbot Design</h4><p name="0c5b" id="0c5b" class="graf graf--p graf-after--h4">Chatbots can be used for various applications, as shown by the myriad applications popping up lately. One significant problem with LLMs like ChatGPT is that they can confidently answer any question while possibly providing false information, also known as <strong class="markup--strong markup--p-strong">hallucinations</strong>.</p><p name="0a47" id="0a47" class="graf graf--p graf-after--p">Hallucinations are acceptable in some domains and not in others. It is also hard to retrain a model not to hallucinate specific information. However, if we used a knowledge base like a graph database to base the chatbot’s answer on, we would have complete and total control over the answers the chatbot might provide.</p><figure name="b3b5" id="b3b5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*21Rie9n9o92D3km_QF4HAQ.png" data-width="715" data-height="395" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*21Rie9n9o92D3km_QF4HAQ.png"><figcaption class="imageCaption">Context-aware knowledge graph chatbot design. Image by the author.</figcaption></figure><p name="5325" id="5325" class="graf graf--p graf-after--figure">It all begins when a user inputs their prompt. The prompt and the dialogue history get sent to GPT-4 endpoint to generate a Cypher statement. The returned Cypher statement is then used to query the Neo4j database. While we could directly return the database results to the user, it is a better user experience if we send the database results along with the dialogue context to generate natural language-looking text as an answer. Here, we use the GPT-3.5-turbo model to create an answer, as the model is good enough and much cheaper than the GPT-4. Finally, we send the generated response back to the user.</p><p name="c756" id="c756" class="graf graf--p graf-after--p">It sounds surprisingly easy and is also quite simple to implement. The code is available on <a href="https://github.com/tomasonjo/NeoGPT-Recommender" data-href="https://github.com/tomasonjo/NeoGPT-Recommender" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p><h4 name="0343" id="0343" class="graf graf--h4 graf-after--p">Neo4j Environment &amp; Dataset</h4><p name="cff1" id="cff1" class="graf graf--p graf-after--h4">First, we will configure the Neo4j environment. We will use the dataset available as the <a href="https://sandbox.neo4j.com/?usecase=recommendations" data-href="https://sandbox.neo4j.com/?usecase=recommendations" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">recommendations project</a> in the Neo4j sandbox, which is based on the <a href="https://grouplens.org/datasets/movielens/" data-href="https://grouplens.org/datasets/movielens/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MovieLens dataset</a>. The easiest solution is simply to create a Neo4j Sandbox instance by following <a href="https://sandbox.neo4j.com/?usecase=recommendations" data-href="https://sandbox.neo4j.com/?usecase=recommendations" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this link</a>. However, if you would prefer a local instance of Neo4j, you can also restore a <a href="https://github.com/neo4j-graph-examples/recommendations/tree/main/data" data-href="https://github.com/neo4j-graph-examples/recommendations/tree/main/data" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">database dump that is available on GitHub</a>.</p><p name="f336" id="f336" class="graf graf--p graf-after--p">After the Neo4j database is instantiated, we should have a graph with the following schema populated.</p><figure name="5139" id="5139" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HorBDLYVPFUvubEeoIYJCw.png" data-width="794" data-height="677" src="https://cdn-images-1.medium.com/max/800/1*HorBDLYVPFUvubEeoIYJCw.png"><figcaption class="imageCaption">Graph schema. Image by the author.</figcaption></figure><p name="1038" id="1038" class="graf graf--p graf-after--figure">The database contains information about movies, actors, directors, and genres. Additionally, it contains ratings about movies by users. We can evaluate the size of the database with the following APOC procedure.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="e880" id="e880" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">CALL</span> apoc.meta.stats YIELD<br />nodeCount, relCount, labels</span></pre><p name="9f51" id="9f51" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="0748" id="0748" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yGpdNokFZGwV2Kjzb7WbJQ.png" data-width="874" data-height="332" src="https://cdn-images-1.medium.com/max/800/1*yGpdNokFZGwV2Kjzb7WbJQ.png"><figcaption class="imageCaption">Database size. Image by the author.</figcaption></figure><p name="a41a" id="a41a" class="graf graf--p graf-after--figure">The database has almost 30 thousand nodes and slightly more than 165 thousand relationships. The <strong class="markup--strong markup--p-strong">labels</strong> column provides the count of nodes per label. For example, there are nearly 10 thousand movies and 20 thousand people. It seems that a Person node can have a secondary label Actor or Director. If you want to get to know the dataset or Cypher a bit more first, you can follow the browser guide that is part of the Sandbox environment.</p><h4 name="8a6d" id="8a6d" class="graf graf--h4 graf-after--p">English2Cypher With GPT-4</h4><p name="abbe" id="abbe" class="graf graf--p graf-after--h4">Generating Cypher statements based on user input is vital to our chatbot. It needs to be able to reliably generate valid Cypher statements, as there is no human in the loop who can fix the queries if they are not working. Additionally, the model should be able to distill context from dialogue history.</p><p name="3c23" id="3c23" class="graf graf--p graf-after--p">The OpenAI’s ChatCompletion endpoint currently supports only <strong class="markup--strong markup--p-strong">GPT-3.5-turbo</strong> and <strong class="markup--strong markup--p-strong">GPT-4</strong> models. I first tried the GPT-3.5-turbo model but quickly realized that it is not a good fit for generating Cypher statements based on dialogue history as it often ignores instructions not to apologize or explain results. There are some rumors that GPT-3.5-turbo is Canadian, as it likes to apologize for apologizing too much. Additionally, GPT-4 is better at understanding context and learning from the training examples.</p><p name="b52a" id="b52a" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">You can use GPT-3.5-turbo as well if you don’t have access to GPT-4 yet. The code includes cleaning the results of unwanted apologies and explanations.</em></p><p name="e892" id="e892" class="graf graf--p graf-after--p">First, we have to define the <strong class="markup--strong markup--p-strong">system</strong> message. In it, we define the role of the model as being a Cypher statement generator, along with some constraints like that it shouldn’t ever explain or apologize. Additionally, we don’t want it to construct any Cypher statements that cannot be inferred based on the training examples.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="b286" id="b286" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">system = <span class="hljs-string">f&quot;&quot;&quot;<br />You are an assistant with an ability to generate Cypher queries based off example Cypher queries.<br />Example Cypher queries are: \n <span class="hljs-subst">{examples}</span> \n<br />Do not response with any explanation or any other information except the Cypher query.<br />You do not ever apologize and strictly generate cypher statements based of the provided Cypher examples.<br />You need to update the database using an appropriate Cypher statement when a user mentions their likes or dislikes, or what they watched already.<br />Do not provide any Cypher statements that can&#x27;t be inferred from Cypher examples.<br />Inform the user when you can&#x27;t infer the cypher statement due to the lack of context of the conversation and state what is the missing context.<br />&quot;&quot;&quot;</span></span></pre><p name="3d70" id="3d70" class="graf graf--p graf-after--pre">For example, suppose we don’t put the constraint that it should only use information in the training examples. In that case, it can generate Cypher statements for any user inputs, no matter how inaccurate.</p><p name="0e04" id="0e04" class="graf graf--p graf-after--p">I have evaluated ChatGPT’s ability to generate Cypher statements based on providing graph schema or sample Cypher statements on a couple of different datasets. However, supplying sample Cypher statements always seemed the better option, so this is now my default approach to using GPT models to generate Cypher statements.</p><p name="64cb" id="64cb" class="graf graf--p graf-after--p">We will be using the following examples to feed into GPT-4.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="7516" id="7516" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">examples = <span class="hljs-string">&quot;&quot;&quot;<br /># I have already watched Top Gun<br />MATCH (u:User {id: $userId}), (m:Movie {title:&quot;Top Gun&quot;})<br />MERGE (u)-[:WATCHED]-&gt;(m)<br />RETURN distinct {answer: &#x27;noted&#x27;} AS result<br /># I like Top Gun<br />MATCH (u:User {id: $userId}), (m:Movie {title:&quot;Top Gun&quot;})<br />MERGE (u)-[:LIKE_MOVIE]-&gt;(m)<br />RETURN distinct {answer: &#x27;noted&#x27;} AS result<br /># What is a good comedy?<br />MATCH (u:User {id:$userId}), (m:Movie)-[:IN_GENRE]-&gt;(:Genre {name:&quot;Comedy&quot;})<br />WHERE NOT EXISTS {(u)-[:WATCHED]-&gt;(m)}<br />RETURN {movie: m.title} AS result<br />ORDER BY m.imdbRating DESC LIMIT 1<br /># Who played in Top Gun?<br />MATCH (m:Movie)&lt;-[:ACTED_IN]-(a)<br />RETURN {actor: a.name} AS result<br /># What is the plot of the Copycat movie?<br />MATCH (m:Movie {title: &quot;Copycat&quot;})<br />RETURN {plot: m.plot} AS result<br /># Did Luis Guzmán appear in any other movies?<br />MATCH (p:Person {name:&quot;Luis Guzmán&quot;})-[:ACTED_IN]-&gt;(movie)<br />RETURN {movie: movie.title} AS result<br /># Recommend a movie<br />MATCH (u:User {id: $userId})-[:LIKE_MOVIE]-&gt;(m:Movie)<br />MATCH (m)&lt;-[r1:RATED]-()-[r2:RATED]-&gt;(otherMovie)<br />WHERE r1.rating &gt; 3 AND r2.rating &gt; 3 AND NOT EXISTS {(u)-[:WATCHED|LIKE_MOVIE|DISLIKE_MOVIE]-&gt;(otherMovie)}<br />WITH otherMovie, count(*) AS count<br />ORDER BY count DESC<br />LIMIT 1<br />RETURN {recommended_movie:otherMovie.title} AS result<br />&quot;&quot;&quot;</span></span></pre><p name="220e" id="220e" class="graf graf--p graf-after--pre">We have provided only seven Cypher examples. However, GPT-4 has a good grasp of Cypher and can use these examples to generate Cypher statements that don’t appear in the training example. For the most part, we teach the model which relationships and properties to use when traversing or filtering nodes.</p><p name="a62d" id="a62d" class="graf graf--p graf-after--p">Now, we can define the function that generates Cypher statements based on the conversation.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="9503" id="9503" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-meta">@retry(<span class="hljs-params">tries=<span class="hljs-number">2</span>, delay=<span class="hljs-number">5</span></span>)</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_cypher</span>(<span class="hljs-params">messages</span>):<br />    messages = [<br />        {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: system}<br />    ] + messages<br />    <span class="hljs-comment"># Make a request to OpenAI</span><br />    completions = openai.ChatCompletion.create(<br />        model=<span class="hljs-string">&quot;gpt-4&quot;</span>,<br />        messages=messages,<br />        temperature=<span class="hljs-number">0.0</span><br />    )<br />    response = completions.choices[<span class="hljs-number">0</span>].message.content<br />    <span class="hljs-comment"># Sometime the models bypasses system prompt and returns</span><br />    <span class="hljs-comment"># data based on previous dialogue history</span><br />    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-string">&quot;MATCH&quot;</span> <span class="hljs-keyword">in</span> response <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;{&quot;</span> <span class="hljs-keyword">in</span> response:<br />        <span class="hljs-keyword">raise</span> Exception(<br />            <span class="hljs-string">&quot;&quot;&quot;GPT bypassed system message and is returning response <br />                based on previous conversation history&quot;&quot;&quot;</span> + response)<br />    <span class="hljs-comment"># If the model apologized, remove the first line</span><br />    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;apologi&quot;</span> <span class="hljs-keyword">in</span> response:<br />        response = <span class="hljs-string">&quot; &quot;</span>.join(response.split(<span class="hljs-string">&quot;\n&quot;</span>)[<span class="hljs-number">1</span>:])<br />    <span class="hljs-comment"># Sometime the model adds quotes around Cypher when it wants to explain stuff</span><br />    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;`&quot;</span> <span class="hljs-keyword">in</span> response:<br />        response = response.split(<span class="hljs-string">&quot;```&quot;</span>)[<span class="hljs-number">1</span>].strip(<span class="hljs-string">&quot;`&quot;</span>)<br />    <span class="hljs-keyword">return</span> response</span></pre><p name="bdad" id="bdad" class="graf graf--p graf-after--pre">If the model apologizes, it is always in the first line or sentence of the response. Additionally, when it offers additional unwanted explanations, it is kind enough to put <code class="markup--code markup--p-code">```</code> quotes around the Cypher statements. These two data cleaning steps are more relevant for the GPT-3.5-turbo model.</p><p name="efda" id="efda" class="graf graf--p graf-after--p">We also raise an exception if the model completely bypasses the system message and returns a message from previous conversations. You might better understand this scenario through an example.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="9aff" id="9aff" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">print</span>(generate_cypher([<br />   {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;What are some good cartoon?&#x27;</span>},<br />   {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;assistant&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;Shrek 3&#x27;</span>},<br />   {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;Which actors appeared in it?&#x27;</span>}]))<br /><span class="hljs-comment"># MATCH (m:Movie {title: &quot;Shrek 3&quot;})&lt;-[:ACTED_IN]-(a:Person)</span><br /><span class="hljs-comment"># RETURN {actor: a.name} AS result</span></span></pre><p name="0079" id="0079" class="graf graf--p graf-after--pre">In this example, we provide the model with some conversation history. For the model to learn the context of the conversation, we don’t offer the previous Cypher statements it generated but the answers we got from Neo4j using those Cypher statements. If you use GPT-3.5-turbo, it will apologize for not providing a Cypher statement in the previous conversation, even when you tell it not to do that five times. However, GPT-4 will not do that. However, it will learn based on the provided conversation history and sometimes altogether bypass the system message and return the <strong class="markup--strong markup--p-strong">Shrek 3</strong> as a result when it should be generating Cypher statements.</p><p name="3956" id="3956" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">The idea that you shouldn’t input proprietary code or information to GPT is nothing new. It was just a newsflash of how obvious the learning process was, as my conversation history is custom generated and doesn’t include previously generated Cypher statements that the model delivered.</em></p><p name="764d" id="764d" class="graf graf--p graf-after--p">Additionally, we can constrain what the model does when it is provided with an input that cannot be inferred from training examples.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="2793" id="2793" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">print</span>(generate_cypher([<br />   {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;What are some good cartoon?&#x27;</span>},<br />   {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;assistant&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;Shrek 3&#x27;</span>},<br />   {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-string">&#x27;Who was the first person on the moon?&#x27;</span>}]))<br /><span class="hljs-comment"># I can only provide Cypher queries based on the provided examples. </span><br /><span class="hljs-comment"># Please ask a question related to the examples or provide a new Cypher query example.</span></span></pre><p name="d227" id="d227" class="graf graf--p graf-after--pre">However, even with the constraints in place, it might not abide by them. Perhaps there are better prompt options to keep GPT-4 in check.</p><h4 name="1a3f" id="1a3f" class="graf graf--h4 graf-after--p">Graph2text</h4><p name="f4a1" id="f4a1" class="graf graf--p graf-after--h4">The second piece of the puzzle is the natural text generation based on the results we get from the graph database. Similar to before, we want to include conversation history to make the generated text sound more authentic. Here, we can use the GPT-3.5-turbo as it is much cheaper and good enough to generate natural language text.</p><p name="3ca5" id="3ca5" class="graf graf--p graf-after--p">Through some trial and error, I developed the following system message.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0cc9" id="0cc9" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">system = <span class="hljs-string">f&quot;&quot;&quot;<br />You are an assistant that helps to generate text to form nice and human understandable answers based.<br />The latest prompt contains the information, and you need to generate a human readable response based on the given information.<br />Make it sound like the information are coming from an AI assistant, but don&#x27;t add any information.<br />Do not add any additional information that is not explicitly provided in the latest prompt.<br />I repeat, do not add any information that is not explicitly given.<br />&quot;&quot;&quot;</span></span></pre><p name="17b6" id="17b6" class="graf graf--p graf-after--pre">Adding the instruction to make the information sound like it is coming from an AI assistant makes it sound more like you have a conversation with the bot instead of it summarizing the database results. However, when you tell it to behave like an AI assistant, it wants to add additional information it knows to the responses. We want to avoid that as the GPT-3.5-turbo can hallucinate results, and it would be impossible to differentiate what information came from the database or the model. Therefore, I added three sentences not to add any additional information as GPT-3.5-turbo isn’t as good in following instructions stated only once. The system prompt also tells it not to apologize. However, being Canadian, it can’t help itself not to.</p><p name="c9c4" id="c9c4" class="graf graf--p graf-after--p">The function to generate natural language from database results is the following.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="9348" id="9348" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_response</span>(<span class="hljs-params">messages</span>):<br />    messages = [<br />        {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: system}<br />    ] + messages<br />    <span class="hljs-comment"># Make a request to OpenAI</span><br />    completions = openai.ChatCompletion.create(<br />        model=<span class="hljs-string">&quot;gpt-3.5-turbo&quot;</span>,<br />        messages=messages,<br />        temperature=<span class="hljs-number">0.0</span><br />    )<br />    response = completions.choices[<span class="hljs-number">0</span>].message.content<br />    <span class="hljs-comment"># If the model apologized, remove the first line or sentence</span><br />    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;apologi&quot;</span> <span class="hljs-keyword">in</span> response:<br />        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;\n&quot;</span> <span class="hljs-keyword">in</span> response:<br />            response = <span class="hljs-string">&quot; &quot;</span>.join(response.split(<span class="hljs-string">&quot;\n&quot;</span>)[<span class="hljs-number">1</span>:])<br />        <span class="hljs-keyword">else</span>:<br />            response = <span class="hljs-string">&quot; &quot;</span>.join(response.split(<span class="hljs-string">&quot;.&quot;</span>)[<span class="hljs-number">1</span>:])<br />    <span class="hljs-keyword">return</span> response</span></pre><p name="0ac1" id="0ac1" class="graf graf--p graf-after--pre">Here, we needed only to filter out apologies as the model doesn’t feel the need to explain how it generated text.</p><p name="c2f4" id="c2f4" class="graf graf--p graf-after--p">We can test the function on the following example.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5fea" id="5fea" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">data = [{<span class="hljs-string">&#x27;actor&#x27;</span>: <span class="hljs-string">&#x27;Sigourney Weaver&#x27;</span>, <span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&quot;Witch&quot;</span>}, <br />        {<span class="hljs-string">&#x27;actor&#x27;</span>: <span class="hljs-string">&#x27;Holly Hunter&#x27;</span>, <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;Assassin&quot;</span>}, <br />        {<span class="hljs-string">&#x27;actor&#x27;</span>: <span class="hljs-string">&#x27;Dermot Mulroney&#x27;</span>},<br />        {<span class="hljs-string">&#x27;actor&#x27;</span>: <span class="hljs-string">&#x27;William McNamara&#x27;</span>}]<br /><span class="hljs-built_in">print</span>(generate_response([{<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-built_in">str</span>(data)}]))<br /><span class="hljs-comment">#The list contains four actors and their respective roles. </span><br /><span class="hljs-comment">#Sigourney Weaver played the role of a witch, while Holly Hunter portrayed an assassin.</span><br /><span class="hljs-comment">#The roles of Dermot Mulroney and William McNamara were not specified.</span></span></pre><p name="e837" id="e837" class="graf graf--p graf-after--pre">I noticed that using JSON or dictionary objects is the best way to provide some context along with the database results. That way, we can present the model with the value context using the dictionary key. So, for example, the model now knows that it has been provided with actors and their respective roles.</p><h4 name="1d88" id="1d88" class="graf graf--h4 graf-after--p">Chatbot Implementation Using Streamlit</h4><p name="0fd9" id="0fd9" class="graf graf--p graf-after--h4">We will use the <a href="https://pypi.org/project/streamlit-chat/" data-href="https://pypi.org/project/streamlit-chat/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">streamlit-chat</a> package to develop the user interface for our chatbot. It is effortless to use and great for simple demos. I will walk you through the parts that I feel are important and not go through all the code. However, you can always check the chatbot implementation on <a href="https://github.com/tomasonjo/NeoGPT-Recommender" data-href="https://github.com/tomasonjo/NeoGPT-Recommender" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p><figure name="3486" id="3486" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*foiMG58qMnRafySZGCw0NQ.png" data-width="1117" data-height="549" src="https://cdn-images-1.medium.com/max/800/1*foiMG58qMnRafySZGCw0NQ.png"><figcaption class="imageCaption">Example dialogue. Image by the author.</figcaption></figure><p name="d710" id="d710" class="graf graf--p graf-after--figure">An important part of our chatbot is storing the conversation history along with information about generated Cypher statements and database results. We will store relevant information with the Streamlit’s <code class="markup--code markup--p-code">session_state</code> method.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="c539" id="c539" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Generated natural language</span><br /><span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;generated&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br />    st.session_state[<span class="hljs-string">&#x27;generated&#x27;</span>] = []<br /><span class="hljs-comment"># Neo4j database results</span><br /><span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;database_results&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br />    st.session_state[<span class="hljs-string">&#x27;database_results&#x27;</span>] = []<br /><span class="hljs-comment"># User input</span><br /><span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;user_input&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br />    st.session_state[<span class="hljs-string">&#x27;user_input&#x27;</span>] = []<br /><span class="hljs-comment"># Generated Cypher statements</span><br /><span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;cypher&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br />    st.session_state[<span class="hljs-string">&#x27;cypher&#x27;</span>] = []</span></pre><p name="c175" id="c175" class="graf graf--p graf-after--pre">Additionally, we must define a function that will construct a conversation history in a form that the OpenAI’s ChatCompletion endpoint expects.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6bfa" id="6bfa" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_context</span>(<span class="hljs-params">prompt, context_data=<span class="hljs-string">&#x27;generated&#x27;</span></span>):<br />    context = []<br />    <span class="hljs-comment"># If any history exists</span><br />    <span class="hljs-keyword">if</span> st.session_state[<span class="hljs-string">&#x27;generated&#x27;</span>]:<br />        <span class="hljs-comment"># Add the last three exchanges</span><br />        size = <span class="hljs-built_in">len</span>(st.session_state[<span class="hljs-string">&#x27;generated&#x27;</span>])<br />        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">max</span>(size-<span class="hljs-number">3</span>, <span class="hljs-number">0</span>), size):<br />            context.append(<br />                {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: st.session_state[<span class="hljs-string">&#x27;past&#x27;</span>][i]})<br />            context.append(<br />                {<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;assistant&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: st.session_state[context_data][i]})<br />    <span class="hljs-comment"># Add the latest user prompt</span><br />    context.append({<span class="hljs-string">&#x27;role&#x27;</span>: <span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>: <span class="hljs-built_in">str</span>(prompt)})<br />    <span class="hljs-keyword">return</span> context</span></pre><p name="e352" id="e352" class="graf graf--p graf-after--pre">In this example, we construct the dialogue context using only up to three exchanges between the user and the assistant. Therefore, we will send at most six messages (three by the user and three by the assistant). While the user message is always taken from the <strong class="markup--strong markup--p-strong">user_input</strong> session state, we can specify which session state should be used as input to the assistant message. By default, we use the <strong class="markup--strong markup--p-strong">generated</strong> session state.</p><p name="7f34" id="7f34" class="graf graf--p graf-after--p">Finally, we have to define how to handle user inputs</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="29a8" id="29a8" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">if</span> user_input:<br />    cypher = generate_cypher(generate_context(user_input, <span class="hljs-string">&#x27;database_results&#x27;</span>))<br />    <span class="hljs-comment"># If not a valid Cypher statement</span><br />    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-string">&quot;MATCH&quot;</span> <span class="hljs-keyword">in</span> cypher:<br />        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;No Cypher was returned&#x27;</span>)<br />        st.session_state.user_input.append(user_input)<br />        st.session_state.generated.append(<br />            cypher)<br />        st.session_state.cypher.append(<br />            <span class="hljs-string">&quot;No Cypher statement was generated&quot;</span>)<br />        st.session_state.database_results.append(<span class="hljs-string">&quot;&quot;</span>)<br />    <span class="hljs-keyword">else</span>:<br />        <span class="hljs-comment"># Query the database, userID is hardcoded</span><br />        results = run_query(cypher, {<span class="hljs-string">&#x27;userId&#x27;</span>: USER_ID})<br />        <span class="hljs-comment"># Harcode result limit to 10</span><br />        results = results[:<span class="hljs-number">10</span>]<br />        <span class="hljs-comment"># Graph2text</span><br />        answer = generate_response(generate_context(<br />            <span class="hljs-string">f&quot;Question was <span class="hljs-subst">{user_input}</span> and the response should include only information that is given here: <span class="hljs-subst">{<span class="hljs-built_in">str</span>(results)}</span>&quot;</span>))<br />        st.session_state.database_results.append(<span class="hljs-built_in">str</span>(results))<br />        st.session_state.user_input.append(user_input)<br />        st.session_state.generated.append(answer),<br />        st.session_state.cypher.append(cypher)</span></pre><p name="85c9" id="85c9" class="graf graf--p graf-after--pre">Every generated Cypher statement should have a <strong class="markup--strong markup--p-strong">MATCH</strong> clause in it. If it doesn’t have it, the GPT-4 model is likely trying to tell us something. For example, it won’t return a Cypher statement when it can’t resolve the prompt as no context is given.</p><figure name="7688" id="7688" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*mUeVouJUQcWAXkOuAzELaQ.png" data-width="1185" data-height="392" src="https://cdn-images-1.medium.com/max/800/1*mUeVouJUQcWAXkOuAzELaQ.png"><figcaption class="imageCaption">Missing context. Image by the author.</figcaption></figure><p name="606d" id="606d" class="graf graf--p graf-after--figure">Otherwise, if the model can generate a valid Cypher statement based on user input, we use that Cypher statement to query the Neo4j database and use the results to create natural language-sounding answers. Since most Cypher statements in the training example don’t have any LIMIT set, we add a manual limit to display ten results at the most from each query.</p><p name="34f7" id="34f7" class="graf graf--p graf-after--p">Additionally, queries that are updating the database use the <em class="markup--em markup--p-em">userId </em>parameter, which is provided by the application and not the model.</p><h4 name="33d9" id="33d9" class="graf graf--h4 graf-after--p">Example Dialogue Flow</h4><p name="15f6" id="15f6" class="graf graf--p graf-after--h4">We can now test the chatbot on an example flow.</p><figure name="fd3f" id="fd3f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*sOLX_uwihOCzMVHEWO_b0w.png" data-width="818" data-height="387" src="https://cdn-images-1.medium.com/max/800/1*sOLX_uwihOCzMVHEWO_b0w.png"><figcaption class="imageCaption">Finding movies by their titles. Image by the author.</figcaption></figure><p name="8d28" id="8d28" class="graf graf--p graf-after--figure">The training set contains an example that allows us to find movies based on their titles. In this example, we have searched for Pokémon movies. We can now ask the model a follow-up question.</p><figure name="8f4d" id="8f4d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AKLj_SxwQgp4boZBB_dJGg.png" data-width="827" data-height="758" src="https://cdn-images-1.medium.com/max/800/1*AKLj_SxwQgp4boZBB_dJGg.png"><figcaption class="imageCaption">Follow-up question about actors. Image by the author.</figcaption></figure><p name="df0c" id="df0c" class="graf graf--p graf-after--figure">Since we provided the context of the conversation, the model could deduct which movie is being referenced. However, unfortunately, the GPT-3.5-turbo model decided to add additional information about the English version (also known as X in English), even though it was explicitly told not to do that three times. I have tested the same flow with GPT-4 generating natural language text.</p><figure name="0784" id="0784" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*f3MgORF2Zf2a_VDSbve4oA.png" data-width="803" data-height="651" src="https://cdn-images-1.medium.com/max/800/1*f3MgORF2Zf2a_VDSbve4oA.png"><figcaption class="imageCaption">GPT-4 is much better at following the rules. Image by author.</figcaption></figure><p name="a2b8" id="a2b8" class="graf graf--p graf-after--figure">Using GPT-4 as a natural language generating model, we were able to avoid adding unwanted additional information by the model.</p><p name="cf54" id="cf54" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">I’ll be using GPT-4 as text generating model in the following examples. The GPT-3.5-turbo was messing with me a bit when I wanted to replicate the conversation flow, so I gave up on it. For the most part, GPT-3.5-turbo is good enough, but it has its moments.</em></p><p name="10f9" id="10f9" class="graf graf--p graf-after--p">We can ask another follow-up question.</p><figure name="bfd0" id="bfd0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*q_7wLdoZM_Xk3nnYKTXwng.png" data-width="700" data-height="444" src="https://cdn-images-1.medium.com/max/800/1*q_7wLdoZM_Xk3nnYKTXwng.png"><figcaption class="imageCaption">Finding other movies. Image by the author.</figcaption></figure><p name="13d2" id="13d2" class="graf graf--p graf-after--figure">Here, I got pretty excited. GPT-4 ability to infer new Cypher queries based on the training examples is astounding. It has a good understanding of the Cypher itself, and when it grasps a given graph schema, it performs very well.</p><p name="3038" id="3038" class="graf graf--p graf-after--p">We have also added training examples that generate Cypher statements that update the database.</p><figure name="e55f" id="e55f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*E31fiPlHGfssXdH5WCyVwg.png" data-width="971" data-height="390" src="https://cdn-images-1.medium.com/max/800/1*E31fiPlHGfssXdH5WCyVwg.png"><figcaption class="imageCaption">Storing information about watched movies. Image by the author.</figcaption></figure><p name="eb00" id="eb00" class="graf graf--p graf-after--figure">Additionally, we can also let it know which movies we like.</p><figure name="c216" id="c216" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*JUsoKiRdVcAo52JxVDv8Ag.png" data-width="834" data-height="296" src="https://cdn-images-1.medium.com/max/800/1*JUsoKiRdVcAo52JxVDv8Ag.png"><figcaption class="imageCaption">Storing information about liked movies. Image by the author.</figcaption></figure><p name="f6fa" id="f6fa" class="graf graf--p graf-after--figure">Now that it knows which movies we like, we can use it to recommend movies based on our likes.</p><figure name="0f44" id="0f44" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZomHBWFOrXF3Rbkw1palNg.png" data-width="1073" data-height="651" src="https://cdn-images-1.medium.com/max/800/1*ZomHBWFOrXF3Rbkw1palNg.png"><figcaption class="imageCaption">Basic recommendations. Image by the author.</figcaption></figure><p name="7d50" id="7d50" class="graf graf--p graf-after--figure">I added a simple recommendation query in the training examples that uses a basic variation of collaborative filtering to recommend movies. Simply put, we utilize the rating of movies available in the graph. If a user liked the same movie as we did, we examine which other movies they also liked that we haven’t watched and recommend the most frequent one.</p><p name="e0af" id="e0af" class="graf graf--p graf-after--p">I was testing out various flows and found it interesting how the chatbot behaved. For example, I tried to make the model second-guess the recommendation.</p><figure name="98fc" id="98fc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*n7zuu1TPpwKwxxtHIomeiQ.png" data-width="1062" data-height="663" src="https://cdn-images-1.medium.com/max/800/1*n7zuu1TPpwKwxxtHIomeiQ.png"><figcaption class="imageCaption">Trying to make the model second guess. Image by the author.</figcaption></figure><p name="0794" id="0794" class="graf graf--p graf-after--figure">Here is another example where the model completely overrides the system message. The English2Cypher part has explicit instructions that it needs to generate only Cypher statements and nothing else. However, in this example, it simply reiterated the recommendation as it was available in the conversation history and didn’t generate any Cypher statement.</p><p name="55b8" id="55b8" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Note that GPT-4 and GPT-3.5-turbo are not deterministic, which means you might get different results (or not).</em></p><p name="fd7d" id="fd7d" class="graf graf--p graf-after--p">However, having the ability to provide conversation history and persist information in the database gives a feeling of great user experience that can be used in many other applications.</p><figure name="4031" id="4031" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4A8KalVq_IWdU5zO_C1F9Q.png" data-width="1073" data-height="645" src="https://cdn-images-1.medium.com/max/800/1*4A8KalVq_IWdU5zO_C1F9Q.png"><figcaption class="imageCaption">Conversation history and storing information in the database allows for human-like dialogues. Image by the author.</figcaption></figure><h4 name="785d" id="785d" class="graf graf--h4 graf-after--figure">Multi-Language Capabilities</h4><p name="d2bf" id="d2bf" class="graf graf--p graf-after--h4">Not often mentioned, but both GPT-3.5-turbo and GPT-4 understand many languages. I gave that a little test as well.</p><figure name="1a98" id="1a98" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*0lqzcBFBDzZSCIRK955qcw.png" data-width="874" data-height="736" src="https://cdn-images-1.medium.com/max/800/1*0lqzcBFBDzZSCIRK955qcw.png"><figcaption class="imageCaption">GPT-4 multilanguage capabilities. Image by the author</figcaption></figure><p name="e65f" id="e65f" class="graf graf--p graf-after--figure">If that is not amazing, I don’t know what is. We provided the model with a few English examples, and now it can be used in many languages. Also, the responses are in an appropriate language.</p><p name="5d4d" id="5d4d" class="graf graf--p graf-after--p">Lastly, I also wanted to test if it will automatically translate plots as the plots are originally in English.</p><figure name="c8b4" id="c8b4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AGtIlGbK8KQQwHxuzd9U1A.png" data-width="1190" data-height="253" src="https://cdn-images-1.medium.com/max/800/1*AGtIlGbK8KQQwHxuzd9U1A.png"><figcaption class="imageCaption">Plot translation. Image by the author</figcaption></figure><p name="bb2c" id="bb2c" class="graf graf--p graf-after--figure">The plot information provided to GPT-4 was:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="db66" id="db66" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-string">&quot;As students at the United States Navy&#x27;s elite fighter weapons school compete <br />to be best in the class, one daring young pilot learns a few things from a <br />civilian instructor that are not taught in the classroom.&quot;</span></span></pre><p name="c3e7" id="c3e7" class="graf graf--p graf-after--pre">And yes, the model knows it should probably translate the answer to the same language the question was asked in. Imagine all the possibilities of bringing information or content worldwide without worrying too much about translations.</p><h4 name="180a" id="180a" class="graf graf--h4 graf-after--p">Summary</h4><p name="524b" id="524b" class="graf graf--p graf-after--h4">The addition of OpenAI’s Chat Completion endpoint allows us to generate more human-like dialogues. It has excellent value for answering follow-up questions that rely on information provided in the conversation history, making the whole discussion smoother. It can make the user feel like they are talking to someone who understands their meaning.</p><p name="d808" id="d808" class="graf graf--p graf-after--p">I would recommend using GPT-4 when possible, especially for generating Cypher statements, as it is better at following instructions and understanding the given task. However, at the moment, I would avoid using any GPT model on private or proprietary data as I have seen firsthand how the model might use previous conversation histories as training examples for the model. With the knowledge-graph-based chatbot, it was fairly obvious. The model should only return Cypher statements, but it somehow decided a couple of times to override system instructions and return the previously provided database results, which were used to understand the conversation context.</p><p name="e620" id="e620" class="graf graf--p graf-after--p">To learn more about this topic, join me at NODES 2023, a free online global conference about graph technologies. The CFP is open now until June 30. <a href="https://dev.neo4j.com/nodes23" data-href="https://dev.neo4j.com/nodes23" class="markup--anchor markup--p-anchor" rel="noopener noreferrer noopener" target="_blank">https://dev.neo4j.com/nodes23</a></p><p name="3ddd" id="3ddd" class="graf graf--p graf-after--p graf--trailing">As always, all the code is available on <a href="https://github.com/tomasonjo/NeoGPT-Recommender" data-href="https://github.com/tomasonjo/NeoGPT-Recommender" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/d3a99e8ae21e"><time class="dt-published" datetime="2023-04-11T19:30:05.395Z">April 11, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/context-aware-knowledge-graph-chatbot-with-gpt-4-and-neo4j-d3a99e8ae21e" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>