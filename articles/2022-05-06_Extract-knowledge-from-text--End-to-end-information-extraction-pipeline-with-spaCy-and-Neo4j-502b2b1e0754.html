<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j</h1>
</header>
<section data-field="subtitle" class="p-summary">
Learn how to implement your custom-tailored information extraction pipeline with spaCy and store results in Neo4j
</section>
<section data-field="body" class="e-content">
<section name="db7d" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6a9e" id="6a9e" class="graf graf--h3 graf--leading graf--title">Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j</h3><h4 name="0209" id="0209" class="graf graf--h4 graf-after--h3 graf--subtitle">Learn how to implement your custom-tailored information extraction pipeline with spaCy and store results in Neo4j</h4><p name="0f47" id="0f47" class="graf graf--p graf-after--h4">Since I first dabbled with natural language processing, I have had a special place in my heart for information extraction (IE) pipelines. Information extraction (IE) pipelines extract structured data from unstructured data like text. The internet provides an abundance of information in the form of various articles and other content formats. However, while you might read the news or subscribe to multiple podcasts, it is virtually impossible to keep track of all the new information released daily. Even if you could manually read all the latest reports and articles, it would be incredibly tedious and labor-intensive to structure the data so that you can easily query and aggregate it with your preferred tools. I definitely wouldn’t want to be doing that as my job. Luckily, we can resort to using the latest state-of-the-art NLP techniques to do the information extraction for us automatically.</p><figure name="291f" id="291f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PFUAMubUijcM3ybXaK793g.png" data-width="545" data-height="275" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*PFUAMubUijcM3ybXaK793g.png"><figcaption class="imageCaption">The goal of information extraction pipeline is to extract structured information from unstructured text. Image by the author.</figcaption></figure><p name="56ae" id="56ae" class="graf graf--p graf-after--figure">While I have already <a href="https://towardsdatascience.com/from-text-to-knowledge-the-information-extraction-pipeline-b65e7e30273e#:~:text=What%20exactly%20is%20an%20information,unstructured%20data%20such%20as%20text." data-href="https://towardsdatascience.com/from-text-to-knowledge-the-information-extraction-pipeline-b65e7e30273e#:~:text=What%20exactly%20is%20an%20information,unstructured%20data%20such%20as%20text." class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">implemented and written about an IE pipeline</a>, I’ve noticed many new advancements in open-source NLP models, particularly around <a href="https://spacy.io/" data-href="https://spacy.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">spaCy</a>. I later learned that most of the models I will be using in this post are simply wrapped as a spaCy component, and you could use other libraries if you liked. However, since spaCy was the first NLP library I’ve played around with, I’ve decided to implement the IE pipeline in spaCy as a way of saying thanks to the developers for making such a great and easy to get started tool.</p><p name="3cf0" id="3cf0" class="graf graf--p graf-after--p">How I perceive the steps in the IE pipeline has remained identical over time.</p><figure name="3d38" id="3d38" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5vQAzTDLR_xHEUCgdTBIIw.png" data-width="748" data-height="161" src="https://cdn-images-1.medium.com/max/800/1*5vQAzTDLR_xHEUCgdTBIIw.png"><figcaption class="imageCaption">Steps in Information extraction pipeline. Image by the author.</figcaption></figure><p name="6cb5" id="6cb5" class="graf graf--p graf-after--figure">The input to the IE pipeline is text. That text can come from articles or perhaps internal business documents. If you deal with PDFs or images, you could use computer vision to extract the text. Also, we could use voice2text models to convert audio recordings into text. After preparing the input text, we first run it through the <strong class="markup--strong markup--p-strong">coreference resolution</strong> model. The coreference resolution converts pronouns into referred entities. Most often, the examples for coreference resolution are personal pronouns, where, for example, the model replaces pronouns with the referred person’s name.</p><p name="5471" id="5471" class="graf graf--p graf-after--p">The next step is identifying entities in the text. Which entities we want to recognize is entirely up to the use case we are dealing with and can vary from domain to domain. For example, you will often see NLP models trained to identify people, organizations, and locations. However, in the biomedical domain, we may want to identify concepts like genes, diseases, etc. I’ve also seen some examples where internal documents of a business are being processed in order to construct a knowledge base that could point to documents that contain answers a user might have or even fuel a chatbot. The process of identifying entities in the text is known as <strong class="markup--strong markup--p-strong">named entity recognition</strong>.</p><p name="9f51" id="9f51" class="graf graf--p graf-after--p">Identifying relevant entities in the text is one step, but you almost always need to standardize entities. For example, say that the text references the <em class="markup--em markup--p-em">Theory of relativity</em> and <em class="markup--em markup--p-em">Relativity theory</em>. It might seem obvious that the two entities refer to the same concept. However, as much as it is simple for you, we want to avoid manual labor as much as possible. And a machine does not automatically recognize both references as the same concept. Here is where the <strong class="markup--strong markup--p-strong">named</strong> <strong class="markup--strong markup--p-strong">entity disambiguation</strong> or <strong class="markup--strong markup--p-strong">entity linking</strong> comes into play. The goal of both the named entity disambiguation and entity linking is to assign a unique id to all the entities in the text. With entity linking, extracted entities from the text are mapped to corresponding unique ids from a target knowledge base. In practice, you will see Wikipedia being used as the target knowledge base a lot. However, if you are working in more specific domains or want to process internal business documents, perhaps Wikipedia might not be the best target knowledge base. Remember, an entity must exist in the target knowledge base for the entity linking process to be able to map entities from text to it. If the text mentions you and your manager, and both of you are not on Wikipedia, then it doesn’t make sense to use Wikipedia as the target knowledge base in the entity linking process.</p><p name="78c4" id="78c4" class="graf graf--p graf-after--p">Lastly, the IE pipeline then uses <strong class="markup--strong markup--p-strong">relation extraction</strong> models to identify any relationships between text that are mentioned in the text. If we hold on to the manager example, let’s say we have the following text.</p><blockquote name="55a4" id="55a4" class="graf graf--blockquote graf-after--p">Alicia is the manager of Zach.</blockquote><p name="b0ae" id="b0ae" class="graf graf--p graf-after--blockquote">Ideally, we would want the relation extraction model to recognize the relationship between the two entities.</p><figure name="c98d" id="c98d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jtp5xOU47EtKHaKGoXb0SQ.png" data-width="545" data-height="279" src="https://cdn-images-1.medium.com/max/800/1*jtp5xOU47EtKHaKGoXb0SQ.png"><figcaption class="imageCaption">Extracting relationships between entities. Image by the author.</figcaption></figure><p name="3e4a" id="3e4a" class="graf graf--p graf-after--figure">Most of relation extraction models are trained to recognize multiple types of relationships, so that the information extraction is as rich as possible.</p><p name="87dc" id="87dc" class="graf graf--p graf-after--p">Now that we quickly went over the theory, we can jump to the practical examples.</p><h4 name="eac0" id="eac0" class="graf graf--h4 graf-after--p">Developing IE pipeline in spaCy</h4><p name="a852" id="a852" class="graf graf--p graf-after--h4">There has been much development around spaCy in the last couple of weeks, so I decided to try out the new plugins and use them to construct an information extraction pipeline.</p><p name="e428" id="e428" class="graf graf--p graf-after--p">As always, all the code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/ie_pipeline/SpaCy_informationextraction.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/ie_pipeline/SpaCy_informationextraction.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Github</a>.</p><h4 name="700a" id="700a" class="graf graf--h4 graf-after--p">Coreference resolution</h4><p name="b004" id="b004" class="graf graf--p graf-after--h4">First off, we are going to be using the new <a href="https://spacy.io/universe/project/crosslingualcoreference" data-href="https://spacy.io/universe/project/crosslingualcoreference" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Crosslingual Coreference</a> model contributed by <a href="https://www.linkedin.com/in/david-berenstein-1bab11105/" data-href="https://www.linkedin.com/in/david-berenstein-1bab11105/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">David Berenstein</a> to the s<a href="https://spacy.io/universe" data-href="https://spacy.io/universe" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">paCy Universe</a>. SpaCy Universe is a collection of open-source plugins or addons for spaCy. The cool thing about the spaCy universe project is that it’s straightforward to add the models to our pipeline.</p><figure name="70dd" id="70dd" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/47df70f80fdd65b1d16d5175083572cb.js"></script></figure><p name="db68" id="db68" class="graf graf--p graf-after--figure">That’s it. It only took a couple of lines to set up the coreference model in spaCy. We can now test out the coreference pipeline.</p><figure name="e2fc" id="e2fc" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/4979676e0a0f379fc4cb933c21cfc4b1.js"></script></figure><p name="0465" id="0465" class="graf graf--p graf-after--figure">As you can notice, the personal pronoun <em class="markup--em markup--p-em">He </em>was replaced with the referred person<em class="markup--em markup--p-em"> Christian Drosten. </em>As simple as it seems, this is a vital step to developing an accurate information extraction pipeline. Massive shoutout to <a href="https://www.linkedin.com/in/david-berenstein-1bab11105/" data-href="https://www.linkedin.com/in/david-berenstein-1bab11105/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">David Berenstein</a> for continually developing this project and making our life easier.</p><h4 name="2d36" id="2d36" class="graf graf--h4 graf-after--p">Relation extraction</h4><p name="25e9" id="25e9" class="graf graf--p graf-after--h4">You might wonder why we skipped the named entity recognition and linking step. Well, the reason is that we will be using the <a href="https://github.com/Babelscape/rebel" data-href="https://github.com/Babelscape/rebel" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Rebel</a> project that recognizes both the entities and relations from the text. If I understand correctly, the Rebel project was developed by <a href="https://www.linkedin.com/in/perelluis/" data-href="https://www.linkedin.com/in/perelluis/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pere-Lluís Huguet Cabot</a> as a part of his PhD study with <a href="https://babelscape.com/" data-href="https://babelscape.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Babelscape</a> and Sapienza University. Again, a massive shoutout to Pere for creating such an incredible library with state-of-the-art results for relation extraction. The Rebel model is <a href="https://huggingface.co/Babelscape/rebel-large" data-href="https://huggingface.co/Babelscape/rebel-large" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">available on Hugginface</a> as well as in <a href="https://github.com/Babelscape/rebel/blob/main/spacy_component.py" data-href="https://github.com/Babelscape/rebel/blob/main/spacy_component.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">the form of spaCy component</a>.</p><p name="d727" id="d727" class="graf graf--p graf-after--p">However, the model doesn’t do any entity linking, so we will implement our version of entity linking. We will simply search for entities on WikiData by calling the search entities WikiData API.</p><figure name="634e" id="634e" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/f40e1c91f84c6b8de340290b36c6c92c.js"></script></figure><p name="67b1" id="67b1" class="graf graf--p graf-after--figure">As you can notice from the code, we simply take the entity id from the first result. I’ve been looking at how to improve this and stumbled upon the <a href="https://github.com/SapienzaNLP/extend" data-href="https://github.com/SapienzaNLP/extend" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ExtEnd project</a>. ExtEnd project is a novel approach to entity disambiguation and is available as a demo on <a href="https://huggingface.co/spaces/poccio/ExtEnD" data-href="https://huggingface.co/spaces/poccio/ExtEnD" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Huggingface</a> as well as a s<a href="https://github.com/SapienzaNLP/extend#spacy" data-href="https://github.com/SapienzaNLP/extend#spacy" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">paCy component</a>. I’ve played around with it a bit and managed to get it working using WikiData API for candidates instead of the original AIDA candidates. However, when I wanted to have all three projects (Coref, Rebel, ExtEnd) in the same pipeline, there were some dependency issues as they use a different version of PyTorch, so I gave up for now. I guess I could dockerize the pipeline to solve the dependency issues, but I wanted to have both the ExtEnd and the Rebel in a single spaCy pipeline. However, the ExtEnd code I developed is <a href="https://github.com/tomasonjo/SpaCIE/blob/master/src/extend_component.py" data-href="https://github.com/tomasonjo/SpaCIE/blob/master/src/extend_component.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">available on GitHub</a>, and if someone wants to help me get it working, I am more than happy to accept Pull requests.</p><p name="db07" id="db07" class="graf graf--p graf-after--p">Ok, so for now, we won’t be using the ExtEnd project but will use a simplified version of entity linking by simply taking the first candidate fetched from the WikiData API. The only thing we need to do is incorporate our simplified entity linking solution in the Rebel pipeline. Since the Rebel component is not available directly as a spaCy Universe project, we must copy the <a href="https://github.com/Babelscape/rebel/blob/main/spacy_component.py" data-href="https://github.com/Babelscape/rebel/blob/main/spacy_component.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">component definition from their repository</a> manually. I’ve taken the liberty to implement my version of the <em class="markup--em markup--p-em">set_annotations</em> function in the Rebel spaCy component, while the rest of the code is the same as the original.</p><figure name="52a9" id="52a9" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/f787bb4fdb605fe24c1a3585f2ed070f.js"></script></figure><p name="1cc0" id="1cc0" class="graf graf--p graf-after--figure">The <em class="markup--em markup--p-em">set_annotations</em> function handles how we store the results back to the spaCy’s Doc object. First, we ignore all the self-loops. Self-loops are relationships that start and end at the same entity. Next, we search for both the head and tail entities of the relation in the text using regex. I’ve noticed that the Rebel model sometimes hallucinates some entities which are not in the original text. For that reason, I added a step that verifies that both entities are actually in the text before appending them to the results.</p><p name="9812" id="9812" class="graf graf--p graf-after--p">Lastly, we use the WikiData API to map extracted entities to WikiData ids. As mentioned, this is a simplified version of entity disambiguation and linking, and you can take a more novel approach like the ExtEnd model, for example.</p><p name="9565" id="9565" class="graf graf--p graf-after--p">Now that the Rebel spaCy component is defined, we can create a new spaCy pipeline to handle the relation extraction part.</p><figure name="5e3f" id="5e3f" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/4a9be475c4aa076e41007038f8404992.js"></script></figure><p name="4651" id="4651" class="graf graf--p graf-after--figure">Finally, we can test the relation extraction pipeline on the sample text we’ve used before for coreference resolution.</p><figure name="21c7" id="21c7" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/69bc808c241b0cf77b7ed5c15239882f.js"></script></figure><p name="f570" id="f570" class="graf graf--p graf-after--figure">The Rebel model extracted two relations from the text. For example, it recognized that Christian Drosten, with the WikiData id Q1079331, is employed by Google, which has an id Q95.</p><h4 name="1104" id="1104" class="graf graf--h4 graf-after--p">Storing the information extraction pipeline results</h4><p name="4717" id="4717" class="graf graf--p graf-after--h4">Whenever I hear about relation information between entities, I think of a graph. A graph database is developed to store relations between entities, so what better fit to store the information extraction pipeline results.</p><p name="feab" id="feab" class="graf graf--p graf-after--p">As you might know, I am biased towards <a href="https://neo4j.com/" data-href="https://neo4j.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Neo4j</a>, but you can use whatever tool you like. Here, I will demonstrate how to store the results of my implementation of the information extraction pipeline into Neo4j. We will process a couple of Wikipedia summaries of famous women scientists and store the results as a graph.</p><p name="5708" id="5708" class="graf graf--p graf-after--p">If you want to follow with the code examples, I would suggest you to create a <a href="https://sandbox.neo4j.com/?usecase=blank-sandbox" data-href="https://sandbox.neo4j.com/?usecase=blank-sandbox" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Blank Project in Neo4j Sandbox environment</a>. After you have created the Neo4j Sandbox instance, you can copy the credentials to the code.</p><figure name="f8fc" id="f8fc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*RPDM8WixGQYfQ-JMSdjuiw.png" data-width="709" data-height="474" src="https://cdn-images-1.medium.com/max/800/1*RPDM8WixGQYfQ-JMSdjuiw.png"><figcaption class="imageCaption">Neo4j Sandbox connection details. Image by the author.</figcaption></figure><figure name="58d6" id="58d6" class="graf graf--figure graf--iframe graf-after--figure"><script src="https://gist.github.com/tomasonjo/46c6feeabc31c3114bb740fe7f30e3a7.js"></script></figure><p name="5e85" id="5e85" class="graf graf--p graf-after--figure">Next, we need to define the function that will retrieve Wikipedia summaries for the famous women scientists, run the text through the information extraction pipeline, and finally store the results to Neo4j.</p><figure name="4d6a" id="4d6a" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/9d3e2e95f7ceedfc17dbe8e59868f3b4.js"></script></figure><p name="d43d" id="d43d" class="graf graf--p graf-after--figure">We’ve used the <a href="https://pypi.org/project/wikipedia/" data-href="https://pypi.org/project/wikipedia/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">wikipedia</a> python library to help us fetch the summaries from Wikipedia. Next, we need to define the Cypher statement used to import the information extraction results. I won’t go into details of Cypher syntax, but basically, we first merge the head and tail entities by their WikiData id and then use a procedure from the <a href="https://neo4j.com/labs/apoc/" data-href="https://neo4j.com/labs/apoc/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">APOC library</a> to merge the relationship. I recommend going through courses in the <a href="https://neo4j.com/graphacademy/" data-href="https://neo4j.com/graphacademy/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Neo4j Graph Academy</a> if you are looking for resources to learn more about Cypher syntax.</p><p name="057f" id="057f" class="graf graf--p graf-after--p">Now that we have everything ready, we can go ahead and parse a couple of Wikipedia summaries.</p><figure name="eff9" id="eff9" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/bfff6798a1c62962202eaa13b6f96c5d.js"></script></figure><p name="f388" id="f388" class="graf graf--p graf-after--figure">Once the processing is finished, you can open Neo4j Browser to inspect the results.</p><figure name="7377" id="7377" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5Trme_bRHVd1YBJYGQGSRQ.png" data-width="1220" data-height="773" src="https://cdn-images-1.medium.com/max/800/1*5Trme_bRHVd1YBJYGQGSRQ.png"><figcaption class="imageCaption">Results of the IE pipeline. Image by the author.</figcaption></figure><p name="0526" id="0526" class="graf graf--p graf-after--figure">The results look surprisingly well. In this example, the Rebel model recognized more than 20 relation types between entities ranging from AWARD and EMPLOYER to DRUG_USED_FOR_TREATMENT. Just for fun, I’ll show you the biomedical relation the model extracted.</p><figure name="e37e" id="e37e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*eHusq4OaCkaoPnXbls4zNg.png" data-width="548" data-height="535" src="https://cdn-images-1.medium.com/max/800/1*eHusq4OaCkaoPnXbls4zNg.png"><figcaption class="imageCaption">Extracted biomedical relations. Image by the author.</figcaption></figure><p name="1049" id="1049" class="graf graf--p graf-after--figure">It seems that some of those ladies worked in the biomedical domain. Interestingly, the model identified that acyclovir is used to treat herpes infection, and that azathioprine is an immunosuppressive drug.</p><h4 name="0fd4" id="0fd4" class="graf graf--h4 graf-after--p">Enriching the graph</h4><p name="1887" id="1887" class="graf graf--p graf-after--h4">Since we have mapped our entities to the WikiData ids, we can further use the WikiData API to enrich our graph. I will show you how to extract <strong class="markup--strong markup--p-strong">INSTANCE_OF</strong> relations from WikiData and store them to Neo4j with the help of the APOC library, which allows us to call web APIs and store results in the database.</p><p name="075f" id="075f" class="graf graf--p graf-after--p">To be able to call WikiData API, you need to have a basic understanding of SPARQL syntax, but that is beyond the scope of this blog post. However, I’ve written a <a href="https://towardsdatascience.com/lord-of-the-wiki-ring-importing-wikidata-into-neo4j-and-analyzing-family-trees-da27f64d675e" data-href="https://towardsdatascience.com/lord-of-the-wiki-ring-importing-wikidata-into-neo4j-and-analyzing-family-trees-da27f64d675e" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">post some time about that shows more SPARQL queries used to enrich a Neo4j graph and delves into SPARQL syntax</a>.</p><p name="9552" id="9552" class="graf graf--p graf-after--p">By executing the following query, we add the <strong class="markup--strong markup--p-strong">Class</strong> nodes to the graph and link them with the appropriate entities.</p><figure name="d678" id="d678" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/c19061941ac762071469a830f8eaf19e.js"></script></figure><p name="f582" id="f582" class="graf graf--p graf-after--figure">Now we can inspect the results of the enrichment in Neo4j Browser.</p><figure name="da89" id="da89" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*X4aEwpJJBfyb7LbznDuKcw.png" data-width="1229" data-height="794" src="https://cdn-images-1.medium.com/max/800/1*X4aEwpJJBfyb7LbznDuKcw.png"><figcaption class="imageCaption">Entities enriched by their class from WikiData API. Image by the author.</figcaption></figure><h4 name="9fe4" id="9fe4" class="graf graf--h4 graf-after--figure">Conclusion</h4><p name="77ed" id="77ed" class="graf graf--p graf-after--h4">I really like what s<a href="https://spacy.io/" data-href="https://spacy.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">paCy</a> is doing lately and all the open-source projects around it. I noticed that various open-source projects are primarily standalone, and it can be tricky to combine multiple models into a single spaCy pipeline. For example, you can see that we had to have two pipelines in this project, one for coreference resolution and one for relation extraction and entity linking.</p><p name="2e5e" id="2e5e" class="graf graf--p graf-after--p">As for the results of the IE pipeline, I am delighted with how well it turned out. As you can observe in the <a href="https://github.com/Babelscape/rebel" data-href="https://github.com/Babelscape/rebel" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Rebel repository</a>, their solution is state-of-the-art on many NLP datasets, so it is not a big surprise that the results are so good. The only weak link in my implementation is the Entity Linking step. As I said, it would probably greatly benefit from adding something like the <a href="https://github.com/SapienzaNLP/extend" data-href="https://github.com/SapienzaNLP/extend" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ExtEnd</a> library for more accurate entity disambiguation and linking. Perhaps that’s something I’ll do next time.</p><p name="cb34" id="cb34" class="graf graf--p graf-after--p">Try out the IE implementation, and please let me know what you think or if you have some ideas for improvements. There are a ton of opportunities to make this pipeline better!</p><p name="25f4" id="25f4" class="graf graf--p graf-after--p graf--trailing">As always, the code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/ie_pipeline/SpaCy_informationextraction.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/ie_pipeline/SpaCy_informationextraction.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/502b2b1e0754"><time class="dt-published" datetime="2022-05-06T14:40:14.885Z">May 6, 2022</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>