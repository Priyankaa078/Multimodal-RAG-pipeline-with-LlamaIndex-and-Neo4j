<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Analyze Your Website with NLP and Knowledge Graphs</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Analyze Your Website with NLP and Knowledge Graphs</h1>
</header>
<section data-field="subtitle" class="p-summary">
Combine various NLP techniques to construct a knowledge graph representing your website
</section>
<section data-field="body" class="e-content">
<section name="d7e0" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="9945" id="9945" class="graf graf--h3 graf--leading graf--title">Analyze Your Website with NLP and Knowledge Graphs</h3><h4 name="8408" id="8408" class="graf graf--h4 graf-after--h3 graf--subtitle">Combine various NLP techniques to construct a knowledge graph representing your website</h4><p name="a18c" id="a18c" class="graf graf--p graf-after--h4">A website is a reflection of the company. For the most part, it is used to inform users about various products and services and drive sales. However, the website grows and changes over time and many minor and major changes are introduced. As a result, it is not uncommon to end up with a disorganized website that fails to accomplish its original mission. Therefore, it makes sense to regularly evaluate the structure and content of the website to make it as optimized as possible. Optimizing websites is a huge business, and consequently, there are multiple commercial tools to help you with SEO and other suggestions. However, I will show you how you can create a comprehensive and detailed representation of the content on your website with a little bit of coding knowledge, which will allow you to analyze and improve it.</p><p name="1e32" id="1e32" class="graf graf--p graf-after--p">You can extract the structure of the website using any of the available web scrapers. Additionally, it makes sense to not only evaluate the structure but also the content of the website by utilizing various natural language processing techniques. Since most websites are copyrighted, I have decided to use the Neo4j documentation website as an example in this tutorial. The content of the documentation website is available under the <a href="https://neo4j.com/docs/license/" data-href="https://neo4j.com/docs/license/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CC 4.0 license</a>. However, you can apply a similar workflow to any web page you desire.</p><figure name="202d" id="202d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_Mdv-G6qvFdMHCqTI65ZGw.png" data-width="1049" data-height="454" src="https://cdn-images-1.medium.com/max/800/1*_Mdv-G6qvFdMHCqTI65ZGw.png"><figcaption class="imageCaption">Extracting information from documentation to construct a knowledge graph. Image by the author.</figcaption></figure><p name="8d11" id="8d11" class="graf graf--p graf-after--figure">It might seem a bit magical (if you ignore my arrows) how you might construct a knowledge graph using information from your website. Throughout this post, I aim to bring more clarity to information extraction and provide you with tools you can use on your own. I have used similar approaches with <a href="https://towardsdatascience.com/construct-a-biomedical-knowledge-graph-with-nlp-1f25eddc54a0" data-href="https://towardsdatascience.com/construct-a-biomedical-knowledge-graph-with-nlp-1f25eddc54a0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">medical documents</a>, <a href="https://medium.com/neo4j/making-sense-of-news-the-knowledge-graph-way-d33810ce5005" data-href="https://medium.com/neo4j/making-sense-of-news-the-knowledge-graph-way-d33810ce5005" class="markup--anchor markup--p-anchor" target="_blank">news</a>, or even <a href="https://medium.com/neo4j/monitoring-the-cryptocurrency-space-with-nlp-and-knowledge-graphs-92a1cfaebd1a" data-href="https://medium.com/neo4j/monitoring-the-cryptocurrency-space-with-nlp-and-knowledge-graphs-92a1cfaebd1a" class="markup--anchor markup--p-anchor" target="_blank">crypto reports</a>, and now we’ll analyze a website with the help of NLP and knowledge graphs.</p><h4 name="2038" id="2038" class="graf graf--h4 graf-after--p">Data collection and modeling workflow</h4><figure name="f063" id="f063" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*d6OBoC0c7pA_xp9MvKWlFw.png" data-width="931" data-height="431" src="https://cdn-images-1.medium.com/max/800/1*d6OBoC0c7pA_xp9MvKWlFw.png"><figcaption class="imageCaption">Data collection and modeling workflow. Image by the author.</figcaption></figure><p name="b90f" id="b90f" class="graf graf--p graf-after--figure">The data collection and preprocessing consist of three parts.</p><ul class="postList"><li name="bbad" id="bbad" class="graf graf--li graf-after--p">Web scraper: A Python script that walks through the documentation web page and collects links and text</li><li name="0b0b" id="0b0b" class="graf graf--li graf-after--li">NLP pipeline: Extracts keywords from text and calculate text embeddings to detect similar/duplicate content</li><li name="2168" id="2168" class="graf graf--li graf-after--li">Knowledge graph: Store results as a knowledge graph for further analysis</li></ul><p name="88f6" id="88f6" class="graf graf--p graf-after--li">The code for the data collection and preprocessing is available <a href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Preprocessing.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Preprocessing.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">on GitHub as a Jupyter notebook</a>.</p><p name="4b2d" id="4b2d" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">You don’t have to run the data collection and processing yourself since it takes a couple of hours. I have prepared a </em><a href="https://drive.google.com/file/d/1vpQe2gfoPiUVXQpUq-J6Fsjxq5BamyXM/view?usp=sharing" data-href="https://drive.google.com/file/d/1vpQe2gfoPiUVXQpUq-J6Fsjxq5BamyXM/view?usp=sharing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Neo4j dump</em></a><em class="markup--em markup--p-em"> that you can use if you want to follow along with the analysis later in the post.</em></p><p name="6d4b" id="6d4b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Web scraper</strong></p><p name="13f0" id="13f0" class="graf graf--p graf-after--p">I usually use <a href="https://selenium-python.readthedocs.io/" data-href="https://selenium-python.readthedocs.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Python Selenium</a> for web scraping, but you can use any of other libraries or languages you want to extract relevant information from websites. I won’t go into too many details about the code, as the goal of this post is not to teach you how to scrape websites. However, you can examine the <a href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Preprocessing.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Preprocessing.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Jupyter notebook</a> that handles the web scraping.</p><p name="d3dd" id="d3dd" class="graf graf--p graf-after--p">Specifically for the Neo4j documentation website, I avoided scraping the links from the left and top navigation bars as that would introduce much noise in the graph since most of the pages have the same navigation bars present.</p><figure name="351d" id="351d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nXJQzqjSkkqAJLVMZBiFog.png" data-width="1344" data-height="513" src="https://cdn-images-1.medium.com/max/800/1*nXJQzqjSkkqAJLVMZBiFog.png"><figcaption class="imageCaption">Links from navigation bars are ignored during scraping. Image by the author.</figcaption></figure><p name="4195" id="4195" class="graf graf--p graf-after--figure">With the Neo4j documentation website, I wanted to capture how a user could traverse the documentation without using the navigation bar. Otherwise, we would introduce noise in the knowledge graph as all the pages would be linking to the same pages in the navigation bars. Additionally, I have focused on extracting text and links from only documentation web pages, so some product or marketing pages were not scraped for their content.</p><p name="1046" id="1046" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Natural language processing</strong></p><p name="f484" id="f484" class="graf graf--p graf-after--p">The natural language processing step involves extracting keywords and calculating text embeddings to detect similar and duplicate content. Before even considering training your own NLP model, it is always a good idea to check the <a href="https://huggingface.co/models" data-href="https://huggingface.co/models" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">HuggingFace model repository</a> and see if any publically available models are a good fit for your use case.</p><p name="2a6b" id="2a6b" class="graf graf--p graf-after--p">After a bit of research, I have found a <a href="https://huggingface.co/yanekyuk/bert-uncased-keyword-extractor" data-href="https://huggingface.co/yanekyuk/bert-uncased-keyword-extractor" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">keyword extraction model</a> made available by <strong class="markup--strong markup--p-strong">Yankı Ekin Yüksel </strong>that we will use. I really love how simple it is to load and run a model using transformers and HuggingFace.</p><p name="c0df" id="c0df" class="graf graf--p graf-after--p">The following code loads the keyword extraction model and prepares a NLP pipeline.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="a3a7" id="a3a7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;yanekyuk/bert-uncased-keyword-extractor&quot;</span>)<br />model = AutoModelForTokenClassification.from_pretrained(<br />    <span class="hljs-string">&quot;yanekyuk/bert-uncased-keyword-extractor&quot;</span><br />)<br /><br />nlp = pipeline(<span class="hljs-string">&quot;ner&quot;</span>, model=model, tokenizer=tokenizer)</span></pre><p name="b5b4" id="b5b4" class="graf graf--p graf-after--pre">You don’t have to download models or worry about file paths. Instead, you can simply define the model name as the argument of the tokenizer and the model, and the transformers library does all the work for you.</p><p name="5b12" id="5b12" class="graf graf--p graf-after--p">The pipeline returns tokens, which are not necessarily a word. Therefore, we need to construct the words back from tokens after the NLP pipeline finishes.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="eb4a" id="eb4a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">extract_keywords</span>(<span class="hljs-params">text</span>):<br />    <span class="hljs-string">&quot;&quot;&quot;<br />    Extract keywords and construct them back from tokens<br />    &quot;&quot;&quot;</span><br />    result = <span class="hljs-built_in">list</span>()<br />    keyword = <span class="hljs-string">&quot;&quot;</span><br />    <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> nlp(text):<br />        <span class="hljs-keyword">if</span> token[<span class="hljs-string">&#x27;entity&#x27;</span>] == <span class="hljs-string">&#x27;I-KEY&#x27;</span>:<br />            keyword += token[<span class="hljs-string">&#x27;word&#x27;</span>][<span class="hljs-number">2</span>:] <span class="hljs-keyword">if</span> \<br />              token[<span class="hljs-string">&#x27;word&#x27;</span>].startswith(<span class="hljs-string">&quot;##&quot;</span>) <span class="hljs-keyword">else</span> <span class="hljs-string">f&quot; <span class="hljs-subst">{token[<span class="hljs-string">&#x27;word&#x27;</span>]}</span>&quot;</span><br />        <span class="hljs-keyword">else</span>:<br />            <span class="hljs-keyword">if</span> keyword:<br />                result.append(keyword)<br />            keyword = token[<span class="hljs-string">&#x27;word&#x27;</span>]<br />    <span class="hljs-comment"># Add the last keyword</span><br />    result.append(keyword)<br />    <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(result))<br />    <br />extract_keywords(<span class="hljs-string">&quot;&quot;&quot;<br />Broadcom agreed to acquire cloud computing company VMware in a $61 billion (€57bn) cash-and stock deal.<br />&quot;&quot;&quot;</span>) <span class="hljs-comment"># [&#x27;cloud computing&#x27;, &#x27;vmware&#x27;, &#x27;broadcom&#x27;]</span></span></pre><p name="1abf" id="1abf" class="graf graf--p graf-after--pre">The example shows that the model extracted <strong class="markup--strong markup--p-strong">cloud computing</strong>, <strong class="markup--strong markup--p-strong">vmware</strong>, and <strong class="markup--strong markup--p-strong">broadcom</strong> from the given text. The results seem fitting for our use case as we are analyzing the Neo4j documentation, which should contain many technological keywords.</p><p name="0a34" id="0a34" class="graf graf--p graf-after--p">Next, we also need to calculate text embeddings that will help us identify similar and duplicate content. Again, I’ve search the HuggingFace model repository a bit and came across a <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" data-href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">sentence transformers model</a> that can be used to identify similar sentences or paragraphs. Again, the model can be loaded and used with as little as three lines of codes.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="9ade" id="9ade" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer<br /><br />model = SentenceTransformer(<span class="hljs-string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>)<br /><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_embeddings</span>(<span class="hljs-params">text</span>):<br />    embeddings = model.encode(text)<br />    <span class="hljs-keyword">return</span> [<span class="hljs-built_in">float</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> embeddings.tolist()]</span></pre><p name="b6bc" id="b6bc" class="graf graf--p graf-after--pre">We need to convert the result to a float of lists as Neo4j Driver doesn’t support NumPy arrays.</p><p name="b6ad" id="b6ad" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Knowledge graph construction</strong></p><p name="ec40" id="ec40" class="graf graf--p graf-after--p">After the web scraper and natural language processing steps are done, we can go ahead and construct a knowledge graph. You might have already guessed that we are going to be using Neo4j to store our knowledge graph. You can use a <a href="https://neo4j.com/cloud/platform/aura-graph-database/" data-href="https://neo4j.com/cloud/platform/aura-graph-database/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">free cloud instance</a> or setup a <a href="https://neo4j.com/download/" data-href="https://neo4j.com/download/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">local environment</a>.</p><p name="dbf7" id="dbf7" class="graf graf--p graf-after--p">The graph schema after the initial import is defined as the following.</p><figure name="625f" id="625f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lLGnM6JNPyW2MG-SBHfYMg.png" data-width="515" data-height="261" src="https://cdn-images-1.medium.com/max/800/1*lLGnM6JNPyW2MG-SBHfYMg.png"><figcaption class="imageCaption">Initial graph schema. Image by the author.</figcaption></figure><p name="218f" id="218f" class="graf graf--p graf-after--figure">In the center of our graph are web pages. We know their URL address, text embedding value, and whether or not the web scraper extracted text from the page. Pages can also link or redirect to other pages, which is represented with according relationship. As a part of the NLP workflow, we have also detected keywords on the site, which we will store as separate nodes.</p><p name="4529" id="4529" class="graf graf--p graf-after--p">Check out the <a href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Preprocessing.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Preprocessing.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">preprocessing notebook</a> if you are interested in the code implementation of the data import. Otherwise, we will jump straight to the network analysis part.</p><h4 name="4c07" id="4c07" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Network Analysis</strong></h4><p name="2b6d" id="2b6d" class="graf graf--p graf-after--h4"><em class="markup--em markup--p-em">I have prepared a </em><a href="https://drive.google.com/file/d/1vpQe2gfoPiUVXQpUq-J6Fsjxq5BamyXM/view?usp=sharing" data-href="https://drive.google.com/file/d/1vpQe2gfoPiUVXQpUq-J6Fsjxq5BamyXM/view?usp=sharing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Neo4j database dump</em></a><em class="markup--em markup--p-em"> if you don’t want to scrape the Neo4j documentation but would still like to follow the network analysis examples.</em></p><figure name="ad50" id="ad50" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1sxldXGPyk_M5CKjR-Rjzw.png" data-width="1078" data-height="860" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*1sxldXGPyk_M5CKjR-Rjzw.png"><figcaption class="imageCaption">Sample subgraph of the Neo4j documentation website knowledge graph. Image by the author.</figcaption></figure><p name="530e" id="530e" class="graf graf--p graf-after--figure">I will walk you through some website network analysis examples that I found interesting. We will be using the <a href="https://neo4j.com/docs/graph-data-science/current/python-client/" data-href="https://neo4j.com/docs/graph-data-science/current/python-client/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Graph Data Science Python</a> client, which is ideal tool to perform network analysis with Neo4j from Python.</p><p name="8693" id="8693" class="graf graf--p graf-after--p">The Jupyter Notebook with all the relevant code for the network analysis is <a href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Analysis.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/Analysis.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">available on GitHub</a>.</p><p name="95de" id="95de" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Overall statistics</strong></p><p name="12b1" id="12b1" class="graf graf--p graf-after--p">First of, we will begin by evaluating the size of our dataset by counting the number of nodes and relationships with the <code class="markup--code markup--p-code">apoc.meta.stats</code> procedure.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="d042" id="d042" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.run_cypher(<span class="hljs-string">&quot;&quot;&quot;<br />CALL apoc.meta.stats()<br />YIELD labels, relTypesCount<br />&quot;&quot;&quot;</span>)</span></pre><p name="b05c" id="b05c" class="graf graf--p graf-after--pre">Our knowledge graph has 15370 pages and 4199 keywords, along with 62365 links and 723 redirects. I was not expecting this many pages. However, considering that the documentation covers multiple products across multiple versions, it makes sense that the number of pages on a website can explode. Additionally, many links point to pages outside the Neo4j website.</p><p name="328d" id="328d" class="graf graf--p graf-after--p">Next, we will evaluate from how many pages we successfuly retrieved content information.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="3885" id="3885" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.run_cypher(<span class="hljs-string">&quot;&quot;&quot;<br />MATCH (p:Page)<br />RETURN p.has_text AS has_text,<br />       count(*) AS count<br />&quot;&quot;&quot;</span>)</span></pre><p name="2913" id="2913" class="graf graf--p graf-after--pre">We have successfully retrieved the content and calculated text embeddings from 9688 web pages. The web scraper focused on recovering content from the documentation website while mostly ignoring the structure and text of the product and similar pages. Therefore, there are 2972 on the Neo4j website for which we haven’t retrieved content. Finally, the Neo4j website links to 2710 outside its primary domain. Pages outside of the Neo4j documentation website were explicitly ignored during web scraping.</p><p name="fcc7" id="fcc7" class="graf graf--p graf-after--p">Out of curiosity, we can list ten random outside web pages that Neo4j links to the most.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="385a" id="385a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.run_cypher(<span class="hljs-string">&quot;&quot;&quot;<br />MATCH (p:Page)<br />WHERE p.has_text IS NULL<br />RETURN p.url AS page,<br />       count{(p)&lt;-[:LINKS_TO|REDIRECTS]-()} AS links<br />ORDER BY links DESC<br />LIMIT 5<br />&quot;&quot;&quot;</span>)</span></pre><p name="391e" id="391e" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="a2d8" id="a2d8" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/2f123d3dc96ae0a024cbb7dce6a7ef0e.js"></script></figure><p name="b28b" id="b28b" class="graf graf--p graf-after--figure">The most linked web page is actually a localhost URL, which is the default address of the Neo4j Browser. Following are some links to APOC releases on GitHub. Finally, it seems that Neo4j has some products or services that support integrations with Microsoft NLP and AWS cloud APIs, as otherwise, they probably wouldn’t link them in their documentation.</p><p name="6d17" id="6d17" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Identify dead links</strong></p><p name="9e0f" id="9e0f" class="graf graf--p graf-after--p">We will proceed by identifying dead or broken links. A broken link is a link that points to a non-existing web page. Like most websites, Neo4j documentation has a designated 404 web page. The web scraper assigns a “404” text value to any URL that responds with a 404 page.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="c4ef" id="c4ef" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.run_cypher(<span class="hljs-string">&quot;&quot;&quot;<br />MATCH (:Page)-[:LINKS_TO|REDIRECTS]-&gt;(:Page{is_404:true})<br />RETURN count(*) AS brokenLinkCount<br />&quot;&quot;&quot;</span>)</span></pre><p name="4866" id="4866" class="graf graf--p graf-after--pre">There are 241 broken links in the dataset. The broken link number sounds small, given that there is a total of 62 thousand links in the database. However, if you performed this analysis on your website, you could always forward the results to the appropriate teams to get the links fixed.</p><p name="79bb" id="79bb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Finding shortest paths</strong></p><p name="d5cd" id="d5cd" class="graf graf--p graf-after--p">Most websites are designed to gently push the users along the journey to the end goal. For example, if you are running an e-commerce website, the goal is probably a purchase event. With Neo4j, you can analyze all the paths a user might follow to reach the desired destination. Since we are dealing with a documentation website, we can’t explore how a user might end up completing a purchase on the website. However, we can apply the same techniques and evaluate the shortest paths between various parts of the website.</p><p name="2507" id="2507" class="graf graf--p graf-after--p">The following code finds all the shortest paths between the two given web pages.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="less" name="1e98" id="1e98" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-selector-tag">gds</span><span class="hljs-selector-class">.run_cypher</span>(<span class="hljs-string">&quot;&quot;</span>&quot;<br />MATCH (<span class="hljs-attribute">start</span>:Page {<span class="hljs-attribute">url</span>:<span class="hljs-string">&quot;https://neo4j.com/docs&quot;</span>}), <br />      (<span class="hljs-attribute">end</span>:Page {<span class="hljs-attribute">url</span>:<span class="hljs-string">&quot;https://console.neo4j.io&quot;</span>})<br />MATCH p=<span class="hljs-built_in">shortestPath</span>((start)-[:LINKS_TO|REDIRECTS*..<span class="hljs-number">10</span>]-&gt;(end))<br />RETURN [n in <span class="hljs-built_in">nodes</span>(p) | n.url] AS path<br /><span class="hljs-string">&quot;&quot;</span>&quot;)</span></pre><p name="c6c5" id="c6c5" class="graf graf--p graf-after--pre">The results show that a user must traverse the following web pages in order to reach the Aura console page from the documentation home:</p><ul class="postList"><li name="865b" id="865b" class="graf graf--li graf-after--p"><a href="https://neo4j.com/docs," data-href="https://neo4j.com/docs," class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://neo4j.com/docs</a></li><li name="e866" id="e866" class="graf graf--li graf-after--li"><a href="https://neo4j.com/docs/aura/auradb," data-href="https://neo4j.com/docs/aura/auradb," class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://neo4j.com/docs/aura/auradb</a></li><li name="a745" id="a745" class="graf graf--li graf-after--li"><a href="https://neo4j.com/docs/aura/auradb/getting-started/create-database," data-href="https://neo4j.com/docs/aura/auradb/getting-started/create-database," class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://neo4j.com/docs/aura/auradb/getting-started/create-database</a></li><li name="a4aa" id="a4aa" class="graf graf--li graf-after--li"><a href="https://console.neo4j.io" data-href="https://console.neo4j.io" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">https://console.neo4j.io</a></li></ul><p name="34e9" id="34e9" class="graf graf--p graf-after--li">Representing your website as a knowledge graph can significantly improve the understanding of designed flows throughout your web page, which in turn can help you optimize them.</p><p name="eaf6" id="eaf6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Link analysis</strong></p><p name="2533" id="2533" class="graf graf--p graf-after--p">Next, we will use centrality algorithms to rank the importance of the web pages. For example, let’s say we simply define the rank of web pages as the number of incoming links. In that case, we can utilize the Degree centrality algorithm to rank the importance of the web pages.</p><p name="330f" id="330f" class="graf graf--p graf-after--p">To execute any graph algorithms from the <a href="https://neo4j.com/product/graph-data-science/" data-href="https://neo4j.com/product/graph-data-science/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Graph Data Science library</a>, we have first to project an in-memory graph.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="e9d1" id="e9d1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">G, metadata = gds.graph.project(<span class="hljs-string">&#x27;structure&#x27;</span>, <span class="hljs-string">&#x27;Page&#x27;</span>, <br />  [<span class="hljs-string">&#x27;LINKS_TO&#x27;</span>, <span class="hljs-string">&#x27;REDIRECTS&#x27;</span>])</span></pre><p name="01ca" id="01ca" class="graf graf--p graf-after--pre">With the Graph Data Science library projection, you have the option to choose a specific subgraph of your knowledge graph you want to evaluate with graph algorithms. In this example, we selected the <strong class="markup--strong markup--p-strong">Page</strong> nodes and <strong class="markup--strong markup--p-strong">LINKS_TO</strong> and <strong class="markup--strong markup--p-strong">REDIRECTS</strong> relationships. For simplicity’s sake, we will treat the links and redirects as identical. However, for more in-depth network analysis, we could define some weights and perhaps treat redirects as more important than links.</p><p name="8903" id="8903" class="graf graf--p graf-after--p">The following code will calculate the incoming degree centrality, which is simply the number of incoming links or redirects a web page has.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f0c0" id="f0c0" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">df = gds.degree.stream(G, orientation=<span class="hljs-string">&quot;REVERSE&quot;</span>)<br />df[<span class="hljs-string">&quot;url&quot;</span>] = [d[<span class="hljs-string">&quot;url&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> gds.util.asNodes(df[<span class="hljs-string">&quot;nodeId&quot;</span>].to_list())]<br />df.sort_values(<span class="hljs-string">&quot;score&quot;</span>, ascending=<span class="hljs-literal">False</span>, inplace=<span class="hljs-literal">True</span>)<br />df.head()</span></pre><p name="8f93" id="8f93" class="graf graf--p graf-after--pre">One thing to note here is that we are using the Graph Data Science Python Client to interact with the database, so the syntax might be slightly different if you are used to Cypher procedure calls. The results are the following:</p><figure name="067c" id="067c" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/c526dba30995241f2ea8633b900d8575.js"></script></figure><p name="a3ec" id="a3ec" class="graf graf--p graf-after--figure">The developer knowledge base page has 598 incoming links. Many links also point to specific tags of the developer blog and graph gists. I would assume that tons of documentation sites point to specific examples which can be found in blogs of graph gists. If we were to understand the intended flow more, we could drill down where the links are coming from and so on.</p><p name="17f1" id="17f1" class="graf graf--p graf-after--p">Sometimes the number of incoming links is not a sufficient ranking metric. The founders of Google were aware of this issue as they derived the most famous graph algorithm, PageRank, which takes into account the number of incoming links and where they are coming from. For example, there is a difference if the web page has a direct link from the home page or some periphery documentation page that only some people visit.</p><p name="2e40" id="2e40" class="graf graf--p graf-after--p">The following code will calculate the PageRank score and merge it with the degree dataframe.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="8495" id="8495" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">pr_df = gds.pageRank.stream(G)<br />pr_df[<span class="hljs-string">&quot;pagerank&quot;</span>] = pr_df.pop(<span class="hljs-string">&quot;score&quot;</span>)<br />combined_df = df.merge(pr_df, on=<span class="hljs-string">&quot;nodeId&quot;</span>)<br />combined_df.sort_values(<span class="hljs-string">&quot;pagerank&quot;</span>, ascending=<span class="hljs-literal">False</span>, inplace=<span class="hljs-literal">True</span>)</span></pre><p name="3b16" id="3b16" class="graf graf--p graf-after--pre">Now we can examine the top five most important web pages judging by the PageRank score.</p><figure name="19a3" id="19a3" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/f21406b7c7b3da094cd08c23075cc07c.js"></script></figure><p name="468f" id="468f" class="graf graf--p graf-after--figure">The score column represent the number of incoming links and redirects, while the pagerank columns hold the PageRank score.</p><p name="2fdb" id="2fdb" class="graf graf--p graf-after--p">Interestingly, only the developer knowledge base page retains its position when using PageRank instead of Degree centrality. It seems that GraphConnect was vital as it is still the second most important web page. As a web UX designer, you might take this information and try to change the structure of the website so that perhaps the latest GraphConnect would be more important. Remember, we are only scratching the surface with this network analysis. However, you could find interesting patterns and then drill down to understand the web page flow and optimize it.</p><p name="bc67" id="bc67" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Keyword analysis and co-occurrence topic clustering</strong></p><p name="991e" id="991e" class="graf graf--p graf-after--p">In the last part of this analysis, we will take a look at the keyword analysis.</p><figure name="1796" id="1796" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-Vx85FymsjzIS8pWXN7L-w.png" data-width="1099" data-height="832" src="https://cdn-images-1.medium.com/max/800/1*-Vx85FymsjzIS8pWXN7L-w.png"><figcaption class="imageCaption">Graph representation of web pages and their keywords. Image by the author.</figcaption></figure><p name="cd8c" id="cd8c" class="graf graf--p graf-after--figure">Having the right keywords on the right web pages is one of the critical aspects of search engine optimization. We can get a high-level overview of the page by examining its most frequent keywords.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="f64e" id="f64e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.run_cypher(<span class="hljs-string">&quot;&quot;&quot;<br />MATCH (k:Keyword)<br />RETURN k.name AS keyword,<br />       count {(k)&lt;-[:HAS_KEYWORD]-()} AS mentions<br />ORDER BY mentions DESC<br />LIMIT 5<br />&quot;&quot;&quot;</span>)</span></pre><p name="8b2f" id="8b2f" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="881c" id="881c" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/67b3a1e7abffdbb8b3e4da53cc0f0cd3.js"></script></figure><p name="1205" id="1205" class="graf graf--p graf-after--figure">The results look exactly what we might expect. The web page talks about nodes, neo4j, graphs, and Java. Not sure why the clipboard is there. Perhaps there are a lot of “Copy to clipboard” sections throughout the documentation.</p><p name="58fa" id="58fa" class="graf graf--p graf-after--p">We can drill down a bit and look at the most frequent keywords for web pages where the “graph-data-science” is present in the URL address. This way, we filter primarily for Neo4j Graph Data Science library documentation.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5d97" id="5d97" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.run_cypher(<span class="hljs-string">&quot;&quot;&quot;<br />MATCH (p:Page)-[:HAS_KEYWORD]-&gt;(k:Keyword)<br />WHERE p.url CONTAINS &quot;graph-data-science&quot;<br />RETURN k.name AS keyword,<br />       count(*) AS mentions<br />ORDER BY mentions DESC<br />LIMIT 5<br />&quot;&quot;&quot;</span>)</span></pre><p name="b884" id="b884" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="ce6f" id="ce6f" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/b59d22f379c27bd52f6746f1dbf3244f.js"></script></figure><p name="e10e" id="e10e" class="graf graf--p graf-after--figure">It looks very similar to the overall keyword presence, except the keyword “algorithm” shows up more frequently here. Now, we could go ahead and drill down keywords by other sections or by individual pages. A knowledge graph is a fantastic tool for either drill-down analysis or to analyze the distribution of keywords and content through designated user flows. Additionally, if you used an NLP model that is able to detect both short- and long-tail keywords, it would greatly help with any SEO analysis and optimization.</p><p name="9797" id="9797" class="graf graf--p graf-after--p">Lastly, we can also perform a keyword co-occurrence clustering with only a couple of lines of code. A keyword co-occurrence clustering can be understood as a task of identifying topics, where the topics consist of multiple keywords that frequently co-occur in the text corpus.</p><figure name="5442" id="5442" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9m3BbjLZjs7qtB2mXiJb3Q.png" data-width="861" data-height="591" src="https://cdn-images-1.medium.com/max/800/1*9m3BbjLZjs7qtB2mXiJb3Q.png"><figcaption class="imageCaption">Diagram of the co-occurrence topic clustering output. Image by the author.</figcaption></figure><p name="a3e1" id="a3e1" class="graf graf--p graf-after--figure">The workflow of the keyword co-occurrence clustering or topic clustering in Neo4j is the following:</p><ol class="postList"><li name="082c" id="082c" class="graf graf--li graf-after--p">Project an in-memory graph with relevant information.</li><li name="56fd" id="56fd" class="graf graf--li graf-after--li">Create a CO_OCCUR relationship between keyword that commonly appear together in text.</li><li name="df7d" id="df7d" class="graf graf--li graf-after--li">Run a community detection algorithm like the Louvain method to identify communities or clusters of keywords.</li></ol><p name="59e4" id="59e4" class="graf graf--p graf-after--li">We will begin by projecting an in-memory graph with all the relevant information. We need to project both the Page and the Keyword nodes along with the connecting HAS_KEYWORD relationship. We need to reverse the relationship orientation in the graph projection as we want to examine clusters of co-occurring keywords and not groups of similar web pages.</p><p name="b7d0" id="b7d0" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">P.s. If you leave the natural orientation and follow the examples you will identify clusters of similar web pages based on the keywords they mention</em></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6f2f" id="6f2f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">G, metadata = gds.graph.project(<br />    <span class="hljs-string">&quot;keywords&quot;</span>, [<span class="hljs-string">&quot;Page&quot;</span>, <span class="hljs-string">&quot;Keyword&quot;</span>], {<span class="hljs-string">&quot;HAS_KEYWORD&quot;</span>: {<span class="hljs-string">&quot;orientation&quot;</span>: <span class="hljs-string">&quot;REVERSE&quot;</span>}}<br />)</span></pre><p name="8dd6" id="8dd6" class="graf graf--p graf-after--pre">Next, we need to create a <strong class="markup--strong markup--p-strong">CO_OCCUR</strong> relationship between keywords frequently appearing together on web pages. To solve this task, we will use the Node Similarity algorithm. The Node Similarity uses the <a href="https://en.wikipedia.org/wiki/Jaccard_index" data-href="https://en.wikipedia.org/wiki/Jaccard_index" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Jaccard similarity coefficient</a> by default to calculate the similarity between two nodes.</p><p name="3d9a" id="3d9a" class="graf graf--p graf-after--p">In this example, each keyword has a set of web pages it is mentioned in. If the Jaccard coefficient between a pair of keywords based on the web pages they appear in is greater than 0.40, then a new <strong class="markup--strong markup--p-strong">CO_OCCUR</strong> relationship is created between them. We use the mutate mode to store the algorithm’s results back to the in-memory projected graph.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0ef8" id="0ef8" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.nodeSimilarity.mutate(<br />    G, mutateRelationshipType=<span class="hljs-string">&quot;CO_OCCUR&quot;</span>, mutateProperty=<span class="hljs-string">&quot;score&quot;</span>, <br />    similarityCutoff=<span class="hljs-number">0.4</span><br />)</span></pre><p name="4d6f" id="4d6f" class="graf graf--p graf-after--pre">Finally, we will use the Louvain method algorithm, a community detection algorithm, to identify clusters of keywords. The algorithm outputs each node and its community id. Therefore, we need to group the results by their community id to create a list of keywords that form a topic or a cluster.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="3f15" id="3f15" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">topic_df = gds.louvain.stream(G, nodeLabels=[<span class="hljs-string">&quot;Keyword&quot;</span>], relationshipTypes=[<span class="hljs-string">&quot;CO_OCCUR&quot;</span>])<br />topic_df[<span class="hljs-string">&quot;keyword&quot;</span>] = [<br />    n[<span class="hljs-string">&quot;name&quot;</span>] <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> gds.util.asNodes(topic_df[<span class="hljs-string">&quot;nodeId&quot;</span>].to_list())<br />]<br />topic_df.groupby(<span class="hljs-string">&quot;communityId&quot;</span>).agg(<br />    {<span class="hljs-string">&quot;keyword&quot;</span>: [<span class="hljs-string">&quot;size&quot;</span>, <span class="hljs-built_in">list</span>]}<br />).reset_index().sort_values([(<span class="hljs-string">&quot;keyword&quot;</span>, <span class="hljs-string">&quot;size&quot;</span>)], ascending=<span class="hljs-literal">False</span>).head()</span></pre><p name="2773" id="2773" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="73b5" id="73b5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fSyk0jAQU2xz1XxN-pLLXA.png" data-width="974" data-height="363" src="https://cdn-images-1.medium.com/max/800/1*fSyk0jAQU2xz1XxN-pLLXA.png"></figure><p name="18ff" id="18ff" class="graf graf--p graf-after--figure">Since the topic clustering workflow we followed is an unsupervised technique, we need to manually assign overall topic names. For example, we can observe that the first and largest topic contains keywords like chewbacca, jedi, christmas day, independence day, and so on. It is a an interesting mix of holidays and Star Wars. We could explore why both holidays and Star Wars are mixed together. Additionally, the second largest topic seems to talk about various panama and paradise papers along with the companies and people involved.</p><h4 name="e423" id="e423" class="graf graf--h4 graf-after--p">Summary</h4><p name="9f86" id="9f86" class="graf graf--p graf-after--h4">In my opinion, knowledge graphs and natural language processing techniques are a match made in heaven. As mentioned, I have seen similar approaches to analyzing <a href="https://towardsdatascience.com/construct-a-biomedical-knowledge-graph-with-nlp-1f25eddc54a0" data-href="https://towardsdatascience.com/construct-a-biomedical-knowledge-graph-with-nlp-1f25eddc54a0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">medical documents</a>, <a href="https://medium.com/neo4j/making-sense-of-news-the-knowledge-graph-way-d33810ce5005" data-href="https://medium.com/neo4j/making-sense-of-news-the-knowledge-graph-way-d33810ce5005" class="markup--anchor markup--p-anchor" target="_blank">news</a>, or even <a href="https://medium.com/neo4j/monitoring-the-cryptocurrency-space-with-nlp-and-knowledge-graphs-92a1cfaebd1a" data-href="https://medium.com/neo4j/monitoring-the-cryptocurrency-space-with-nlp-and-knowledge-graphs-92a1cfaebd1a" class="markup--anchor markup--p-anchor" target="_blank">crypto reports</a>. The idea is to use NLP and other tools to extract valuable information from unstructured data, which is then used to construct a knowledge graph. A knowledge graph offers a friendly and flexible structure of the extracted information that can be used to support various analytical workflows.</p><p name="267d" id="267d" class="graf graf--p graf-after--p graf--trailing">All the code of this blog post is <a href="https://github.com/tomasonjo/blogs/tree/master/neo4jdocs" data-href="https://github.com/tomasonjo/blogs/tree/master/neo4jdocs" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">available on GitHub</a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/88e291f6cbf4"><time class="dt-published" datetime="2023-01-05T15:16:33.848Z">January 5, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/analyze-your-website-with-nlp-and-knowledge-graphs-88e291f6cbf4" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>