<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>LangChain Library Adds Full Support for Neo4j Vector Index</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">LangChain Library Adds Full Support for Neo4j Vector Index</h1>
</header>
<section data-field="subtitle" class="p-summary">
Streamlining data ingestion and querying in Retrieval-Augmented Generation Applications
</section>
<section data-field="body" class="e-content">
<section name="8647" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="ee73" id="ee73" class="graf graf--h3 graf--leading graf--title">LangChain Library Adds Full Support for Neo4j Vector Index</h3><h4 name="465e" id="465e" class="graf graf--h4 graf-after--h3 graf--subtitle">Streamlining data ingestion and querying in Retrieval-Augmented Generation Applications</h4><figure name="2369" id="2369" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*NknZJMDanDIgbAd0NcmZYQ.png" data-width="1024" data-height="1024" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*NknZJMDanDIgbAd0NcmZYQ.png"><figcaption class="imageCaption">A detective looking at lists of numbers as imagined by Midjourney.</figcaption></figure><p name="355d" id="355d" class="graf graf--p graf-after--figure">If you have been on vacation for the past six months, first of all, congratulations. Secondly, you should know that since the introduction of ChatGPT-like large language models (LLM), the technology ecosystem has changed dramatically. Nowadays, it’s all about retrieval-augmented generation (RAG) applications. The idea behind RAG applications is to provide additional context at query time to have the LLM generate accurate and up-to-date answers.</p><figure name="18ec" id="18ec" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*tQRjmGo0ngjNB8c4j7QP8g.png" data-width="782" data-height="393" src="https://cdn-images-1.medium.com/max/800/1*tQRjmGo0ngjNB8c4j7QP8g.png"><figcaption class="imageCaption">Retrieval-augmented generation workflow. Image by the author.</figcaption></figure><p name="d586" id="d586" class="graf graf--p graf-after--figure">Neo4j has been and is excellent at <a href="https://medium.com/neo4j/knowledge-graphs-llms-real-time-graph-analytics-89b392eaaa95" data-href="https://medium.com/neo4j/knowledge-graphs-llms-real-time-graph-analytics-89b392eaaa95" class="markup--anchor markup--p-anchor" target="_blank">storing and analyzing structured information in RAG applications</a>. Furthermore, Neo4j added the <a href="https://neo4j.com/blog/vector-search-deeper-insights/" data-href="https://neo4j.com/blog/vector-search-deeper-insights/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">vector index search</a> only a couple of days ago, which brings it closer to supporting RAG applications based on unstructured text.</p><p name="6430" id="6430" class="graf graf--p graf-after--p">To streamline the use of Neo4j’s vector index, I have integrated it into the <a href="https://docs.langchain.com/docs" data-href="https://docs.langchain.com/docs" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LangChain library</a> properly. LangChain is a leading framework for building LLM applications, integrating most LLM providers, databases, and more. It supports data ingestion as well as reading workflows and is especially useful for developing question-answering chatbots using the RAG architecture.</p><p name="0686" id="0686" class="graf graf--p graf-after--p">In this blog post, I’ll guide you through an end-to-end example that demonstrates how to leverage LangChain for efficient data ingestion into Neo4j vector index, followed by the construction of a straightforward yet effective RAG application.</p><figure name="8480" id="8480" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2c9ssjs2pJS7L5KWIA0AAw.png" data-width="1062" data-height="646" src="https://cdn-images-1.medium.com/max/800/1*2c9ssjs2pJS7L5KWIA0AAw.png"><figcaption class="imageCaption">Intended workflow. Image by the author.</figcaption></figure><p name="56ab" id="56ab" class="graf graf--p graf-after--figure">The tutorial will consist of the following steps:</p><ul class="postList"><li name="51ff" id="51ff" class="graf graf--li graf-after--p">Read a Wikipedia article using a LangChain Document Reader</li><li name="e20f" id="e20f" class="graf graf--li graf-after--li">Chunk the text</li><li name="25f7" id="25f7" class="graf graf--li graf-after--li">Store the text in Neo4j and index it using the newly added vector index</li><li name="d7cb" id="d7cb" class="graf graf--li graf-after--li">Implement a question-answering workflow to support RAG applications.</li></ul><p name="cfd2" id="cfd2" class="graf graf--p graf-after--li">As always, the code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/llm/official_langchain_neo4jvector.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/llm/official_langchain_neo4jvector.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p><h4 name="d256" id="d256" class="graf graf--h4 graf-after--p">Neo4j Environment Setup</h4><p name="5ba4" id="5ba4" class="graf graf--p graf-after--h4">You need to set up a Neo4j 5.11 or greater to follow along with the examples in this blog post. The easiest way is to start a free instance on <a href="https://neo4j.com/cloud/platform/aura-graph-database/" data-href="https://neo4j.com/cloud/platform/aura-graph-database/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Neo4j Aura</a>, which offers cloud instances of Neo4j database. Alternatively, you can also set up a local instance of the Neo4j database by downloading the <a href="https://neo4j.com/download/" data-href="https://neo4j.com/download/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Neo4j Desktop</a> application and creating a local database instance.</p><h4 name="a6d4" id="a6d4" class="graf graf--h4 graf-after--p">Reading and Chunking a Wikipedia Article</h4><p name="46f8" id="46f8" class="graf graf--p graf-after--h4">We will begin by reading and chunking a Wikipedia article. The process is pretty simple, as LangChain has integrated the Wikipedia document loader as well as the text chunking modules.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="3b4c" id="3b4c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> WikipediaLoader<br /><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br /><br /><span class="hljs-comment"># Read the wikipedia article</span><br />raw_documents = WikipediaLoader(query=<span class="hljs-string">&quot;Leonhard Euler&quot;</span>).load()<br /><span class="hljs-comment"># Define chunking strategy</span><br />text_splitter = CharacterTextSplitter.from_tiktoken_encoder(<br />    chunk_size=<span class="hljs-number">1000</span>, chunk_overlap=<span class="hljs-number">20</span><br />)<br /><span class="hljs-comment"># Chunk the document</span><br />documents = text_splitter.split_documents(raw_documents)<br /><br /><span class="hljs-comment"># Remove summary from metadata</span><br /><span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> documents:<br />    <span class="hljs-keyword">del</span> d.metadata[<span class="hljs-string">&#x27;summary&#x27;</span>]</span></pre><p name="b266" id="b266" class="graf graf--p graf-after--pre">Since Neo4j is a graph database, I thought using the Wikipedia article about Leonhard Euler as the example was only fitting. Next, we use the <strong class="markup--strong markup--p-strong">tiktoken</strong> text chunking module, which uses a tokenizer made by OpenAI, to split the article into chunks with <em class="markup--em markup--p-em">1000</em> tokens. You can learn more about <a href="https://www.pinecone.io/learn/chunking-strategies/" data-href="https://www.pinecone.io/learn/chunking-strategies/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">text chunking strategies in this article</a>.</p><p name="f090" id="f090" class="graf graf--p graf-after--p">LangChain’s <code class="markup--code markup--p-code">WikipediaLoader</code>adds a summary to each chunk by default. I thought the added summaries were a bit redundant. For example, if you used a vector similarity search to retrieve the top three results, the summary would be repeated three times. Therefore, I decided to remove it from the dataset.</p><h4 name="790e" id="790e" class="graf graf--h4 graf-after--p">Store and Index the Text With Neo4j</h4><p name="7e2f" id="7e2f" class="graf graf--p graf-after--h4">LangChain makes it easy to import the documents into Neo4j and index them using the newly added vector index. We tried to make it very user-friendly, which means that you don’t have to know anything about Neo4j or graphs to use it. On the other hand, we provided several customization options for more experienced users, which will be presented in a separate blog post.</p><p name="ef92" id="ef92" class="graf graf--p graf-after--p">Neo4j vector index is wrapped as a LangChain vector store and, therefore, follows the syntax used to interact with other vector databases.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="cdcb" id="cdcb" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Neo4jVector<br /><span class="hljs-keyword">from</span> langchain.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbeddings<br /><br /><span class="hljs-comment"># Neo4j Aura credentials</span><br />url=<span class="hljs-string">&quot;neo4j+s://.databases.neo4j.io&quot;</span><br />username=<span class="hljs-string">&quot;neo4j&quot;</span><br />password=<span class="hljs-string">&quot;&lt;insert password&gt;&quot;</span><br /><br /><span class="hljs-comment"># Instantiate Neo4j vector from documents</span><br />neo4j_vector = Neo4jVector.from_documents(<br />    documents,<br />    OpenAIEmbeddings(),<br />    url=url,<br />    username=username,<br />    password=password<br />)</span></pre><p name="ada1" id="ada1" class="graf graf--p graf-after--pre">The <code class="markup--code markup--p-code">from_documents</code> method connects to a Neo4j database, imports and embeds the documents, and creates a vector index. The data will be represented as the `Chunk` nodes by default. As mentioned, you can customize how the data should be stored, as well as which data should be returned. However, that will be discussed in the following blog post.</p><p name="30b4" id="30b4" class="graf graf--p graf-after--p">If you already have an existing vector index with populated data, you can use the <code class="markup--code markup--p-code">from_existing_index</code> method.</p><h4 name="27a0" id="27a0" class="graf graf--h4 graf-after--p">Vector Similarity Search</h4><p name="2333" id="2333" class="graf graf--p graf-after--h4">We will begin with a simple vector similarity search to verify that everything works as intended.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="2abc" id="2abc" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">query = <span class="hljs-string">&quot;Where did Euler grow up?&quot;</span><br /><br />results = neo4j_vector.similarity_search(query, k=<span class="hljs-number">1</span>)<br /><span class="hljs-built_in">print</span>(results[<span class="hljs-number">0</span>].page_content)</span></pre><p name="0cbb" id="0cbb" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="e3a2" id="e3a2" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zNZCmj_lhhJoQ_efmrKaZA.png" data-width="856" data-height="158" src="https://cdn-images-1.medium.com/max/800/1*zNZCmj_lhhJoQ_efmrKaZA.png"><figcaption class="imageCaption">Image by the author.</figcaption></figure><p name="3500" id="3500" class="graf graf--p graf-after--figure">The LangChain module used the specified embedding function (OpenAI in this example) to embed the question and then find the most similar documents by comparing the cosine similarity between the user question and indexed documents.</p><p name="4dc8" id="4dc8" class="graf graf--p graf-after--p">Neo4j Vector index also supports the Euclidean similarity metric along with the cosine similarity.</p><h4 name="55b7" id="55b7" class="graf graf--h4 graf-after--p">Question-Answer Workflow With LangChain</h4><p name="555a" id="555a" class="graf graf--p graf-after--h4">The nice thing about LangChain is that it supports question-answering workflows using only a line or two of code. For example, if we wanted to create a question-answering that generates answers based on the provided context but also provides which documents it used as the context, we can use the following code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="be8f" id="be8f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain.chat_models <span class="hljs-keyword">import</span> ChatOpenAI<br /><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQAWithSourcesChain<br /><br />chain = RetrievalQAWithSourcesChain.from_chain_type(<br />    ChatOpenAI(temperature=<span class="hljs-number">0</span>),<br />    chain_type=<span class="hljs-string">&quot;stuff&quot;</span>,<br />    retriever=neo4j_vector.as_retriever()<br />)<br /><br />query = <span class="hljs-string">&quot;What is Euler credited for popularizing?&quot;</span><br /><br />chain(<br />    {<span class="hljs-string">&quot;question&quot;</span>: query},<br />    return_only_outputs=<span class="hljs-literal">True</span>,<br />)</span></pre><p name="b31c" id="b31c" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="da95" id="da95" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9uI4-u2Zcyfi-3oJunKbMA.png" data-width="848" data-height="176" src="https://cdn-images-1.medium.com/max/800/1*9uI4-u2Zcyfi-3oJunKbMA.png"><figcaption class="imageCaption">Image by author.</figcaption></figure><p name="c91e" id="c91e" class="graf graf--p graf-after--figure">As you can see, the LLM constructed an accurate answer based on the provided Wikipedia article but also returned which source documents it used. And we only required one line of code to achieve this, which is pretty awesome if you ask me.</p><p name="9ab0" id="9ab0" class="graf graf--p graf-after--p">While testing the code, I noticed that the sources were not always returned. The problem here is not Neo4j Vector implementation but rather GPT-3.5-turbo. Sometimes, it doesn’t listen to instructions to return the source documents. However, if you use GPT-4, the problem goes away.</p><p name="903e" id="903e" class="graf graf--p graf-after--p">Lastly, to replicate the ChatGPT interface, you can add a <a href="https://python.langchain.com/docs/modules/memory/types/buffer" data-href="https://python.langchain.com/docs/modules/memory/types/buffer" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">memory module</a>, which additionally provides the LLM with dialogue history so that we can ask follow-up questions. Again, we only need two lines of codes.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="ec57" id="ec57" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> ConversationalRetrievalChain<br /><span class="hljs-keyword">from</span> langchain.memory <span class="hljs-keyword">import</span> ConversationBufferMemory<br /><br />memory = ConversationBufferMemory(memory_key=<span class="hljs-string">&quot;chat_history&quot;</span>, return_messages=<span class="hljs-literal">True</span>)<br />qa = ConversationalRetrievalChain.from_llm(<br />    ChatOpenAI(temperature=<span class="hljs-number">0</span>), neo4j_vector.as_retriever(), memory=memory)</span></pre><p name="58e5" id="58e5" class="graf graf--p graf-after--pre">Let’s now test it out.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="e5d5" id="e5d5" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">print</span>(qa({<span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What is Euler credited for popularizing?&quot;</span>})[<span class="hljs-string">&quot;answer&quot;</span>])</span></pre><p name="367d" id="367d" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="0f3a" id="0f3a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1bOrPSTJxgntNaBmZvdqTg.png" data-width="846" data-height="288" src="https://cdn-images-1.medium.com/max/800/1*1bOrPSTJxgntNaBmZvdqTg.png"><figcaption class="imageCaption">Image by author.</figcaption></figure><p name="6e49" id="6e49" class="graf graf--p graf-after--figure">And now a follow-up question.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="b83b" id="b83b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">print</span>(qa({<span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;Where did he grow up?&quot;</span>})[<span class="hljs-string">&quot;answer&quot;</span>])</span></pre><p name="cd41" id="cd41" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="314e" id="314e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*APzneOsoRiiuGmpYgYyBzQ.png" data-width="466" data-height="34" src="https://cdn-images-1.medium.com/max/800/1*APzneOsoRiiuGmpYgYyBzQ.png"><figcaption class="imageCaption">Image by author.</figcaption></figure><h4 name="3586" id="3586" class="graf graf--h4 graf-after--figure">Summary</h4><p name="6b2d" id="6b2d" class="graf graf--p graf-after--h4">The vector index is a great addition to Neo4j, making it an excellent solution for handling structured and unstructured data for RAG applications. Hopefully, the LangChain integration will streamline the process of integrating the vector index into your existing or new RAG applications, so you don’t have to worry about the details. Remember, <a href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" data-href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LangChain already supports generating Cypher statements</a> and using them to retrieve context, so you can use it today to retrieve both structured and unstructured information. We have many ideas on upgrading the LangChain support for Neo4j, so stay tuned!</p><p name="18d3" id="18d3" class="graf graf--p graf-after--p">As always, the code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/llm/official_langchain_neo4jvector.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/llm/official_langchain_neo4jvector.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p><p name="277b" id="277b" class="graf graf--p graf-after--p">Make sure to register and attend <a href="https://neo4j.com/nodes" data-href="https://neo4j.com/nodes" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Nodes 2023</a> to learn more about generative AI and graph technology!</p><div name="d702" id="d702" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="http://neo4j.com/nodes" data-href="http://neo4j.com/nodes" class="markup--anchor markup--mixtapeEmbed-anchor" title="http://neo4j.com/nodes"><strong class="markup--strong markup--mixtapeEmbed-strong">NODES 2023 - Save the Date</strong><br><em class="markup--em markup--mixtapeEmbed-em">Join us for NODES 2023, a free 24-hour online conference for developers, data scientists, architects, and data analysts…</em>neo4j.com</a><a href="http://neo4j.com/nodes" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="710ef3c50be2e2eeded988639caf51d8" data-thumbnail-img-id="0*MdmSsTOITeM6DqJt" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*MdmSsTOITeM6DqJt);"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/fa94b8eab334"><time class="dt-published" datetime="2023-08-31T16:57:15.503Z">August 31, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/langchain-library-adds-full-support-for-neo4j-vector-index-fa94b8eab334" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>