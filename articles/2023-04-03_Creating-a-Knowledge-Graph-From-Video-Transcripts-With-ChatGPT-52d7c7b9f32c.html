<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Creating a Knowledge Graph From Video Transcripts With ChatGPT</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Creating a Knowledge Graph From Video Transcripts With ChatGPT</h1>
</header>
<section data-field="subtitle" class="p-summary">
Use GPT-4 as a domain expert to help you extract knowledge from a video transcript
</section>
<section data-field="body" class="e-content">
<section name="10de" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="ca72" id="ca72" class="graf graf--h3 graf--leading graf--title">Creating a Knowledge Graph From Video Transcripts With ChatGPT</h3><h4 name="7edb" id="7edb" class="graf graf--h4 graf-after--h3 graf--subtitle">Use GPT-4 as a domain expert to help you extract knowledge from a video transcript.</h4><figure name="5abd" id="5abd" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*_7rT-ahaII7FCQimddagXQ.png" data-width="1064" data-height="643" src="https://cdn-images-1.medium.com/max/800/1*_7rT-ahaII7FCQimddagXQ.png"><figcaption class="imageCaption">Image by the author.</figcaption></figure><p name="c32d" id="c32d" class="graf graf--p graf-after--figure">A couple of days ago, I got access to GPT-4.</p><p name="bc9c" id="bc9c" class="graf graf--p graf-after--p">The first thing that came to my mind was to test how well it performs as an information extraction model, where the task is to extract relevant entities and relationships from a given text.</p><p name="7d83" id="7d83" class="graf graf--p graf-after--p">I have already played around with GPT-3.5 a bit. The most important thing I noticed is that we don’t want to use the GPT endpoint as an entity linking solution or have it come up with any other external references like citations, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">as it likes to hallucinate those types of information</em></strong>.</p><p name="f929" id="f929" class="graf graf--p graf-after--p">However, a great thing about GPT-3 or GPT-4 is that it performs well in various domains. For example, we can use it to extract people, organizations, or locations from a text.</p><p name="bcf0" id="bcf0" class="graf graf--p graf-after--p">However, I feel that competing against dedicated NLP models is not where the GPT models shine (although they perform well). Instead, the strength of GPT models is in their <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">ability to generalize and be used in other domains</em></strong> where other open-sourced models fail due to their limited training data.</p><p name="00fd" id="00fd" class="graf graf--p graf-after--p">My friend <a href="https://medium.com/u/3865848842f9" data-href="https://medium.com/u/3865848842f9" data-anchor-type="2" data-user-id="3865848842f9" data-action-value="3865848842f9" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Michael Hunger</a> gave me a great idea to test the GPT-4 on extracting information from a nature documentary.</p><figure name="abb4" id="abb4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*h0jA_3aWGijgaiZF" data-width="4608" data-height="3456" data-unsplash-photo-id="79mNMAvSORg" src="https://cdn-images-1.medium.com/max/800/0*h0jA_3aWGijgaiZF"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@turnlip19?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@turnlip19?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Jong Marshes</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><p name="070d" id="070d" class="graf graf--p graf-after--figure">I always liked the <strong class="markup--strong markup--p-strong">deep sea documentary</strong> as the ecosystem and animals vastly differ from terrestrial ones. Therefore, I decided to test GPT-4 information extraction capabilities on an underwater documentary. Additionally, I don’t know of any open-source NLP models trained to detect relationships between <em class="markup--em markup--p-em">sea plants</em> and <em class="markup--em markup--p-em">creatures</em>. So, a deep sea documentary makes for an excellent example of using a GPT-4 to construct a knowledge graph.</p><p name="7ded" id="7ded" class="graf graf--p graf-after--p">All the code is available on <a href="https://github.com/tomasonjo/blogs/tree/master/youtube" data-href="https://github.com/tomasonjo/blogs/tree/master/youtube" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub in the form of a Jupyter N</a>otebook.</p><h4 name="78a6" id="78a6" class="graf graf--h4 graf-after--p">Dataset</h4><p name="31a5" id="31a5" class="graf graf--p graf-after--h4">The most accessible place to find documentaries is YouTube. Although the GPT-4 is multi-modal (supports video, audio, and text), the current version of the endpoint only supports text inputs. Therefore, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">we will analyze a video’s audio transcript</em></strong>, not the video itself.</p><p name="f45d" id="f45d" class="graf graf--p graf-after--p">We will be analyzing the transcript of the following documentary.</p><figure name="0261" id="0261" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/nrI483C5Tro?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="63a6" id="63a6" class="graf graf--p graf-after--figure">First of all, I like the topic of the documentary: “<a href="https://www.youtube.com/watch?v=nrI483C5Tro" data-href="https://www.youtube.com/watch?v=nrI483C5Tro" class="markup--anchor markup--p-anchor" rel="noopener noreferrer noopener noopener" target="_blank"><strong class="markup--strong markup--p-strong">The Spectacular Underwater World of Coral Reefs</strong></a>.”</p><p name="2e98" id="2e98" class="graf graf--p graf-after--p">Secondly, extracting captions from a YouTube video is effortless as we don’t have to use any <em class="markup--em markup--p-em">audio2text</em> models at all. However, converting audio to text with all the available models on HuggingFace or even OpenAI’s Whisper should not be a big problem.</p><p name="54f3" id="54f3" class="graf graf--p graf-after--p">Thirdly, this video has captions that are <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">not auto-generated</em></strong>. At first, I tried to extract information from auto-generated captions on YouTube, but I learned that they might not be the best input. So if you can, <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">avoid using auto-generated YouTube captions</em></strong>.</p><p name="d796" id="d796" class="graf graf--p graf-after--p">The captions can be retrieved straightforwardly with the <a href="https://pypi.org/project/youtube-transcript-api/" data-href="https://pypi.org/project/youtube-transcript-api/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">YouTube Transcript/Subtitle library</a>. All we have to do is to provide the video id.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6cfd" id="6cfd" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> youtube_transcript_api <span class="hljs-keyword">import</span> YouTubeTranscriptApi<br /><br />video_id = <span class="hljs-string">&quot;nrI483C5Tro&quot;</span><br />transcript = YouTubeTranscriptApi.get_transcript(video_id)<br /><span class="hljs-built_in">print</span>(transcript[:<span class="hljs-number">5</span>])</span></pre><p name="f7b7" id="f7b7" class="graf graf--p graf-after--pre">The transcript has the following structure.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="json" name="973a" id="973a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-punctuation">[</span><br />   <span class="hljs-punctuation">{</span><br />      <span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;water the liquid that oceans are made of&quot;</span><span class="hljs-punctuation">,</span><br />      <span class="hljs-attr">&quot;start&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">5.46</span><span class="hljs-punctuation">,</span><br />      <span class="hljs-attr">&quot;duration&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">4.38</span><br />   <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br />   <span class="hljs-punctuation">{</span><br />      <span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;and it fills endless depths only few will venture\\xa0\\xa0&quot;</span><span class="hljs-punctuation">,</span><br />      <span class="hljs-attr">&quot;start&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">12.24</span><span class="hljs-punctuation">,</span><br />      <span class="hljs-attr">&quot;duration&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">4.92</span><br />   <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br />   <span class="hljs-punctuation">{</span><br />      <span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;out into the endless open ocean\\xa0\nof this vast underwater world&quot;</span><span class="hljs-punctuation">,</span><br />      <span class="hljs-attr">&quot;start&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">17.16</span><span class="hljs-punctuation">,</span><br />      <span class="hljs-attr">&quot;duration&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">4.68</span><br />   <span class="hljs-punctuation">}</span><br /><span class="hljs-punctuation">]</span></span></pre><p name="566b" id="566b" class="graf graf--p graf-after--pre">The captions are split into chunks, which can be used as video subtitles. Therefore, the start and duration information is provided along with the text. You might also notice a couple of special characters like <code class="markup--code markup--p-code">\xa0</code> and <code class="markup--code markup--p-code">\n</code> .</p><p name="ee7f" id="ee7f" class="graf graf--p graf-after--p">Even though GPT-4 endpoint support up to 8k tokens per request, more is needed to process the whole transcript in a single request. Therefore, we need to split the transcript into several parts.</p><p name="3e8b" id="3e8b" class="graf graf--p graf-after--p">So, I decided to split the transcript into multiple parts, where the end of the part is determined when there are <em class="markup--em markup--p-em">five or more seconds of no captions</em>, announcing a brief pause in narration. Using this approach, I aim to keep all connecting text together and retain relevant information in a single section.</p><p name="db57" id="db57" class="graf graf--p graf-after--p">I used the following code to group the transcript into several sections.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f6dd" id="f6dd" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Split into sections and include start and end timestamps</span><br />sections = []<br />current_section = <span class="hljs-string">&quot;&quot;</span><br />start_time = <span class="hljs-literal">None</span><br />previous_end = <span class="hljs-number">0</span><br />pause_threshold = <span class="hljs-number">5</span><br /><br /><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> transcript:<br />    <span class="hljs-keyword">if</span> current_section <span class="hljs-keyword">and</span> (line[<span class="hljs-string">&quot;start&quot;</span>] - previous_end &gt; pause_threshold):<br />        <span class="hljs-comment"># If there is a pause greater than 5s, we deem the end of section</span><br />        end_time = line[<span class="hljs-string">&quot;start&quot;</span>]<br />        sections.append(<br />            {<br />                <span class="hljs-string">&quot;text&quot;</span>: current_section.strip(),<br />                <span class="hljs-string">&quot;start_time&quot;</span>: start_time,<br />                <span class="hljs-string">&quot;end_time&quot;</span>: end_time,<br />            }<br />        )<br />        current_section = <span class="hljs-string">&quot;&quot;</span><br />        start_time = <span class="hljs-literal">None</span><br />    <span class="hljs-keyword">else</span>:<br />        <span class="hljs-comment"># If this is the start of a new section, record the start time</span><br />        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> start_time:<br />            start_time = line[<span class="hljs-string">&quot;start&quot;</span>]<br /><br />        <span class="hljs-comment"># Add the line to the current paragraph</span><br />        clean_text = line[<span class="hljs-string">&quot;text&quot;</span>].replace(<span class="hljs-string">&quot;\n&quot;</span>, <span class="hljs-string">&quot; &quot;</span>).replace(<span class="hljs-string">&quot;\xa0&quot;</span>, <span class="hljs-string">&quot; &quot;</span>)<br />        current_section += <span class="hljs-string">&quot; &quot;</span>.join(clean_text.split()) + <span class="hljs-string">&quot; &quot;</span><br />        <span class="hljs-comment"># Tag the end of the dialogue</span><br />        previous_end = line[<span class="hljs-string">&quot;start&quot;</span>] + line[<span class="hljs-string">&quot;duration&quot;</span>]<br /><br /><span class="hljs-comment"># If there&#x27;s a paragraph left at the end, add it to the list of paragraphs</span><br /><span class="hljs-keyword">if</span> current_section:<br />    end_time = transcript[-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;start&quot;</span>] + transcript[-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;duration&quot;</span>]<br />    sections.append(<br />        {<br />            <span class="hljs-string">&quot;text&quot;</span>: current_section.strip().replace(<span class="hljs-string">&quot;\n&quot;</span>, <span class="hljs-string">&quot; &quot;</span>).replace(<span class="hljs-string">&quot;\xa0&quot;</span>, <span class="hljs-string">&quot; &quot;</span>),<br />            <span class="hljs-string">&quot;start_time&quot;</span>: start_time,<br />            <span class="hljs-string">&quot;end_time&quot;</span>: end_time,<br />        }<br />    )<br /><span class="hljs-comment"># Remove empty paragraphs</span><br />sections = [p <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> sections <span class="hljs-keyword">if</span> p[<span class="hljs-string">&quot;text&quot;</span>]]</span></pre><p name="2644" id="2644" class="graf graf--p graf-after--pre">To evaluate the results of the section grouping, I printed the following information.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="d425" id="d425" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Number of paragraphs</span><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Number of paragraphs: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(sections)}</span>&quot;</span>)<br /><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Max characters per paragraph: <span class="hljs-subst">{<span class="hljs-built_in">max</span>([<span class="hljs-built_in">len</span>(el[<span class="hljs-string">&#x27;text&#x27;</span>]) <span class="hljs-keyword">for</span> el <span class="hljs-keyword">in</span> sections])}</span>&quot;</span>)</span></pre><p name="1715" id="1715" class="graf graf--p graf-after--pre">There are 77 sections, with the longest having 1267 characters in it. We are far from the GPT-4 token limit, and I think the above approach delivers a nice text granularity, at least in this example.</p><h4 name="58f8" id="58f8" class="graf graf--h4 graf-after--p">Information Extraction With GPT-4</h4><p name="cde0" id="cde0" class="graf graf--p graf-after--h4">GPT-4 endpoint is optimized for chat but works well for traditional completion tasks. As the model is optimized for conversation, we can provide a <strong class="markup--strong markup--p-strong">system</strong> message, which helps set the assistant’s behavior along with any previous messages that can help keep the context of the dialogue. However, as we are using the GPT-4 endpoint for a text completion task, we will not provide any previous messages.</p><p name="8f6b" id="8f6b" class="graf graf--p graf-after--p">I used the following prompt as the <strong class="markup--strong markup--p-strong">system</strong> message.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="8213" id="8213" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">system = <span class="hljs-string">&quot;You are an archeology and biology expert helping us extract relevant information.&quot;</span></span></pre><p name="e0b1" id="e0b1" class="graf graf--p graf-after--pre">However, I noticed that the model behaved almost identically when I provided the system message or not. Next, I developed the following prompt through some iterations, which extracts relevant entities and relationships from a given text.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="250d" id="250d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Set up the prompt for GPT-3 to complete</span><br />prompt = <span class="hljs-string">&quot;&quot;&quot;#This a transcript from a sea documentary.<br />#The task is to extract as many relevant entities to biology, chemistry, or archeology.<br />#The entities should include all animals, biological entities, locations.<br />#However, the entities should not include distances or time durations.<br />#Also, return the type of an entity using the Wikipedia class system and the sentiment of the mentioned entity,<br />#where the sentiment value ranges from -1 to 1, and -1 being very negative, 1 being very positive<br />#Additionally, extract all relevant relationships between identified entities.<br />#The relationships should follow the Wikipedia schema type.<br />#The output of a relationship should be in a form of a triple Head, Relationship, Tail, for example<br />#Peter, WORKS_AT, Hospital/n<br /># An example &quot;St. Peter is located in Paris&quot; should have an output with the following format<br />entity<br />St. Peter, person, 0.0<br />Paris, location, 0.0<br /><br />relationships<br />St.Peter, LOCATED_IN, Paris\n&quot;&quot;&quot;</span></span></pre><p name="30ff" id="30ff" class="graf graf--p graf-after--pre">The GPT-4 is prompted to extract relevant entities from a given text. Additionally, I added some constraints that distances and time durations should not be treated as entities. The extracted entities should contain their name, type, and sentiment. As for the relationships, they should be provided in the form of a triple. I added some hints that the model should follow the Wikipedia schema type, making the extracted relationship types more standardized. I learned that it is always good to provide an example of the output. Otherwise, the model might use different output formats at will.</p><p name="85bb" id="85bb" class="graf graf--p graf-after--p">One thing to note is that we might have instructed the model to provide us with a nice JSON representation of extracted entities and relationships. Nicely structured data might certainly be plus. However, you are paying the price for nicely structured JSON objects as the cost of the API is calculated per input and output token count. Therefore, the JSON boilerplate comes with a price.</p><p name="00db" id="00db" class="graf graf--p graf-after--p">Next, we need to define the function that calls the GPT-4 endpoint and processes the response.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f563" id="f563" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-meta">@retry(<span class="hljs-params">tries=<span class="hljs-number">3</span>, delay=<span class="hljs-number">5</span></span>)</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">process_gpt4</span>(<span class="hljs-params">text</span>):<br />    paragraph = text<br /><br />    completion = openai.ChatCompletion.create(<br />        model=<span class="hljs-string">&quot;gpt-4&quot;</span>,<br />        <span class="hljs-comment"># Try to be as deterministic as possible</span><br />        temperature=<span class="hljs-number">0</span>,<br />        messages=[<br />            {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: system},<br />            {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: prompt + paragraph},<br />        ],<br />    )<br /><br />    nlp_results = completion.choices[<span class="hljs-number">0</span>].message.content<br />    <br />    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-string">&quot;relationships&quot;</span> <span class="hljs-keyword">in</span> nlp_results:<br />        <span class="hljs-keyword">raise</span> Exception(<br />            <span class="hljs-string">&quot;GPT-4 is not being nice and isn&#x27;t returning results in correct format&quot;</span><br />        )<br />    <br />    <span class="hljs-keyword">return</span> parse_entities_and_relationships(nlp_results)</span></pre><p name="8526" id="8526" class="graf graf--p graf-after--pre">Even though we explicitly defined the output format in the prompt, the GPT-4 model sometimes does its own thing and does not follow the rules. It happened to me only twice out of a couple of hundred requests. However, it is annoying when that happens, and all the downstream dataflow doesn’t work as intended. Therefore, I added a simple check of the response and added a retry decorator in case that happens.</p><p name="b912" id="b912" class="graf graf--p graf-after--p">Additionally, I only added the <strong class="markup--strong markup--p-strong">temperature</strong> parameter to make the model behave as deterministic as possible. However, when I rerun the transcript a couple of times, I got slightly different results. It costs around $1.6 to process the transcript of the chosen video with GPT-4.</p><h4 name="b5bf" id="b5bf" class="graf graf--h4 graf-after--p">Graph Model and Import</h4><p name="0e99" id="0e99" class="graf graf--p graf-after--h4">We will be using Neo4j to store the results of the information extraction pipeline. I have used a <a href="https://sandbox.neo4j.com/?usecase=blank-sandbox" data-href="https://sandbox.neo4j.com/?usecase=blank-sandbox" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">free Neo4j Sandbox instance</a> for this project, but you can also use the <a href="https://neo4j.com/cloud/platform/aura-graph-database/" data-href="https://neo4j.com/cloud/platform/aura-graph-database/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AuraDB Free</a> or <a href="https://neo4j.com/download/" data-href="https://neo4j.com/download/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">local Desktop environment</a>.</p><p name="3e16" id="3e16" class="graf graf--p graf-after--p">One thing is certain. No NLP model is perfect. Therefore, we want all extracted entities and relationships to point to the text where they were extracted, which allows us to verify the validity of information if necessary.</p><figure name="374b" id="374b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*K9EB5Vr8xo31IpXcTfHvRg.png" data-width="575" data-height="355" src="https://cdn-images-1.medium.com/max/800/1*K9EB5Vr8xo31IpXcTfHvRg.png"><figcaption class="imageCaption">Graph schema. Image by the author.</figcaption></figure><p name="7d7c" id="7d7c" class="graf graf--p graf-after--figure">Since we want to point the extracted entities and relationships to the relevant text, we need to include the sections along with the video in our graph. The section nodes contain the text, start, and end time. Entities and relationships are then connected to the section nodes. What might be counterintuitive is that we represent extracted relationships as a node in our graph. The reason is that Neo4j doesn’t allow to have relationships to point to another relationship. However, we want to have a link between extracted relationship and its source text. Therefore, we need to model the extracted relationship as a separate node.</p><p name="398f" id="398f" class="graf graf--p graf-after--p">The Cypher statement for the graph import is the following:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="425b" id="425b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">import_query = <span class="hljs-string">&quot;&quot;&quot;<br />MERGE (v:Video {id:$videoId})<br />CREATE (v)-[:HAS_SECTION]-&gt;(p:Section)<br />SET p.startTime = toFloat($start),<br />    p.endTime = toFloat($end),<br />    p.text = $text<br />FOREACH (e in $entities |<br />  MERGE (entity:Entity {name: e[0]})<br />  ON CREATE SET entity.type = e[1] <br />  MERGE (p)-[:MENTIONS{sentiment:toFloat(e[2])}]-&gt;(entity))<br />WITH p<br />UNWIND $relationships AS relation<br />MERGE (source:Entity {name: relation[0]})<br />MERGE (target:Entity {name: relation[2]})<br />MERGE (source)-[:RELATIONSHIP]-&gt;(r:Relationship {type: relation[1]})-[:RELATIONSHIP]-&gt;(target)<br />MERGE (p)-[mr:MENTIONS_RELATIONSHIP]-&gt;(r)<br />&quot;&quot;&quot;</span></span></pre><p name="ba11" id="ba11" class="graf graf--p graf-after--pre">Finally, we can go ahead and process the whole transcript and import the extracted information into Neo4j using the following code:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f64c" id="f64c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">with</span> driver.session() <span class="hljs-keyword">as</span> session:<br />    <span class="hljs-keyword">for</span> i, section <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sections):<br />        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Processing <span class="hljs-subst">{i}</span> paragraph&quot;</span>)<br />        text = section[<span class="hljs-string">&quot;text&quot;</span>]<br />        start = section[<span class="hljs-string">&quot;start_time&quot;</span>]<br />        end = section[<span class="hljs-string">&quot;end_time&quot;</span>]<br />        entities, relationships = process_gpt4(text)<br />        params = {<br />            <span class="hljs-string">&quot;videoId&quot;</span>: video_id,<br />            <span class="hljs-string">&quot;start&quot;</span>: start,<br />            <span class="hljs-string">&quot;end&quot;</span>: end,<br />            <span class="hljs-string">&quot;text&quot;</span>: text,<br />            <span class="hljs-string">&quot;entities&quot;</span>: entities,<br />            <span class="hljs-string">&quot;relationships&quot;</span>: relationships,<br />        }<br />        session.run(import_query, params)</span></pre><p name="8e50" id="8e50" class="graf graf--p graf-after--pre">You can open Neo4j Browser and validate the import by executing the following Cypher statement.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="b81a" id="b81a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">CALL</span> apoc.meta.graph()</span></pre><p name="8ba6" id="8ba6" class="graf graf--p graf-after--pre">The meta-graph procedure should return the following graph visualization.</p><figure name="fcfa" id="fcfa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*H1Fku8hOHHz1cOkdlPdkGQ.png" data-width="1031" data-height="647" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*H1Fku8hOHHz1cOkdlPdkGQ.png"><figcaption class="imageCaption">Generated graph schema. Image by the author.</figcaption></figure><h4 name="deb8" id="deb8" class="graf graf--h4 graf-after--figure">Entity Disambiguation With GPT-4</h4><p name="c89c" id="c89c" class="graf graf--p graf-after--h4">After inspecting the GPT-4 results, I have decided that performing a simple entity disambiguation would be best. For example, there are currently five different nodes for a Moray Eels:</p><ul class="postList"><li name="4b4f" id="4b4f" class="graf graf--li graf-after--p">moray eel</li><li name="2f86" id="2f86" class="graf graf--li graf-after--li">Moray</li><li name="2b17" id="2b17" class="graf graf--li graf-after--li">Moray Eel</li><li name="e5b0" id="e5b0" class="graf graf--li graf-after--li">moray</li><li name="0799" id="0799" class="graf graf--li graf-after--li">morays</li></ul><p name="094c" id="094c" class="graf graf--p graf-after--li">We could lowercase all entities and use various NLP techniques to identify which nodes refer to the same entities. However, we can also use the GPT-4 endpoint to perform entity disambiguation. I wrote the following prompt to perform entity disambiguation.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="2dd7" id="2dd7" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">disambiguation_prompt = <span class="hljs-string">&quot;&quot;&quot;<br />#Act as a entity disambiugation tool and tell me which values reference the same entity. <br />#For example if I give you<br />#<br />#Birds<br />#Bird<br />#Ant<br />#<br />#You return to me<br />#<br />#Birds, 1<br />#Bird, 1<br />#Ant, 2<br />#<br />#As the Bird and Birds values have the same integer assigned to them, it means that they reference the same entity.<br />#Now process the following values\n<br />&quot;&quot;&quot;</span></span></pre><p name="2e4f" id="2e4f" class="graf graf--p graf-after--pre">The idea is to assign the same integers to nodes that refer to the same entity. Using this prompt, we are able to tag all nodes with additional <strong class="markup--strong markup--p-strong">disambiguation</strong> properties.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="6805" id="6805" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">disambiguate</span>(<span class="hljs-params">entities</span>):<br />    completion = openai.ChatCompletion.create(<br />        model=<span class="hljs-string">&quot;gpt-4&quot;</span>,<br />        <span class="hljs-comment"># Try to be as deterministic as possible</span><br />        temperature=<span class="hljs-number">0</span>,<br />        messages=[<br />            {<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: disambiguation_prompt + <span class="hljs-string">&quot;\n&quot;</span>.join(all_animals)},<br />        ],<br />    )<br /><br />    disambiguation_results = completion.choices[<span class="hljs-number">0</span>].message.content<br />    <span class="hljs-keyword">return</span> [row.split(<span class="hljs-string">&quot;, &quot;</span>) <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> disambiguation_results.split(<span class="hljs-string">&quot;\n&quot;</span>)]<br /><br />all_animals = run_query(<span class="hljs-string">&quot;&quot;&quot;<br />MATCH (e:Entity {type: &#x27;animal&#x27;})<br />RETURN e.name AS animal<br />&quot;&quot;&quot;</span>)[<span class="hljs-string">&#x27;animal&#x27;</span>].to_list()<br /><br /><br />disambiguation_params = disambiguate(all_animals)<br />run_query(<br />    <span class="hljs-string">&quot;&quot;&quot;<br />UNWIND $data AS row<br />MATCH (e:Entity {name:row[0]})<br />SET e.disambiguation = row[1]<br />&quot;&quot;&quot;</span>,<br />    {<span class="hljs-string">&quot;data&quot;</span>: disambiguation_params},<br />)</span></pre><p name="05f1" id="05f1" class="graf graf--p graf-after--pre">Now that the disambiguation information is in the database, we can use it to evaluate the results.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="e826" id="e826" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">MATCH</span> (e:Entity {type:&quot;animal&quot;})<br /><span class="hljs-keyword">RETURN</span> e.disambiguation <span class="hljs-keyword">AS</span> i, <span class="hljs-keyword">collect</span>(e.name) <span class="hljs-keyword">AS</span> entities<br /><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> size(entities) <span class="hljs-keyword">DESC</span><br />LIMIT <span class="hljs-number">5</span></span></pre><p name="73b7" id="73b7" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="7616" id="7616" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/e2b368649a1dd2e81e88fece4fac0687.js"></script></figure><p name="749e" id="749e" class="graf graf--p graf-after--figure">While this disambiguation is not that complicated, it is still worth noting that we can achieve this without NLP knowledge or having to develop any hand-crafted rules.</p><h4 name="6f16" id="6f16" class="graf graf--h4 graf-after--p">Analysis</h4><p name="072b" id="072b" class="graf graf--p graf-after--h4">In the final step of this blog post, we will evaluate the results of the information extraction pipeline using the GPT-4 model.</p><p name="7ad5" id="7ad5" class="graf graf--p graf-after--p">First, we will examine the type and count of extracted entities.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="89c1" id="89c1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">MATCH (e:Entity)<br />RETURN e.<span class="hljs-built_in">type</span> AS <span class="hljs-built_in">type</span>, count(*) AS count<br />ORDER BY count DESC<br />LIMIT <span class="hljs-number">5</span></span></pre><p name="f7a9" id="f7a9" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="bb9c" id="bb9c" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/78d2597e33b4da80d79e773c763414b2.js"></script></figure><p name="5a1c" id="5a1c" class="graf graf--p graf-after--figure">Most entities are animals, locations, and biological entities. However, we can notice that sometimes the model decides to use the whitespace and other times underscore for biological entities.</p><p name="01c0" id="01c0" class="graf graf--p graf-after--p">Throughout my experiments with GPT endpoints, I have observed that the best approach is to be as specific as possible in what information and how you want it to be categorized. Therefore, it is good practice with GPT-4 to define the types of entities we want to extract, as the resulting types will be more consistent.</p><p name="2475" id="2475" class="graf graf--p graf-after--p">Additionally, the model didn’t classify 33 entity types. The thing is that GPT-4 might come up with some types for these entities if asked. However, they only appear in the relationship extraction part of the results, where entity types are not requested. One workaround could be to ask for entity types in the relationship extraction part.</p><p name="c6ed" id="c6ed" class="graf graf--p graf-after--p">Next, we will examine which animals are the most mentioned in the video.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="f867" id="f867" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">MATCH</span> (e:Entity {type:&quot;animal&quot;})<br /><span class="hljs-keyword">RETURN</span> e.name <span class="hljs-keyword">AS</span> entity, e.type <span class="hljs-keyword">AS</span> type,<br />       count{(e)<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[:MENTIONS]<span class="hljs-operator">-</span>()} <span class="hljs-keyword">AS</span> mentions<br /><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> mentions <span class="hljs-keyword">DESC</span><br />LIMIT <span class="hljs-number">5</span></span></pre><p name="1ba6" id="1ba6" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="6d96" id="6d96" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/0fd5f20fc91d044540252f0bee19117d.js"></script></figure><p name="a82c" id="a82c" class="graf graf--p graf-after--figure">The most mentioned animals are moray eels, lionfish, and brittle stars. I am familiar only with eels, so watching the documentary to learn about other fishes might be a good idea.</p><p name="c293" id="c293" class="graf graf--p graf-after--p">We can also evaluate which relationships or facts have been extracted regarding moray eels.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="4f3a" id="4f3a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">MATCH</span> (e:Entity {name:&quot;morays&quot;})<span class="hljs-operator">-</span>[:RELATIONSHIP]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>(r)<span class="hljs-operator">-</span>[:RELATIONSHIP]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>(target)<br /><span class="hljs-keyword">RETURN</span> e.name <span class="hljs-keyword">AS</span> source, r.type <span class="hljs-keyword">AS</span> relationship, target.name <span class="hljs-keyword">AS</span> target,<br />       count{(r)<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[:MENTIONS_RELATIONSHIP]<span class="hljs-operator">-</span>()} <span class="hljs-keyword">AS</span> mentions<br /><span class="hljs-keyword">UNION</span> <span class="hljs-keyword">ALL</span><br /><span class="hljs-keyword">MATCH</span> (e:Entity {name:&quot;morays&quot;})<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[:RELATIONSHIP]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>(r)<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[:RELATIONSHIP]<span class="hljs-operator">-</span>(source)<br /><span class="hljs-keyword">RETURN</span> source.name <span class="hljs-keyword">AS</span> source, r.type <span class="hljs-keyword">AS</span> relationship, e.name <span class="hljs-keyword">AS</span> target,<br />       count{(r)<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[:MENTIONS_RELATIONSHIP]<span class="hljs-operator">-</span>()} <span class="hljs-keyword">AS</span> mentions</span></pre><p name="4f7a" id="4f7a" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="6587" id="6587" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/829f161c96fba25f45006eb057bb0f78.js"></script></figure><p name="6dbd" id="6dbd" class="graf graf--p graf-after--figure">There is quite a lot we can learn about moray eels. They cooperate with groupers, coexist with Triggerfishes, and are being cleaned by cleaner shrimps. Additionally, a moray searching for a female moray can be relatable.</p><p name="09a7" id="09a7" class="graf graf--p graf-after--p">Let’s say we want to check if the relationship that morays interact with lionfish is accurate. We can retrieve the source text and validate the claim manually.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="fb40" id="fb40" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">MATCH</span> (e:Entity)<span class="hljs-operator">-</span>[:RELATIONSHIP]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>(r)<span class="hljs-operator">-</span>[:RELATIONSHIP]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>(t:Entity)<br /><span class="hljs-keyword">WHERE</span> e.name <span class="hljs-operator">=</span> &quot;morays&quot; <span class="hljs-keyword">AND</span> r.type <span class="hljs-operator">=</span> &quot;INTERACTS_WITH&quot; <span class="hljs-keyword">AND</span> t.name <span class="hljs-operator">=</span> &quot;Lionfish&quot;<br /><span class="hljs-keyword">MATCH</span> (r)<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[:MENTIONS_RELATIONSHIP]<span class="hljs-operator">-</span>(s:Section)<br /><span class="hljs-keyword">RETURN</span> s.text <span class="hljs-keyword">AS</span> text</span></pre><p name="c92c" id="c92c" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><pre data-code-block-mode="0" spellcheck="false" name="f8cf" id="f8cf" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">ly tough are its cousins the scorpion fishes they <br>lie there as if dead especially when others around<br>them freak out and even when moray eels fight with lionfishes for food</span></pre><p name="0c60" id="0c60" class="graf graf--p graf-after--pre">The text mentions that eels fight with lionfish for food. We can also notice that the transcript is hard to read and understand, even for a human. Therefore, we can commend GPT-4 for doing a good job on a transcript where even a human might struggle.</p><p name="d88b" id="d88b" class="graf graf--p graf-after--p">Lastly, we can use the knowledge graph as a search engine that returns timestamps of sections where relevant entities we want to see. So, for example, we can ask the database to return all the timestamps of sections in which lionfish is mentioned.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="f76f" id="f76f" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">MATCH</span> (e:Entity {name:&quot;Lionfish&quot;})<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[:MENTIONS]<span class="hljs-operator">-</span>(s:Section)<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[:HAS_SECTION]<span class="hljs-operator">-</span>(v:Video)<br /><span class="hljs-keyword">RETURN</span> s.startTime <span class="hljs-keyword">AS</span> <span class="hljs-type">timestamp</span>, s.endTime <span class="hljs-keyword">AS</span> endTime,<br />       &quot;https://youtube.com/watch?v=&quot; <span class="hljs-operator">+</span> v.id <span class="hljs-operator">+</span> &quot;&amp;t=&quot; <span class="hljs-operator">+</span> toString(toInteger(s.startTime)) <span class="hljs-keyword">AS</span> URL<br /><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> <span class="hljs-type">timestamp</span></span></pre><p name="58bf" id="58bf" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="9e3b" id="9e3b" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/5668386d26ecac6452607aad2418b861.js"></script></figure><h4 name="2017" id="2017" class="graf graf--h4 graf-after--figure">Summary</h4><p name="b10f" id="b10f" class="graf graf--p graf-after--h4">The remarkable ability of GPT-3.5 and GPT-4 models to generalize across various domains is a powerful tool for exploring and analyzing different datasets to extract relevant information. Honestly, I’m not entirely sure which endpoint I would use to recreate this blog post without GPT-4. As far as I know, there are no open-source relation extraction models or datasets on sea creatures. Therefore, to avoid the hassle of labeling a dataset and training a custom model, we can simply utilize a GPT endpoint. Furthermore, I eagerly anticipate the opportunity to examine its promised capability for multi-modal analysis based on audio or text input.</p><p name="8aac" id="8aac" class="graf graf--p graf-after--p">To learn more about this topic, join me at NODES 2023, a free online global conference about graph technologies. The CFP is open now until June 30. <a href="https://dev.neo4j.com/nodes23" data-href="https://dev.neo4j.com/nodes23" class="markup--anchor markup--p-anchor" rel="noopener noreferrer noopener" target="_blank">https://dev.neo4j.com/nodes23</a></p><p name="2d8c" id="2d8c" class="graf graf--p graf-after--p graf--trailing">As always, the code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/youtube/video2graph.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/youtube/video2graph.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/52d7c7b9f32c"><time class="dt-published" datetime="2023-04-03T15:02:42.562Z">April 3, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/creating-a-knowledge-graph-from-video-transcripts-with-gpt-4-52d7c7b9f32c" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>