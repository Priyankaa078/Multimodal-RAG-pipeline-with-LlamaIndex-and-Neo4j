<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Integrating Neo4j into the LangChain ecosystem</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Integrating Neo4j into the LangChain ecosystem</h1>
</header>
<section data-field="subtitle" class="p-summary">
Learn how to develop a LangChain agent that has multiple ways of interacting with the Neo4j database
</section>
<section data-field="body" class="e-content">
<section name="7be0" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="9443" id="9443" class="graf graf--h3 graf--leading graf--title">Integrating Neo4j into the LangChain ecosystem</h3><h4 name="4080" id="4080" class="graf graf--h4 graf-after--h3 graf--subtitle">Learn how to develop a LangChain agent that has multiple ways of interacting with the Neo4j database</h4><figure name="7f5a" id="7f5a" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*KMHZDUibql-2fYBy" data-width="4896" data-height="3264" data-unsplash-photo-id="2EJCSULRwC8" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*KMHZDUibql-2fYBy"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@agk42?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@agk42?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Alex Knight</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><p name="e577" id="e577" class="graf graf--p graf-after--figure"><em class="markup--em markup--p-em">Update: The so-called Cypher Search, where the LLM generates a Cypher statement to query the Neo4j database, has been integrated directly to the LangChain library. Learn more </em><a href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" data-href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">here</em></a></p><p name="3176" id="3176" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">2nd update: Vector search is now supported directly by vector index in Neo4j, so I have changed the vector search code to use the new index introduced in 5.11</em></p><p name="8914" id="8914" class="graf graf--p graf-after--p">ChatGPT inspired the world and started a new AI revolution. However, it seems that the latest trend is supplying ChatGPT with external information to increase its accuracy and give it the ability to answer questions where the answers are not present in public datasets. Another trend around large language models (LLMs) is to turn them into agents, where they have an ability to interact with their environment through various API calls or other integrations.</p><p name="fe00" id="fe00" class="graf graf--p graf-after--p">Since enhancing LLMs is relatively new, there aren’t a lot of open-source libraries yet. However, it seems that the go-to library for building applications around LLMs like ChatGPT is called <a href="https://python.langchain.com/en/latest/index.html" data-href="https://python.langchain.com/en/latest/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LangChain</a>. The library provides the ability to enhance an LLM by giving it access to various tools and external data sources. Not only can it improve its responses by accessing external data, but it can also act as an agent and manipulate its environment through external endpoints.</p><p name="1ef5" id="1ef5" class="graf graf--p graf-after--p">I randomly stumbled upon a LangChain project by <a href="https://medium.com/u/fd610570f1c7" data-href="https://medium.com/u/fd610570f1c7" data-anchor-type="2" data-user-id="fd610570f1c7" data-action-value="fd610570f1c7" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Ibis Prevedello</a> that uses graph search to enhance the LLMs by providing additional external context.</p><div name="d005" id="d005" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ibiscp/LLM-IMDB" data-href="https://github.com/ibiscp/LLM-IMDB" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ibiscp/LLM-IMDB"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - ibiscp/LLM-IMDB: Proof of concept app using LangChain and LLMs to retrieve information…</strong><br><em class="markup--em markup--mixtapeEmbed-em">Proof of concept app using LangChain and LLMs to retrieve information from graphs, built with the IMDB dataset - GitHub…</em>github.com</a><a href="https://github.com/ibiscp/LLM-IMDB" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="ae4752515d16dd60db7ae81b61ed70c3" data-thumbnail-img-id="0*6kL-DQuOIlmugduB" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*6kL-DQuOIlmugduB);"></a></div><p name="246b" id="246b" class="graf graf--p graf-after--mixtapeEmbed">The project by Ibis uses <a href="https://networkx.org/" data-href="https://networkx.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">NetworkX library</a> to store the graph information. I really liked his approach and how easy it was to integrate graph search into the LangChain ecosystem. Therefore, I have decided to develop a project that would integrate <a href="https://neo4j.com/" data-href="https://neo4j.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Neo4j</a>, a graph database, into the LangChain ecosystem.</p><div name="c311" id="c311" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/tomasonjo/langchain2neo4j" data-href="https://github.com/tomasonjo/langchain2neo4j" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/tomasonjo/langchain2neo4j"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - tomasonjo/langchain2neo4j: Integrating Neo4j database into langchain ecosystem</strong><br><em class="markup--em markup--mixtapeEmbed-em">The Langchain2Neo4j is a proof of concept application of how to integrate Neo4j into the Langchain ecosystem. This…</em>github.com</a><a href="https://github.com/tomasonjo/langchain2neo4j" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="03ad94d4a54f57abefe16c000084a07c" data-thumbnail-img-id="0*opuamE5_HZ1-5ils" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*opuamE5_HZ1-5ils);"></a></div><p name="d5d0" id="d5d0" class="graf graf--p graf-after--mixtapeEmbed">After two weeks of coding, the project now allows a LangChain agent to interact with Neo4j in three different modes:</p><ul class="postList"><li name="2df9" id="2df9" class="graf graf--li graf-after--p">Generating Cypher statements to query the database</li><li name="ce72" id="ce72" class="graf graf--li graf-after--li">Full-text keyword search of relevant entities</li><li name="e55e" id="e55e" class="graf graf--li graf-after--li">Vector similarity search</li></ul><p name="228a" id="228a" class="graf graf--p graf-after--li">In this blog post, I will walk you through the reasoning and implementation of each approach I developed.</p><h4 name="d9e7" id="d9e7" class="graf graf--h4 graf-after--p">Environment setup</h4><p name="cff1" id="cff1" class="graf graf--p graf-after--h4">First, we will configure the Neo4j environment. We will use the dataset available as the <a href="https://sandbox.neo4j.com/?usecase=recommendations" data-href="https://sandbox.neo4j.com/?usecase=recommendations" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">recommendations project</a> in the Neo4j sandbox. The easiest solution is simply to create a Neo4j Sandbox instance by following <a href="https://sandbox.neo4j.com/?usecase=recommendations" data-href="https://sandbox.neo4j.com/?usecase=recommendations" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this link</a>. However, if you would prefer a local instance of Neo4j, you can also restore a <a href="https://github.com/neo4j-graph-examples/recommendations/tree/main/data" data-href="https://github.com/neo4j-graph-examples/recommendations/tree/main/data" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">database dump that is available on GitHub</a>. The dataset is part of the <a href="https://grouplens.org/datasets/movielens/" data-href="https://grouplens.org/datasets/movielens/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">MovieLens datasets</a> [1], specifically the small version.</p><p name="f336" id="f336" class="graf graf--p graf-after--p">After the Neo4j database is instantiated, we should have a graph with the following schema populated.</p><figure name="5139" id="5139" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HorBDLYVPFUvubEeoIYJCw.png" data-width="794" data-height="677" src="https://cdn-images-1.medium.com/max/800/1*HorBDLYVPFUvubEeoIYJCw.png"><figcaption class="imageCaption">Graph schema. Image by the author.</figcaption></figure><p name="1038" id="1038" class="graf graf--p graf-after--figure">Next, you need to clone the langchain2neo4j repository by executing the following command:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="bash" name="12e9" id="12e9" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">git <span class="hljs-built_in">clone</span> https://github.com/tomasonjo/langchain2neo4j</span></pre><p name="c6ab" id="c6ab" class="graf graf--p graf-after--pre">In the next step, you need to create an <code class="markup--code markup--p-code">.env</code> file and populate the neo4j and OpenAI credentials as shown in the <code class="markup--code markup--p-code">.env.example</code> file.</p><p name="c377" id="c377" class="graf graf--p graf-after--p">Lastly, you need to create a full-text index in Neo4j and import movie title embeddings by running:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="typescript" name="60b4" id="60b4" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">sh seed_db.<span class="hljs-property">sh</span></span></pre><p name="0a4a" id="0a4a" class="graf graf--p graf-after--pre">If you are a Windows user, the <code class="markup--code markup--p-code">seed_db</code> script probably won’t work. In that case, I have prepared a Jupyter notebook that can help you seed the database as an alternative to the shell script.</p><p name="fd61" id="fd61" class="graf graf--p graf-after--p">Now, let’s jump to the LangChain integration.</p><h4 name="c0b1" id="c0b1" class="graf graf--h4 graf-after--p">LangChain agent</h4><p name="538c" id="538c" class="graf graf--p graf-after--h4">As far as I have seen, the most common data flow of using a LangChain agent to answer a user question is the following:</p><figure name="9f9e" id="9f9e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WqRmmUg0cDGmaO9wYJIH3A.png" data-width="705" data-height="405" src="https://cdn-images-1.medium.com/max/800/1*WqRmmUg0cDGmaO9wYJIH3A.png"><figcaption class="imageCaption">LangChain agent flow. Image by the author.</figcaption></figure><p name="9d87" id="9d87" class="graf graf--p graf-after--figure">The agent data flow is initiated when it receives input from a user. The agent then sends a request to an LLM model that includes the user question along with the agent prompt, which is a set of instructions in a natural language the agent should follow. In turn, the LLM responds with further instructions to the agent. Most often, the first response is to use any available tools to gain additional information from external sources. However, tools are not limited to read-only operations. For example, you could use them to update a database. After the tool returns additional context, another call is made to an LLM that includes the newly gained information. The LLM now has the option to produce a final answer that is returned to a user, or it can decide it needs to perform more actions through its available tools.</p><p name="9ff7" id="9ff7" class="graf graf--p graf-after--p">A LangChain agent uses LLMs for its reasoning. Therefore, the first step is to define which model to use. At the moment, the langchain2neo4j project supports only OpenAI’s chat completion models, specifically GPT-3.5-turbo, and GPT-4 models.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4027" id="4027" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">if</span> model_name <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;gpt-3.5-turbo&#x27;</span>, <span class="hljs-string">&#x27;gpt-4&#x27;</span>]:<br />    llm = ChatOpenAI(temperature=<span class="hljs-number">0</span>, model_name=model_name)<br /><span class="hljs-keyword">else</span>:<br />    <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">f&quot;Model <span class="hljs-subst">{model_name}</span> is currently not supported&quot;</span>)</span></pre><p name="0432" id="0432" class="graf graf--p graf-after--pre">I haven’t yet explored other LLMs besides OpenAI’s. However, with LangChain, it should be easy, as it has integration with more than ten other LLMs. I didn’t know that that many existed.</p><div name="1bbc" id="1bbc" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://python.langchain.com/en/latest/modules/models/llms/integrations.html" data-href="https://python.langchain.com/en/latest/modules/models/llms/integrations.html" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://python.langchain.com/en/latest/modules/models/llms/integrations.html"><strong class="markup--strong markup--mixtapeEmbed-strong">Integrations</strong><br><em class="markup--em markup--mixtapeEmbed-em">Edit description</em>python.langchain.com</a><a href="https://python.langchain.com/en/latest/modules/models/llms/integrations.html" class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="750555b5b6bdf0ce86a4a81d6ba405c7"></a></div><p name="b366" id="b366" class="graf graf--p graf-after--mixtapeEmbed">Next, we need to add a conversational memory with the following line:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="525c" id="525c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">memory = ConversationBufferMemory(<br />    memory_key=<span class="hljs-string">&quot;chat_history&quot;</span>, return_messages=<span class="hljs-literal">True</span>)</span></pre><p name="b86f" id="b86f" class="graf graf--p graf-after--pre">LangChain support <a href="https://python.langchain.com/en/latest/modules/agents/agents.html" data-href="https://python.langchain.com/en/latest/modules/agents/agents.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">multiple types of agents</a>. For example, some agents can use the memory component, while others cannot. Since the object was to build a chatbot, I chose the <a href="https://python.langchain.com/en/latest/modules/agents/agents/examples/chat_conversation_agent.html" data-href="https://python.langchain.com/en/latest/modules/agents/agents/examples/chat_conversation_agent.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Conversation Agent (for Chat Models) agent</a> type. What is interesting about the LangChain library is that half the code is written in Python, while the other half is prompt engineering. We can explore the <a href="https://github.com/hwchase17/langchain/blob/master/langchain/agents/conversational_chat/prompt.py" data-href="https://github.com/hwchase17/langchain/blob/master/langchain/agents/conversational_chat/prompt.py" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">prompts that the conversational agent uses</a>. For example, the agents has some basic instructions it must follow:</p><blockquote name="0e42" id="0e42" class="graf graf--blockquote graf-after--p">Assistant is a large language model trained by OpenAI. Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand. Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics. Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.</blockquote><p name="be20" id="be20" class="graf graf--p graf-after--blockquote">Additionally, the agent has instructions to use any of the specified tools if needed.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5675" id="5675" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Assistant can ask the user to use tools to look up information <br />that may be helpful <span class="hljs-keyword">in</span> answering the users original question.<br />The tools the human can use are:<br />{{tools}}<br />{format_instructions}<br />USE<span class="hljs-string">R&#x27;S INPUT - - - - - - - - - - <br />Here is the user&#x27;</span>s <span class="hljs-built_in">input</span> <br />(remember to respond <span class="hljs-keyword">with</span> a markdown code snippet of a <br />json blob <span class="hljs-keyword">with</span> a single action, <span class="hljs-keyword">and</span> NOTHING <span class="hljs-keyword">else</span>):<br />{{{{<span class="hljs-built_in">input</span>}}}}</span></pre><p name="ea59" id="ea59" class="graf graf--p graf-after--pre">Interestingly, the prompt states that the assistant can ask the user to look up additional information using tools. However, the user is not a human but an application built on top of the LangChain library. Therefore, the entire process of finding further information is done automatically without any human in the loop. Of course, we can change the prompts if needed. The prompt also includes the format the LLMs should use to communicate with the agent.</p><p name="00f6" id="00f6" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Note that the agent prompt doesn’t include that the agent shouldn’t answer a question if the answer is not provided in the context returned by tools.</em></p><p name="a2c8" id="a2c8" class="graf graf--p graf-after--p">Now, all we have to do is to define the available tools. As mentioned, I have prepared three methods of interacting with Neo4j database.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="9d66" id="9d66" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">tools = [<br />    Tool(<br />        name=<span class="hljs-string">&quot;Cypher search&quot;</span>,<br />        func=cypher_tool.run,<br />        description=<span class="hljs-string">&quot;&quot;&quot;<br />        Utilize this tool to search within a movie database, <br />        specifically designed to answer movie-related questions.<br />        This specialized tool offers streamlined search capabilities<br />        to help you find the movie information you need with ease.<br />        Input should be full question.&quot;&quot;&quot;</span>,<br />    ),<br />    Tool(<br />        name=<span class="hljs-string">&quot;Keyword search&quot;</span>,<br />        func=fulltext_tool.run,<br />        description=<span class="hljs-string">&quot;Utilize this tool when explicitly told to use <br />        keyword search.Input should be a list of relevant movies <br />        inferred from the question.&quot;</span>,<br />    ),<br />    Tool(<br />        name=<span class="hljs-string">&quot;Vector search&quot;</span>,<br />        func=vector_tool.run,<br />        description=<span class="hljs-string">&quot;Utilize this tool when explicity told to use <br />        vector search.Input should be full question.Do not include <br />        agent instructions.&quot;</span>,<br />    ),<br /><br />]</span></pre><p name="d7a3" id="d7a3" class="graf graf--p graf-after--pre">The description of a tool is used to specify the capabilities of the tool as well as to inform the agent when to use it. Additionally, we need to specify the format of the input a tool expects. For example, both the Cypher and vector search expect a full question as an input, while the keyword search expects a list of relevant movies as input.</p><p name="0902" id="0902" class="graf graf--p graf-after--p">LangChain is quite different from what I am used to in coding. It uses prompts to instruct the LLMs to do the work for you instead of coding it yourself. For example, the keyword search instructs the ChatGPT to extract relevant movies and use that as input. I spent 2 hours debugging the tool input format before realizing I could specify it using natural language, and the LLM will handle the rest.</p><p name="6b0f" id="6b0f" class="graf graf--p graf-after--p">Remember how I mentioned that the agent doesn’t have instructions that it shouldn’t answer questions where the information is not provided in the context? Let’s examine the following dialogue.</p><figure name="85b5" id="85b5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*R0e6DRcrumBQ2SZJB8iLsw.png" data-width="682" data-height="111" src="https://cdn-images-1.medium.com/max/800/1*R0e6DRcrumBQ2SZJB8iLsw.png"><figcaption class="imageCaption">Image by the author.</figcaption></figure><p name="cb33" id="cb33" class="graf graf--p graf-after--figure">The LLM decided that based on the tool descriptions, it cannot use any of them to retrieve relevant context. However, the LLM knows a lot by default, and since the agent has no constraints that it should only rely on external sources, the LLM can form the answer independently. We would need to change the agent prompt if we wanted to enforce different behavior.</p><h4 name="3739" id="3739" class="graf graf--h4 graf-after--p">Generating Cypher statements</h4><p name="8bc7" id="8bc7" class="graf graf--p graf-after--h4">I have already developed a chatbot interacting with a Neo4j database by generating Cypher statements using the OpenAI’s <a href="https://medium.com/neo4j/context-aware-knowledge-graph-chatbot-with-gpt-4-and-neo4j-d3a99e8ae21e" data-href="https://medium.com/neo4j/context-aware-knowledge-graph-chatbot-with-gpt-4-and-neo4j-d3a99e8ae21e" class="markup--anchor markup--p-anchor" target="_blank">conversational models like the GPT-3.5-turbo and GPT-4</a>. Therefore, I could borrow most of the ideas to implement a tool that allows the LangChain agent to retrieve information from the Neo4j database by constructing Cypher statements.</p><p name="842c" id="842c" class="graf graf--p graf-after--p">The older models like text-davinci-003 and GPT-3.5-turbo work better as a few-shot Cypher generator, where we provide a couple of Cypher examples that the model can use to generate new Cypher statements. However, it seems the GPT-4 works well when we only present the graph schema. Consequently, since graph schema can be extracted with a Cypher query, the GPT-4 can be theoretically used on any graph schema without any manual work required by a human.</p><p name="7bef" id="7bef" class="graf graf--p graf-after--p">I won’t walk you through what LangChain does under the hood. We will just look at the function that gets executed when the LangChain agents decides to interact with the Neo4j database using Cypher statements.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0849" id="0849" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_call</span>(<span class="hljs-params">self, inputs: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]:<br />    chat_prompt = ChatPromptTemplate.from_messages(<br />        [self.system_prompt] + inputs[<span class="hljs-string">&#x27;chat_history&#x27;</span>] + [self.human_prompt])<br />    cypher_executor = LLMChain(<br />        prompt=chat_prompt, llm=self.llm, callback_manager=self.callback_manager<br />    )<br />    cypher_statement = cypher_executor.predict(<br />        question=inputs[self.input_key], stop=[<span class="hljs-string">&quot;Output:&quot;</span>])<br />    <span class="hljs-comment"># If Cypher statement was not generated due to lack of context</span><br />    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-string">&quot;MATCH&quot;</span> <span class="hljs-keyword">in</span> cypher_statement:<br />        <span class="hljs-keyword">return</span> {<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Missing context to create a Cypher statement&#x27;</span>}<br />    context = self.graph.query(cypher_statement)<br /><br />    <span class="hljs-keyword">return</span> {<span class="hljs-string">&#x27;answer&#x27;</span>: context}</span></pre><p name="dc7a" id="dc7a" class="graf graf--p graf-after--pre">The Cypher generating tool gets the question along with the chat history as input. The input to the LLM is then combined by using the <strong class="markup--strong markup--p-strong">system</strong> message, <strong class="markup--strong markup--p-strong">chat history</strong>, and the current question. I have prepared the following <strong class="markup--strong markup--p-strong">system</strong> message prompt for the Cypher generating tool.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="ini" name="8dbe" id="8dbe" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-attr">SYSTEM_TEMPLATE</span> = <span class="hljs-string">&quot;&quot;&quot;<br />You are an assistant with an ability to generate Cypher queries based off <br />example Cypher queries. Example Cypher queries are:\n&quot;&quot;&quot;</span> + examples + <span class="hljs-string">&quot;&quot;&quot;\n<br />Do not response with any explanation or any other information except the <br />Cypher query. You do not ever apologize and strictly generate cypher statements<br />based of the provided Cypher examples. Do not provide any Cypher statements <br />that can&#x27;t be inferred from Cypher examples. Inform the user when you can&#x27;t <br />infer the cypher statement due to the lack of context of the conversation <br />and state what is the missing context.<br />&quot;&quot;&quot;</span></span></pre><p name="e86f" id="e86f" class="graf graf--p graf-after--pre">Prompt engineering feels more like art than science. In this example, we provide the LLM with a couple of Cypher statement examples and let it generate Cypher statements based on that information. Additionally, we place a couple of constraints, like allowing it to construct only Cypher statements that could be inferred from training examples. Additionally, we don’t let the model apologize or explain its thoughts (however, GPT-3.5-turbo won’t listen to that instructions). Finally, if the question lacks context, we allow the model to respond with that information instead of forcing it to generate Cypher statements.</p><p name="e037" id="e037" class="graf graf--p graf-after--p">After the LLM construct a Cypher statements, we simply use it to query a Neo4j database, and return the results to the Agent. Here is an example flow.</p><figure name="3ed9" id="3ed9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*_cgRn4j7FRdNHk2yhYKeoA.png" data-width="1213" data-height="859" src="https://cdn-images-1.medium.com/max/800/1*_cgRn4j7FRdNHk2yhYKeoA.png"><figcaption class="imageCaption">Flow of the agent using a Cypher generating tool. Image by the author.</figcaption></figure><p name="912f" id="912f" class="graf graf--p graf-after--figure">When a user inputs their question, it gets sent to an LLM along with the agent prompt. In this example, the LLM responds that it needs to use the <strong class="markup--strong markup--p-strong">Cypher search</strong> tool. The Cypher search tool constructs a Cypher statement and uses it to query Neo4j. The results of the query are then passed back to the agent. Next, the agent sends another request to an LLM along with the new context. As the context contains the needed information to construct an answer, the LLM forms the final answer and instructs the agent to return it to the user.</p><p name="8787" id="8787" class="graf graf--p graf-after--p">Of course, we can now ask follow up questions.</p><figure name="bd2e" id="bd2e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MOcUwNTz8U66WZQ91sgREQ.png" data-width="682" data-height="326" src="https://cdn-images-1.medium.com/max/800/1*MOcUwNTz8U66WZQ91sgREQ.png"><figcaption class="imageCaption">Follow up question. Image by the author.</figcaption></figure><p name="eb7d" id="eb7d" class="graf graf--p graf-after--figure">Since the agent has memory, it is aware of who is the second actor and, therefore, can pass the information along to the Cypher search tool to construct appropriate Cypher statements.</p><h4 name="0bb1" id="0bb1" class="graf graf--h4 graf-after--p">Keyword search of relevant triples</h4><p name="5c31" id="5c31" class="graf graf--p graf-after--h4">I got the idea for keyword search from existing knowledge graph index implementations in both <a href="https://python.langchain.com/en/latest/modules/chains/index_examples/graph_qa.html?highlight=graph" data-href="https://python.langchain.com/en/latest/modules/chains/index_examples/graph_qa.html?highlight=graph" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LangChain</a> and <a href="https://gpt-index.readthedocs.io/en/latest/reference/indices/kg_query.html" data-href="https://gpt-index.readthedocs.io/en/latest/reference/indices/kg_query.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GPT-index libraries</a>. Both implementations are fairly similar. They ask an LLM to extract relevant entities from a question and search the graph for any triples that contain those entities. So I figured we could do something similar with Neo4j. However, while we could search for entities with a simple <strong class="markup--strong markup--p-strong">MATCH</strong> statement, I have decided that using Neo4j’s full-text index would be better. After relevant entities are found using the full-text index, we return the triples and hope the relevant information to answer the question is there.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4889" id="4889" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_call</span>(<span class="hljs-params">self, inputs: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:<br />    <span class="hljs-string">&quot;&quot;&quot;Extract entities, look up info and answer question.&quot;&quot;&quot;</span><br />    question = inputs[self.input_key]<br />    params = generate_params(question)<br />    context = self.graph.query(<br />        fulltext_search, {<span class="hljs-string">&#x27;query&#x27;</span>: params})<br />    <span class="hljs-keyword">return</span> {self.output_key: context}</span></pre><p name="dd34" id="dd34" class="graf graf--p graf-after--pre">Remember, the agent has instructions to parse out relevant movie titles already and use that as input to the Keyword search tool. Therefore, we don’t have to deal with that. However, since multiple entities could exist in the question, we must construct appropriate Lucene query parameters as the full-text index is based on Lucene. Then, we simply query the full-text index and return hopefully relevant triples. The Cypher statement we use is the following:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="a9ae" id="a9ae" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">CALL</span> db.index.fulltext.queryNodes(&quot;movie&quot;, $query) <br />YIELD node, score<br /><span class="hljs-keyword">WITH</span> node, score LIMIT <span class="hljs-number">5</span><br /><span class="hljs-keyword">CALL</span> {<br />  <span class="hljs-keyword">WITH</span> node<br />  <span class="hljs-keyword">MATCH</span> (node)<span class="hljs-operator">-</span>[r:<span class="hljs-operator">!</span>RATED]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>(target)<br />  <span class="hljs-keyword">RETURN</span> <span class="hljs-built_in">coalesce</span>(node.name, node.title) <span class="hljs-operator">+</span> &quot; &quot; <span class="hljs-operator">+</span> type(r) <span class="hljs-operator">+</span> &quot; &quot; <span class="hljs-operator">+</span> <span class="hljs-built_in">coalesce</span>(target.name, target.title) <span class="hljs-keyword">AS</span> <span class="hljs-keyword">result</span><br />  <span class="hljs-keyword">UNION</span><br />  <span class="hljs-keyword">WITH</span> node<br />  <span class="hljs-keyword">MATCH</span> (node)<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[r:<span class="hljs-operator">!</span>RATED]<span class="hljs-operator">-</span>(target)<br />  <span class="hljs-keyword">RETURN</span> <span class="hljs-built_in">coalesce</span>(target.name, target.title) <span class="hljs-operator">+</span> &quot; &quot; <span class="hljs-operator">+</span> type(r) <span class="hljs-operator">+</span> &quot; &quot; <span class="hljs-operator">+</span> <span class="hljs-built_in">coalesce</span>(node.name, node.title) <span class="hljs-keyword">AS</span> <span class="hljs-keyword">result</span><br />}<br /><span class="hljs-keyword">RETURN</span> <span class="hljs-keyword">result</span> LIMIT <span class="hljs-number">100</span></span></pre><p name="ecb2" id="ecb2" class="graf graf--p graf-after--pre">So, we take the top five relevant entities returned by the full-text index. Next, we generate triples by traversing to their neighbors. I have specifically excluded the <strong class="markup--strong markup--p-strong">RATED</strong> relationships from being traversed because they contain irrelevant information. I haven’t explored it, but I have a good feeling we could also instruct the LLM to provide a list of relevant relationships to be investigated along with the appropriate entities, which would make our keyword search more focused. The keyword search can be initiated by explicitly instructing the agent.</p><figure name="e7c7" id="e7c7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*LQ1fqeSbSaTLMwshGzXr7A.png" data-width="1056" data-height="753" src="https://cdn-images-1.medium.com/max/800/1*LQ1fqeSbSaTLMwshGzXr7A.png"><figcaption class="imageCaption">Keyword search flow. Image by the author.</figcaption></figure><p name="46f3" id="46f3" class="graf graf--p graf-after--figure">The LLM is instructed to use the <strong class="markup--strong markup--p-strong">keyword search</strong> tool. Additionally, the agent is told to provide the keywords search a list of relevant entities as input, which is only <strong class="markup--strong markup--p-strong">Pokemon</strong> in this example. The Lucene parameter is then used to query Neo4j. This approach casts a broader net and hopes the extracted triples contain relevant information. For example, the retrieved context includes information on the genre of Pokemon, which is irrelevant. Still, it also has information about who acted in the movie, which allows the agent to answer the user’s question.</p><p name="b8b2" id="b8b2" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">As mentioned, we could instruct the LLM to produce a list of relevant relationship types along with appropriate entities, which could help the agent retrieve more relevant information.</em></p><h4 name="522f" id="522f" class="graf graf--h4 graf-after--p">Vector similarity search</h4><p name="8203" id="8203" class="graf graf--p graf-after--h4">The vector similarity search is the last mode to interact with a Neo4j database we will examine. Vector search is trendy at the moment. For example, LangChain offers <a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores.html" data-href="https://python.langchain.com/en/latest/modules/indexes/vectorstores.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">integrations with more than ten vector databases</a>. Additionally, Neo4j has <a href="https://neo4j.com/blog/vector-search-deeper-insights/" data-href="https://neo4j.com/blog/vector-search-deeper-insights/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">added a vector index in their version 5.11</a>, which we will be using in this example. The idea behind vector similarity search is to embed a question into embedding space and find relevant documents based on the similarity of the embeddings of the question and documents. We only need to be careful to use the same embedding model to produce the vector representation of documents and the question. I have used the OpenAI’s embeddings in the vector search implementation.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="7871" id="7871" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_call</span>(<span class="hljs-params">self, inputs: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]:<br />    <span class="hljs-string">&quot;&quot;&quot;Embed a question and do vector search.&quot;&quot;&quot;</span><br />    question = inputs[self.input_key]<br />    embedding = self.embeddings.embed_query(question)<br />    context = self.graph.query(<br />        vector_search, {<span class="hljs-string">&#x27;embedding&#x27;</span>: embedding})<br />    <span class="hljs-keyword">return</span> {self.output_key: context}</span></pre><p name="b7f0" id="b7f0" class="graf graf--p graf-after--pre">So, the first thing we do is embed the question. Next, we use the embedding to find relevant movies in the database. Usually, the vector databases return the text of a relevant document. However, we are dealing with a graph database. Therefore, I have decided to produce relevant information using the triple structure. The Cypher statement used is:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="641e" id="641e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">WITH</span> $embedding <span class="hljs-keyword">AS</span> e<br /><span class="hljs-keyword">CALL</span> db.index.vector.queryNodes(<span class="hljs-string">&#x27;movies&#x27;</span>,<span class="hljs-number">5</span>, e) yield node <span class="hljs-keyword">as</span> m, score<br /><span class="hljs-keyword">CALL</span> {<br />  <span class="hljs-keyword">WITH</span> m<br />  <span class="hljs-keyword">MATCH</span> (m)<span class="hljs-operator">-</span>[r:<span class="hljs-operator">!</span>RATED]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>(target)<br />  <span class="hljs-keyword">RETURN</span> <span class="hljs-built_in">coalesce</span>(m.name, m.title) <span class="hljs-operator">+</span> &quot; &quot; <span class="hljs-operator">+</span> type(r) <span class="hljs-operator">+</span> &quot; &quot; <span class="hljs-operator">+</span> <span class="hljs-built_in">coalesce</span>(target.name, target.title) <span class="hljs-keyword">AS</span> <span class="hljs-keyword">result</span><br />  <span class="hljs-keyword">UNION</span><br />  <span class="hljs-keyword">WITH</span> m<br />  <span class="hljs-keyword">MATCH</span> (m)<span class="hljs-operator">&lt;</span><span class="hljs-operator">-</span>[r:<span class="hljs-operator">!</span>RATED]<span class="hljs-operator">-</span>(target)<br />  <span class="hljs-keyword">RETURN</span> <span class="hljs-built_in">coalesce</span>(target.name, target.title) <span class="hljs-operator">+</span> &quot; &quot; <span class="hljs-operator">+</span> type(r) <span class="hljs-operator">+</span> &quot; &quot; <span class="hljs-operator">+</span> <span class="hljs-built_in">coalesce</span>(m.name, m.title) <span class="hljs-keyword">AS</span> <span class="hljs-keyword">result</span><br />}<br /><span class="hljs-keyword">RETURN</span> <span class="hljs-keyword">result</span> LIMIT <span class="hljs-number">100</span></span></pre><p name="7df1" id="7df1" class="graf graf--p graf-after--pre">The Cypher statement is similar to the keyword search example. The only difference is that we use vector index instead of a full-text index to identify relevant movies.</p><figure name="b678" id="b678" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*isgP7zzeADYcodMmwpAZig.png" data-width="1204" data-height="891" src="https://cdn-images-1.medium.com/max/800/1*isgP7zzeADYcodMmwpAZig.png"><figcaption class="imageCaption">Vector search flow. Image by the author.</figcaption></figure><p name="6258" id="6258" class="graf graf--p graf-after--figure">When an agent is instructed to use the <strong class="markup--strong markup--p-strong">vector search</strong> tool, the first step is to embed the question as a vector. The OpenAI’s embedding model produces vector representations with a dimension of 1536. So, the next step is to use the constructed vector and search for relevant information in the database using the vector index. Again, since we are dealing with a graph database, I have decided to return the information to the agent in the form of a triple.</p><p name="2039" id="2039" class="graf graf--p graf-after--p">What is interesting about vector search is that even though we instructed the agent to search for the <strong class="markup--strong markup--p-strong">Lord of the Ring</strong> movies, the vector similarity search also returned information about the <strong class="markup--strong markup--p-strong">Hobbit</strong> movies. It looks like that Lord of the Ring and Hobbit movies are close in the embedded space, which is understandable.</p><h4 name="96ec" id="96ec" class="graf graf--h4 graf-after--p">Summary</h4><p name="be09" id="be09" class="graf graf--p graf-after--h4">It looks like chatbots and generative agents that can access external tools and information are the next wave that follows the original ChatGPT hype. Having the ability to provide additional context to an LLM can greatly improve its results. Additionally, the agent’s tools are not restricted to read-only operations, which means they can update a database or even make orders on Amazon. For the most part, it seems that the LangChain library is the primary library at the moment to be used to implement generative agents. When you start using LangChain, you might need a bit of a shift in the coding process, as you need to combine LLM prompts with code to complete tasks. For example, messages between LLMs and tools can be shaped and reshaped with natural language instructions as prompts instead of Python code. I hope this project will help you implement the capabilities of a graph database like Neo4j into your LangChain project.</p><p name="bb26" id="bb26" class="graf graf--p graf-after--p">As always, the code is available on <a href="https://github.com/tomasonjo/langchain2neo4j" data-href="https://github.com/tomasonjo/langchain2neo4j" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p><h4 name="0244" id="0244" class="graf graf--h4 graf-after--p">References</h4><p name="04ed" id="04ed" class="graf graf--p graf-after--h4 graf--trailing">[1] <em class="markup--em markup--p-em">F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. </em><a href="https://doi.org/10.1145/2827872" data-href="https://doi.org/10.1145/2827872" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">https://doi.org/10.1145/2827872</em></a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/df0e988344d2"><time class="dt-published" datetime="2023-04-17T20:18:43.345Z">April 17, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/integrating-neo4j-into-the-langchain-ecosystem-df0e988344d2" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>