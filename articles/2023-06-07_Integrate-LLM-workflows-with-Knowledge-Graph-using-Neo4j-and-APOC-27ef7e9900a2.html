<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Integrate LLM workflows with Knowledge Graph using Neo4j and APOC</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Integrate LLM workflows with Knowledge Graph using Neo4j and APOC</h1>
</header>
<section data-field="subtitle" class="p-summary">
OpenAI and VertexAI endpoints are now available as APOC Extended procedures
</section>
<section data-field="body" class="e-content">
<section name="5e6c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2756" id="2756" class="graf graf--h3 graf--leading graf--title">Integrate LLM workflows with Knowledge Graph using Neo4j and APOC</h3><h4 name="6254" id="6254" class="graf graf--h4 graf-after--h3 graf--subtitle">OpenAI and VertexAI endpoints are now available as APOC Extended procedures</h4><p name="5bba" id="5bba" class="graf graf--p graf-after--h4">Probably a day doesn’t go by that you don’t hear about new and exciting things happening in the Large Language Model (LLM) space. There are so many opportunities and use cases for any company to utilize the power of LLMs to enhance their productivity, transform or manipulate their data, and be used in conversational AI and QA systems.</p><p name="df47" id="df47" class="graf graf--p graf-after--p">To make it easier for you to <a href="https://neo4j.com/generativeai/" data-href="https://neo4j.com/generativeai/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">integrate LLMs with Knowledge Graphs</a>, the team at Neo4j has begun the journey of adding support for LLM integrations. The integrations are available as APOC Extended procedures. At the moment, OpenAI and VertexAI endpoints are supported, but we plan to add support for many more.</p><p name="b45e" id="b45e" class="graf graf--p graf-after--p">When I was brainstorming what would be a cool use case to demonstrate the newly added APOC procedures, my friend <a href="https://medium.com/u/3865848842f9" data-href="https://medium.com/u/3865848842f9" data-anchor-type="2" data-user-id="3865848842f9" data-action-value="3865848842f9" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Michael Hunger</a> suggested an exciting idea. What if we used graph context, or the neighborhood of a node, to enrich the information stored in text embeddings? That way, the vector similarity search could produce better results due to the increased richness of embedded information. The idea is simple but compelling and could be helpful in many use cases.</p><p name="6842" id="6842" class="graf graf--p graf-after--p">All the code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/llm/Neo4jOpenAIApoc.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/llm/Neo4jOpenAIApoc.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p><h4 name="7f42" id="7f42" class="graf graf--h4 graf-after--p">Neo4j environment setup</h4><p name="3f9e" id="3f9e" class="graf graf--p graf-after--h4">In this example, we will use both the APOC and Graph Data Science libraries. Luckily, <a href="https://neo4j.com/sandbox/" data-href="https://neo4j.com/sandbox/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Neo4j Sandbox</a> projects have both libraries installed and additionally come with a prepopulated database. Therefore, you can set up the environment with a couple of clicks. We will use the <a href="https://sandbox.neo4j.com/?usecase=movies" data-href="https://sandbox.neo4j.com/?usecase=movies" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">small Movie project</a> to avoid incurring a more considerable LLM API cost.</p><figure name="a06e" id="a06e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FdRXxPhWJSaOmDIeHLoWGw.png" data-width="858" data-height="714" src="https://cdn-images-1.medium.com/max/800/1*FdRXxPhWJSaOmDIeHLoWGw.png"><figcaption class="imageCaption">Sample of the movies graph. Image by the author.</figcaption></figure><p name="9a4a" id="9a4a" class="graf graf--p graf-after--figure">The dataset contains <strong class="markup--strong markup--p-strong">Movie</strong> and <strong class="markup--strong markup--p-strong">Person</strong> nodes. There are only 38 movies, so we are dealing with a tiny dataset. The information provides a movie’s title and tagline, when it was released, and who acted in or directed it.</p><h4 name="16ce" id="16ce" class="graf graf--h4 graf-after--p">Constructing text embedding values</h4><p name="c90b" id="c90b" class="graf graf--p graf-after--h4">We will be using the <a href="https://openai.com/" data-href="https://openai.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI API endpoints</a>. Therefore, you will end to create an OpenAI account if you haven’t already.</p><p name="38bb" id="38bb" class="graf graf--p graf-after--p">As mentioned, the idea is to use the neighborhood of a node to construct its text embedding representation. Since the graph model is simple, we don’t have a lot of creative freedom. We will create text embedding representations of movies by using their properties and neighbor information. In this instance, the neighbor information is only about its actors and directors. However, I believe that this concept can be applied to more complex graph schema and be used to improve your vector similarity search applications.</p><figure name="5c6d" id="5c6d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*HiCYExxmRoJ4K2KolV2B1w.png" data-width="575" data-height="475" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*HiCYExxmRoJ4K2KolV2B1w.png"><figcaption class="imageCaption">Idea of using the information from neighbor nodes to enrich the text embedding representations of central node. Image by the author. Icons from <a href="https://www.flaticon.com/" data-href="https://www.flaticon.com/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">flaticon</a>.</figcaption></figure><p name="1dbb" id="1dbb" class="graf graf--p graf-after--figure">The typical approach we see nowadays, where we simply chunk and embed documents, might fail when looking for information that spans multiple documents. This problem is also known as multi-hop question answering. However, the multi-hop QA problem can be solved using knowledge graphs. One way to look at a knowledge graph is as condensed information storage. For example, an <a href="https://medium.com/neo4j/creating-a-knowledge-graph-from-video-transcripts-with-gpt-4-52d7c7b9f32c" data-href="https://medium.com/neo4j/creating-a-knowledge-graph-from-video-transcripts-with-gpt-4-52d7c7b9f32c" class="markup--anchor markup--p-anchor" target="_blank">information extraction pipeline</a> can be used to extract relevant information from various records. Using knowledge graphs, you can represent highly-connected information that spans multiple documents as relationships between various entities.</p><p name="d7e0" id="d7e0" class="graf graf--p graf-after--p">One solution is to use <a href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" data-href="https://towardsdatascience.com/langchain-has-added-cypher-search-cb9d821120d5" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LLMs to generate a Cypher statement that can be used to retrieve connected information from the database</a>. Another solution, which we will use here, is to use the connection information to enrich the text embedding representations. Additionally, the enhanced information can be retrieved at query time to provide additional context to the LLM from which it can base its response.</p><p name="9c7d" id="9c7d" class="graf graf--p graf-after--p">The following Cypher query can be used to retrieve all the relevant information about the movie nodes from their neighbors.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="sql" name="f582" id="f582" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">MATCH</span> (m:Movie)<br /><span class="hljs-keyword">MATCH</span> (m)<span class="hljs-operator">-</span>[r:ACTED_IN<span class="hljs-operator">|</span>DIRECTED]<span class="hljs-operator">-</span>(t)<br /><span class="hljs-keyword">WITH</span> m, type(r) <span class="hljs-keyword">as</span> type, <span class="hljs-keyword">collect</span>(t.name) <span class="hljs-keyword">as</span> names<br /><span class="hljs-keyword">WITH</span> m, type<span class="hljs-operator">+</span>&quot;: &quot;<span class="hljs-operator">+</span>reduce(s<span class="hljs-operator">=</span>&quot;&quot;, n <span class="hljs-keyword">IN</span> names <span class="hljs-operator">|</span> s <span class="hljs-operator">+</span> n <span class="hljs-operator">+</span> &quot;, &quot;) <span class="hljs-keyword">as</span> types<br /><span class="hljs-keyword">WITH</span> m, <span class="hljs-keyword">collect</span>(types) <span class="hljs-keyword">as</span> contexts<br /><span class="hljs-keyword">WITH</span> m, &quot;Movie title: &quot;<span class="hljs-operator">+</span> m.title <span class="hljs-operator">+</span> &quot; year: &quot;<span class="hljs-operator">+</span><span class="hljs-built_in">coalesce</span>(m.released,&quot;&quot;) <span class="hljs-operator">+</span>&quot; plot: &quot;<span class="hljs-operator">+</span> <span class="hljs-built_in">coalesce</span>(m.tagline,&quot;&quot;)<span class="hljs-operator">+</span>&quot;\n&quot; <span class="hljs-operator">+</span><br />       reduce(s<span class="hljs-operator">=</span>&quot;&quot;, c <span class="hljs-keyword">in</span> contexts <span class="hljs-operator">|</span> s <span class="hljs-operator">+</span> <span class="hljs-built_in">substring</span>(c, <span class="hljs-number">0</span>, size(c)<span class="hljs-number">-2</span>) <span class="hljs-operator">+</span>&quot;\n&quot;) <span class="hljs-keyword">as</span> context<br /><span class="hljs-keyword">RETURN</span> context LIMIT <span class="hljs-number">1</span></span></pre><p name="8cf7" id="8cf7" class="graf graf--p graf-after--pre">The query return the following context for the Matrix movie.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="d446" id="d446" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Movie title: The Matrix <span class="hljs-keyword">year</span>: <span class="hljs-number">1999</span> plot: Welcome <span class="hljs-keyword">to</span> the <span class="hljs-type">Real</span> World<br />ACTED_IN: Emil Eifrem, Hugo Weaving, Laurence Fishburne, Carrie<span class="hljs-operator">-</span>Anne Moss, Keanu Reeves<br />DIRECTED: Lana Wachowski, Lilly Wachowski</span></pre><p name="d835" id="d835" class="graf graf--p graf-after--pre">Depending on your domain, you might also use custom queries that retrieve information more than one hop away or sometimes want to aggregate some results.</p><p name="0c96" id="0c96" class="graf graf--p graf-after--p">We will now use OpenAI’s embedding endpoint to generate text embeddings representing the movies and their context and store them as node properties.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="7326" id="7326" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">CALL</span> apoc.periodic.iterate(<br />  <span class="hljs-string">&#x27;MATCH (m:Movie) RETURN id(m) AS id&#x27;</span>,<br />  <span class="hljs-string">&#x27;MATCH (m:Movie)<br />   WHERE id(m) = id<br />   MATCH (m)-[r:ACTED_IN|DIRECTED]-(t)<br />   WITH m, type(r) as type, collect(t.name) as names<br />   WITH m, type+&quot;: &quot;+reduce(s=&quot;&quot;, n IN names | s + n + &quot;, &quot;) as types<br />   WITH m, collect(types) as contexts<br />   WITH m, &quot;Movie title: &quot;+ m.title + &quot; year: &quot;+coalesce(m.released,&quot;&quot;) +&quot; plot: &quot;+ coalesce(m.tagline,&quot;&quot;)+&quot;\n&quot; +<br />        reduce(s=&quot;&quot;, c in contexts | s + substring(c, 0, size(c)-2) +&quot;\n&quot;) as context<br />   CALL apoc.ml.openai.embedding([context], $apiKey) YIELD embedding<br />   SET m.embedding = embedding&#x27;</span>,<br />  {batchSize:<span class="hljs-number">1</span>, retries:<span class="hljs-number">3</span>, params: {apiKey: $apiKey}})</span></pre><p name="b2be" id="b2be" class="graf graf--p graf-after--pre">The newly added <code class="markup--code markup--p-code">apoc.ml.openai.embedding</code>procedures make generating text embeddings very easy using OpenAI’s API. We wrap the API call with <code class="markup--code markup--p-code">apoc.periodic.iterate</code> to batch the transactions and introduce the retry policy.</p><h4 name="f57a" id="f57a" class="graf graf--h4 graf-after--p">Retrieval-augmented LLMs</h4><p name="2bc6" id="2bc6" class="graf graf--p graf-after--h4">It looks like the mainstream trend is to provide LLMs with external information at query time. We can even find OpenAI’s guides how to <a href="https://platform.openai.com/docs/guides/gpt-best-practices/strategy-provide-reference-text" data-href="https://platform.openai.com/docs/guides/gpt-best-practices/strategy-provide-reference-text" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">provide relevant information as part of the prompt to generate the answer</a>.</p><figure name="a182" id="a182" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*zydD2GKzjpEyvL-d_cP0vA.png" data-width="1196" data-height="597" src="https://cdn-images-1.medium.com/max/800/1*zydD2GKzjpEyvL-d_cP0vA.png"><figcaption class="imageCaption">Retrieval-augmented approach to LLMs. Image by the author.</figcaption></figure><p name="6854" id="6854" class="graf graf--p graf-after--figure">Here, we will use vector similarity search to find relevant movies given the user input. The workflow is the following:</p><ul class="postList"><li name="f05b" id="f05b" class="graf graf--li graf-after--p">We embed the user question with the same text embedding model we used to embed node context information</li><li name="abf3" id="abf3" class="graf graf--li graf-after--li">We use the cosine similarity to find the top 3 most relevant nodes and return their information to the LLM</li><li name="7e8d" id="7e8d" class="graf graf--li graf-after--li">The LLM constructs the final answer based on the provided information</li></ul><p name="c98b" id="c98b" class="graf graf--p graf-after--li">Since we will be using the <strong class="markup--strong markup--p-strong">gpt-3.5-turbo</strong> model to generate the final answer, it is a good practice to define the system prompt. To make it more readable, we will define the system prompt as Python variable and then use query parameters when executing Cypher statements.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="8f8e" id="8f8e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">system_prompt = <span class="hljs-string">&quot;&quot;&quot;<br />You are an assistant that helps to generate text to form nice and human understandable answers based.<br />The latest prompt contains the information, and you need to generate a human readable response based on the given information.<br />Make the answer sound as a response to the question. Do not mention that you based the result on the given information.<br />Do not add any additional information that is not explicitly provided in the latest prompt.<br />I repeat, do not add any information that is not explicitly given.<br />&quot;&quot;&quot;</span></span></pre><p name="b8b6" id="b8b6" class="graf graf--p graf-after--pre">Next, we will define a function that constructs a user prompt based on the user question and the provided context from the database.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="552b" id="552b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_user_prompt</span>(<span class="hljs-params">question, context</span>):<br />   <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;&quot;&quot;<br />   The question is <span class="hljs-subst">{question}</span><br />   Answer the question by using the provided information:<br />   <span class="hljs-subst">{context}</span><br />   &quot;&quot;&quot;</span></span></pre><p name="b0fe" id="b0fe" class="graf graf--p graf-after--pre">Before asking the LLM to generate answers, we must define the intelligent search tool that will provide relevant context information based on the vector similarity search. As mentioned, we need to embed the user input and then use the cosine similarity to identify relevant nodes. With graphs, you can decide the type of information you want to retrieve and provide as context. In this example, we will return the same context information that was used to generate text embeddings along with similar movie information.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f86d" id="f86d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">retrieve_context</span>(<span class="hljs-params">question, k=<span class="hljs-number">3</span></span>):<br />  data = run_query(<span class="hljs-string">&quot;&quot;&quot;<br />    // retrieve the embedding of the question<br />    CALL apoc.ml.openai.embedding([$question], $apiKey) YIELD embedding<br />    // match relevant movies<br />    MATCH (m:Movie)<br />    WITH m, gds.similarity.cosine(embedding, m.embedding) AS score<br />    ORDER BY score DESC<br />    // limit the number of relevant documents<br />    LIMIT toInteger($k)<br />    // retrieve graph context<br />    MATCH (m)--()--(m1:Movie)<br />    WITH m,m1, count(*) AS count<br />    ORDER BY count DESC<br />    WITH m, apoc.text.join(collect(m1.title)[..3], &quot;, &quot;) AS similarMovies<br />    MATCH (m)-[r:ACTED_IN|DIRECTED]-(t)<br />    WITH m, similarMovies, type(r) as type, collect(t.name) as names<br />    WITH m, similarMovies, type+&quot;: &quot;+reduce(s=&quot;&quot;, n IN names | s + n + &quot;, &quot;) as types<br />    WITH m, similarMovies, collect(types) as contexts<br />    WITH m, &quot;Movie title: &quot;+ m.title + &quot; year: &quot;+coalesce(m.released,&quot;&quot;) +&quot; plot: &quot;+ coalesce(m.tagline,&quot;&quot;)+&quot;\n&quot; +<br />          reduce(s=&quot;&quot;, c in contexts | s + substring(c, 0, size(c)-2) +&quot;\n&quot;) + &quot;similar movies:&quot; + similarMovies + &quot;\n&quot; as context<br />    RETURN context<br />  &quot;&quot;&quot;</span>, {<span class="hljs-string">&#x27;question&#x27;</span>: question, <span class="hljs-string">&#x27;k&#x27;</span>: k, <span class="hljs-string">&#x27;apiKey&#x27;</span>: openai_api_key})<br />  <span class="hljs-keyword">return</span> data[<span class="hljs-string">&#x27;context&#x27;</span>].to_list()</span></pre><p name="d3a2" id="d3a2" class="graf graf--p graf-after--pre">At the moment, you need to use the <code class="markup--code markup--p-code">gds.similarity.cosine</code> function to calculate the cosine similarity between the question and relevant nodes. After identifying the relevant nodes, we retrieve the context using two additional <code class="markup--code markup--p-code">MATCH</code>clauses. You can check out <a href="https://graphacademy.neo4j.com/" data-href="https://graphacademy.neo4j.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Neo4j’s GraphAcademy</a> to learn more about Cypher query language.</p><p name="fce4" id="fce4" class="graf graf--p graf-after--p">Finally, we can define the function that takes in the user question and returns an answer.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="d184" id="d184" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_answer</span>(<span class="hljs-params">question</span>):<br />    <span class="hljs-comment"># Retrieve context</span><br />    context = retrieve_context(question)<br />    <span class="hljs-comment"># Print context</span><br />    <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> context:<br />        <span class="hljs-built_in">print</span>(c)<br />    <span class="hljs-comment"># Generate answer</span><br />    response = run_query(<br />        <span class="hljs-string">&quot;&quot;&quot;<br />  CALL apoc.ml.openai.chat([{role:&#x27;system&#x27;, content: $system},<br />                      {role: &#x27;user&#x27;, content: $user}], $apiKey) YIELD value<br />  RETURN value.choices[0].message.content AS answer<br />  &quot;&quot;&quot;</span>,<br />        {<br />            <span class="hljs-string">&quot;system&quot;</span>: system_prompt,<br />            <span class="hljs-string">&quot;user&quot;</span>: generate_user_prompt(question, context),<br />            <span class="hljs-string">&quot;apiKey&quot;</span>: openai_api_key,<br />        },<br />    )<br />    <span class="hljs-keyword">return</span> response[<span class="hljs-string">&quot;answer&quot;</span>][<span class="hljs-number">0</span>]</span></pre><p name="aba9" id="aba9" class="graf graf--p graf-after--pre">Let’s test our retrieval-augmented LLM workflow.</p><figure name="ac25" id="ac25" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*blhrFk4t5JIkOGVWq_l2LA.png" data-width="795" data-height="321" src="https://cdn-images-1.medium.com/max/800/1*blhrFk4t5JIkOGVWq_l2LA.png"><figcaption class="imageCaption">Image by author</figcaption></figure><p name="77ed" id="77ed" class="graf graf--p graf-after--figure">We can observe that the workflow first retrieves relevant movies and uses that information to generate an answer. An easter egg is hidden here that Emil Eifrem supposedly played in The Matrix.</p><p name="61ee" id="61ee" class="graf graf--p graf-after--p">Let’s try another one.</p><figure name="6530" id="6530" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*WbyauKI9iAeD8Rr-5EymmQ.png" data-width="798" data-height="351" src="https://cdn-images-1.medium.com/max/800/1*WbyauKI9iAeD8Rr-5EymmQ.png"><figcaption class="imageCaption">Image by author</figcaption></figure><p name="cff6" id="cff6" class="graf graf--p graf-after--figure">The vector similarity search was able to retrieve movies where Jack Nicholson is mentioned and use that information to construct the answer. As mentioned, we can retrieve information from the graph that wasn’t included in the text embedding generation of the node.</p><figure name="16b1" id="16b1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*e3a9b0lE6dWFBIPZQhr--g.png" data-width="795" data-height="327" src="https://cdn-images-1.medium.com/max/800/1*e3a9b0lE6dWFBIPZQhr--g.png"><figcaption class="imageCaption">Image by author</figcaption></figure><h4 name="e1b3" id="e1b3" class="graf graf--h4 graf-after--figure">Summary</h4><p name="6e51" id="6e51" class="graf graf--p graf-after--h4">And there you have it, a glimpse into the fascinating world of integrating Large Language Models with Knowledge Graphs. As the field continues to evolve, so too will the tools and techniques at our disposal. With Neo4j and APOC’s continued advancements, we can expect even greater innovation in how we handle and process data.</p><p name="8856" id="8856" class="graf graf--p graf-after--p graf--trailing">As always, the code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/llm/Neo4jOpenAIApoc.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/llm/Neo4jOpenAIApoc.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/27ef7e9900a2"><time class="dt-published" datetime="2023-06-07T14:38:25.422Z">June 7, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/integrate-llm-workflows-with-knowledge-graph-using-neo4j-and-apoc-27ef7e9900a2" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>