<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>How to implement Weaviate RAG applications with Local LLMs and Embedding models</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">How to implement Weaviate RAG applications with Local LLMs and Embedding models</h1>
</header>
<section data-field="subtitle" class="p-summary">
Develop RAG applications and don’t share your private data with anyone!
</section>
<section data-field="body" class="e-content">
<section name="2290" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1460" id="1460" class="graf graf--h3 graf--leading graf--title">How to implement Weaviate RAG applications with Local LLMs and Embedding models</h3><h4 name="7cdf" id="7cdf" class="graf graf--h4 graf-after--h3 graf--subtitle">Develop RAG applications and don’t share your private data with anyone!</h4><p name="80bd" id="80bd" class="graf graf--p graf-after--h4">In the spirit of Hacktoberfest, I decided to write a blog post using a vector database for change. The main reason for that is that in spirit of open source love, I have to give something back to <a href="https://medium.com/u/fcd92e552939" data-href="https://medium.com/u/fcd92e552939" data-anchor-type="2" data-user-id="fcd92e552939" data-action-value="fcd92e552939" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Philip Vollet</a> in exchange for all the significant exposure he provided me, starting from many years ago.</p><p name="3886" id="3886" class="graf graf--p graf-after--p">Philip works at <a href="https://weaviate.io/" data-href="https://weaviate.io/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Weaviate</a>, which is a vector database, and vector similarity search is prevalent in retrieval-augmented applications nowadays. As you might imagine, we will be using Weaviate to power our RAG application. In addition, we’ll be using local LLM and embedding models, making it safe and convenient when dealing with private and confidential information that mustn’t leave your premises.</p><figure name="cc4e" id="cc4e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2wrx2joD1PaAsC27pshn3A.png" data-width="1082" data-height="646" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*2wrx2joD1PaAsC27pshn3A.png"><figcaption class="imageCaption">Agenda for this blog post. Image by author</figcaption></figure><p name="05ba" id="05ba" class="graf graf--p graf-after--figure">They say that knowledge is power, and <a href="https://www.youtube.com/@hubermanlab" data-href="https://www.youtube.com/@hubermanlab" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Huberman Labs podcast</a> is one of the finer source of information of scientific discussion and scientific-based tools to enhance your life. In this blog post, we will use LangChain to fetch podcast captions from YouTube, embed and store them in Weaviate, and then use a local LLM to build a RAG application.</p><p name="31ed" id="31ed" class="graf graf--p graf-after--p">The code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/weaviate/HubermanWeaviate.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/weaviate/HubermanWeaviate.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p><h4 name="2e63" id="2e63" class="graf graf--h4 graf-after--p">Weaviate cloud services</h4><p name="0f8b" id="0f8b" class="graf graf--p graf-after--h4">To follow the examples in this blog post, you first need to <a href="https://console.weaviate.cloud/" data-href="https://console.weaviate.cloud/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">register with WCS</a>. Once you are registered, you can create a new Weaviate Cluster by clicking the “Create cluster” button. For this tutorial, we will be using the free trial plan, which will provide you with a sandbox for 14 days.</p><p name="6498" id="6498" class="graf graf--p graf-after--p">For the next steps, you will need the following two pieces of information to access your cluster:</p><ul class="postList"><li name="d126" id="d126" class="graf graf--li graf-after--p">The cluster URL</li><li name="a31d" id="a31d" class="graf graf--li graf-after--li">Weaviate API key (under “Enabled — Authentication”)</li></ul><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="07b1" id="07b1" class="graf graf--pre graf-after--li graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> weaviate<br /><br />WEAVIATE_URL = <span class="hljs-string">&quot;WEAVIATE_CLUSTER_URL&quot;</span><br />WEAVIATE_API_KEY = <span class="hljs-string">&quot;WEAVIATE_API_KEY&quot;</span><br /><br />client = weaviate.Client(<br />    url=WEAVIATE_URL, auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY)<br />)</span></pre><h4 name="c981" id="c981" class="graf graf--h4 graf-after--pre">Local embedding and LLM models</h4><p name="9d2d" id="9d2d" class="graf graf--p graf-after--h4">I am most familiar with the LangChain LLM framework, so we will be using it to ingest documents as well as retrieve them. We will be using <code class="markup--code markup--p-code">sentence_transformers/all-mpnet-base-v2</code> embedding model and <code class="markup--code markup--p-code">zephyr-7b-alpha</code> llm. Both of these models are open source and available on HuggingFace. The implementation code for these two models in LangChain was kindly borrowed from the following repository:</p><div name="2893" id="2893" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/aigeek0x0/zephyr-7b-alpha-langchain-chatbot" data-href="https://github.com/aigeek0x0/zephyr-7b-alpha-langchain-chatbot" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/aigeek0x0/zephyr-7b-alpha-langchain-chatbot"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - aigeek0x0/zephyr-7b-alpha-langchain-chatbot: Chat with PDF using Zephyr 7B Alpha…</strong><br><em class="markup--em markup--mixtapeEmbed-em">Chat with PDF using Zephyr 7B Alpha, Langchain, ChromaDB, and Gradio with Free Google Colab - GitHub …</em>github.com</a><a href="https://github.com/aigeek0x0/zephyr-7b-alpha-langchain-chatbot" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="04985615015d251e4a7e576de3834f0d" data-thumbnail-img-id="0*ZSklyyoLj5vPWxhj" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*ZSklyyoLj5vPWxhj);"></a></div><p name="acd1" id="acd1" class="graf graf--p graf-after--mixtapeEmbed"><em class="markup--em markup--p-em">If you are using Google Collab environment, make sure to use GPU runtime.</em></p><p name="981a" id="981a" class="graf graf--p graf-after--p">We will begin by defining the embedding model, which can be easily retrieved from HuggingFace using the following code:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="414c" id="414c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># specify embedding model (using huggingface sentence transformer)</span><br />embedding_model_name = <span class="hljs-string">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span><br />model_kwargs = {<span class="hljs-string">&quot;device&quot;</span>: <span class="hljs-string">&quot;cuda&quot;</span>}<br />embeddings = HuggingFaceEmbeddings(<br />  model_name=embedding_model_name, <br />  model_kwargs=model_kwargs<br />)</span></pre><h4 name="b839" id="b839" class="graf graf--h4 graf-after--pre">Ingest HubermanLabs podcasts into Weaviate</h4><p name="9620" id="9620" class="graf graf--p graf-after--h4">I have learned that each channel on YouTube has an RSS feed, that can be used to fetch links to the latest 10 videos. As the RSS feed returns a XML, we need to employ a simple Python script to extract the links.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="d067" id="d067" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">import requests<br />import xml.etree.ElementTree as ET<br /><br />URL = <span class="hljs-string">&quot;https://www.youtube.com/feeds/videos.xml?channel_id=UC2D2CMWXMOVWx7giW1n3LIg&quot;</span><br />response = requests.get(URL)<br />xml_data = response.content<br /><br /><span class="hljs-comment"># Parse the XML data</span><br />root = ET.fromstring(xml_data)<br /><span class="hljs-comment"># Define the namespace</span><br />namespaces = {<br />    <span class="hljs-string">&quot;atom&quot;</span>: <span class="hljs-string">&quot;http://www.w3.org/2005/Atom&quot;</span>,<br />    <span class="hljs-string">&quot;media&quot;</span>: <span class="hljs-string">&quot;http://search.yahoo.com/mrss/&quot;</span>,<br />}<br /><span class="hljs-comment"># Extract YouTube links</span><br />youtube_links = [<br />    link.get(<span class="hljs-string">&quot;href&quot;</span>)<br />    for link in root.findall(<span class="hljs-string">&quot;.//atom:link[@rel=&#x27;alternate&#x27;]&quot;</span>, namespaces)<br /><span class="hljs-section">][1:]</span></span></pre><p name="f87b" id="f87b" class="graf graf--p graf-after--pre">Now that we have the links to the videos at hand, we can use the <code class="markup--code markup--p-code">YoutubeLoader</code> from LangChain to retrieve the captions. Next, as with most RAG ingestions pipelines, we have to chunk the text into smaller pieces before ingestion. We can use the text splitter functionality that is built into LangChain.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="7d95" id="7d95" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> YoutubeLoader<br /><br />all_docs = []<br /><span class="hljs-keyword">for</span> link <span class="hljs-keyword">in</span> youtube_links:<br />    <span class="hljs-comment"># Retrieve captions</span><br />    loader = YoutubeLoader.from_youtube_url(link)<br />    docs = loader.load()<br />    all_docs.extend(docs)<br /><span class="hljs-comment"># Split documents</span><br />text_splitter = TokenTextSplitter(chunk_size=<span class="hljs-number">128</span>, chunk_overlap=<span class="hljs-number">0</span>)<br />split_docs = text_splitter.split_documents(all_docs)<br /><br /><span class="hljs-comment"># Ingest the documents into Weaviate</span><br />vector_db = Weaviate.from_documents(<br />    split_docs, embeddings, client=client, by_text=<span class="hljs-literal">False</span><br />)</span></pre><p name="ceae" id="ceae" class="graf graf--p graf-after--pre">You can test the vector retriever using the following code:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4b19" id="4b19" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-built_in">print</span>(<br />    vector_db.similarity_search(<br />        <span class="hljs-string">&quot;Which are tools to bolster your mental health?&quot;</span>, k=<span class="hljs-number">3</span>)<br />    )</span></pre><h4 name="e6bb" id="e6bb" class="graf graf--h4 graf-after--pre">Setting up a local LLM</h4><p name="fc39" id="fc39" class="graf graf--p graf-after--h4">This part of the code was completely <a href="https://github.com/aigeek0x0/zephyr-7b-alpha-langchain-chatbot" data-href="https://github.com/aigeek0x0/zephyr-7b-alpha-langchain-chatbot" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">copied from the example provided by the AI Geek</a>. It loads the <strong class="markup--strong markup--p-strong">zephyr-7b-alpha-sharded</strong> model and its tokenizer from HuggingFace and loads it as a LangChain LLM module.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="e458" id="e458" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># specify model huggingface mode name</span><br />model_name = <span class="hljs-string">&quot;anakin87/zephyr-7b-alpha-sharded&quot;</span><br /><br /><span class="hljs-comment"># function for loading 4-bit quantized model</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_quantized_model</span>(<span class="hljs-params">model_name: <span class="hljs-built_in">str</span></span>):<br />    <span class="hljs-string">&quot;&quot;&quot;<br />    :param model_name: Name or path of the model to be loaded.<br />    :return: Loaded quantized model.<br />    &quot;&quot;&quot;</span><br />    bnb_config = BitsAndBytesConfig(<br />        load_in_4bit=<span class="hljs-literal">True</span>,<br />        bnb_4bit_use_double_quant=<span class="hljs-literal">True</span>,<br />        bnb_4bit_quant_type=<span class="hljs-string">&quot;nf4&quot;</span>,<br />        bnb_4bit_compute_dtype=torch.bfloat16,<br />    )<br /><br />    model = AutoModelForCausalLM.from_pretrained(<br />        model_name,<br />        load_in_4bit=<span class="hljs-literal">True</span>,<br />        torch_dtype=torch.bfloat16,<br />        quantization_config=bnb_config,<br />    )<br />    <span class="hljs-keyword">return</span> model<br /><br /><span class="hljs-comment"># function for initializing tokenizer</span><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_tokenizer</span>(<span class="hljs-params">model_name: <span class="hljs-built_in">str</span></span>):<br />    <span class="hljs-string">&quot;&quot;&quot;<br />    Initialize the tokenizer with the specified model_name.<br /><br />    :param model_name: Name or path of the model for tokenizer initialization.<br />    :return: Initialized tokenizer.<br />    &quot;&quot;&quot;</span><br />    tokenizer = AutoTokenizer.from_pretrained(model_name, return_token_type_ids=<span class="hljs-literal">False</span>)<br />    tokenizer.bos_token_id = <span class="hljs-number">1</span>  <span class="hljs-comment"># Set beginning of sentence token id</span><br />    <span class="hljs-keyword">return</span> tokenizer<br /><br /><br /><span class="hljs-comment"># initialize tokenizer</span><br />tokenizer = initialize_tokenizer(model_name)<br /><span class="hljs-comment"># load model</span><br />model = load_quantized_model(model_name)<br /><span class="hljs-comment"># specify stop token ids</span><br />stop_token_ids = [<span class="hljs-number">0</span>]<br /><br /><br /><span class="hljs-comment"># build huggingface pipeline for using zephyr-7b-alpha</span><br />pipeline = pipeline(<br />    <span class="hljs-string">&quot;text-generation&quot;</span>,<br />    model=model,<br />    tokenizer=tokenizer,<br />    use_cache=<span class="hljs-literal">True</span>,<br />    device_map=<span class="hljs-string">&quot;auto&quot;</span>,<br />    max_length=<span class="hljs-number">2048</span>,<br />    do_sample=<span class="hljs-literal">True</span>,<br />    top_k=<span class="hljs-number">5</span>,<br />    num_return_sequences=<span class="hljs-number">1</span>,<br />    eos_token_id=tokenizer.eos_token_id,<br />    pad_token_id=tokenizer.eos_token_id,<br />)<br /><br /><span class="hljs-comment"># specify the llm</span><br />llm = HuggingFacePipeline(pipeline=pipeline)</span></pre><p name="28a8" id="28a8" class="graf graf--p graf-after--pre">I haven’t played around yet, but you could probably reuse this code to load other LLMs from HuggingFace.</p><h4 name="6760" id="6760" class="graf graf--h4 graf-after--p">Building a conversation chain</h4><p name="7c4b" id="7c4b" class="graf graf--p graf-after--h4">Now that we have our vector retrieval and th LLM ready, we can implement a retrieval-augmented chatbot in only a couple lines of code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="fda1" id="fda1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">qa_chain = RetrievalQA.from_chain_type(<br />    llm=llm, chain_type=<span class="hljs-string">&quot;stuff&quot;</span>, retriever=vector_db.as_retriever()<br />)</span></pre><p name="db98" id="db98" class="graf graf--p graf-after--pre">Let’s now test how well it works:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="5d62" id="5d62" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">response = qa_chain.run(<br />    <span class="hljs-string">&quot;How does one increase their mental health?&quot;</span>)<br /><span class="hljs-built_in">print</span>(response)</span></pre><figure name="5a87" id="5a87" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*b8pT6KIERk3ZPd-729TmXA.png" data-width="669" data-height="167" src="https://cdn-images-1.medium.com/max/800/1*b8pT6KIERk3ZPd-729TmXA.png"></figure><p name="ac0f" id="ac0f" class="graf graf--p graf-after--figure">Let’s try another one:</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="b5f8" id="b5f8" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">response = qa_chain.run(<span class="hljs-string">&quot;How to increase your willpower?&quot;</span>)<br /><span class="hljs-built_in">print</span>(response)</span></pre><figure name="4a6c" id="4a6c" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*oY5JoNIikQZj3RsXyMrb3w.png" data-width="656" data-height="181" src="https://cdn-images-1.medium.com/max/800/1*oY5JoNIikQZj3RsXyMrb3w.png"></figure><h4 name="52a1" id="52a1" class="graf graf--h4 graf-after--figure">Summary</h4><p name="1069" id="1069" class="graf graf--p graf-after--h4">Only a couple of months ago, most of us didn’t realize that we will be able to run LLMs on our laptop or free-tier Google Collab so soon. Many RAG applications deal with private and confidential data, where it can’t be shared with third-party LLM providers. In those cases, using a local embedding and LLM models as described in this blog post is the ideal solution.</p><p name="08a6" id="08a6" class="graf graf--p graf-after--p graf--trailing">As always, the code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/weaviate/HubermanWeaviate.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/weaviate/HubermanWeaviate.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/24a9128eaf84"><time class="dt-published" datetime="2023-10-30T17:56:20.011Z">October 30, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/how-to-implement-weaviate-rag-applications-with-local-llms-and-embedding-models-24a9128eaf84" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>