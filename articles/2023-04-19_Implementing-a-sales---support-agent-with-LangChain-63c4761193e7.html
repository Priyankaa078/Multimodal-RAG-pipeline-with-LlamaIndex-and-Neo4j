<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Implementing a sales &amp; support agent with LangChain</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Implementing a sales &amp; support agent with LangChain</h1>
</header>
<section data-field="subtitle" class="p-summary">
Learn how to develop a chatbot that can answer questions based on the information provided in your company’s documentation
</section>
<section data-field="body" class="e-content">
<section name="2993" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b2dd" id="b2dd" class="graf graf--h3 graf--leading graf--title">Implementing a sales &amp; support agent with LangChain</h3><h4 name="0749" id="0749" class="graf graf--h4 graf-after--h3 graf--subtitle">Learn how to develop a chatbot that can answer questions based on the information provided in your company’s documentation</h4><p name="994d" id="994d" class="graf graf--p graf-after--h4">Recently, I have been fascinated by the power of ChatGPT and its ability to construct various types of chatbots. I have tried and written about multiple approaches to implementing a chatbot that can access external information to improve its answers. I joined a few Discord channels during my chatbot coding sessions, hoping to get some help as the libraries are relatively new, and not much documentation is available yet. To my amazement, I found custom bots that could answer most of the questions for the given library.</p><figure name="3bf3" id="3bf3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*88Recu-5w0cdIrPqXdTyJA.png" data-width="1281" data-height="590" src="https://cdn-images-1.medium.com/max/800/1*88Recu-5w0cdIrPqXdTyJA.png"><figcaption class="imageCaption">Example of a discord support bot. Image by the author.</figcaption></figure><p name="36f2" id="36f2" class="graf graf--p graf-after--figure">The idea is to provide the chatbot the ability to dig through various resources like company documentation, code, or other content in order to allow it to answer company support questions. Since I already have some experience with chatbots, I decided to test how hard it is to implement a custom bot with access to the company’s resources.</p><p name="511b" id="511b" class="graf graf--p graf-after--p">In this blog post, I will walk you through how I used OpenAI’s models to implement a sales &amp; support agent with in the <a href="https://python.langchain.com/en/latest/index.html" data-href="https://python.langchain.com/en/latest/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LangChain library</a> that can be used to answer information about applications with a <a href="https://neo4j.com/" data-href="https://neo4j.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">graph database Neo4j</a>. The agent can also help you debug or produce any Cypher statement you are struggling with. Such an agent could then be deployed to serve users on Discord or other platforms.</p><figure name="134e" id="134e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MebmJBA4ehKO0mAqbkvjMg.png" data-width="550" data-height="375" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*MebmJBA4ehKO0mAqbkvjMg.png"><figcaption class="imageCaption">Agent design. Image by the author.</figcaption></figure><p name="87dd" id="87dd" class="graf graf--p graf-after--figure">We will be using the <a href="https://python.langchain.com/en/latest/index.html" data-href="https://python.langchain.com/en/latest/index.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LangChain library</a> to implement the support bot. The library is easy to use and provides an excellent integration of LLM prompts and Python code, allowing us to develop chatbots in only a few minutes. In addition, the library supports a range of LLMs, text embedding models, and vector databases, along with utility functions that help us load and embed frequent types of files we might come across, like text, PowerPoint, images, HTML, PDF, and more.</p><p name="0d6b" id="0d6b" class="graf graf--p graf-after--p">The code for this blog post is available on GitHub.</p><div name="5e15" id="5e15" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb"><strong class="markup--strong markup--mixtapeEmbed-strong">blogs/neo4j_support_bot.ipynb at master · tomasonjo/blogs</strong><br><em class="markup--em markup--mixtapeEmbed-em">You can&#39;t perform that action at this time. You signed in with another tab or window. You signed out in another tab or…</em>github.com</a><a href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="061360b637971ed4a0a875f38336ca32" data-thumbnail-img-id="0*p6QWAMrdBaQfKqMu" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*p6QWAMrdBaQfKqMu);"></a></div><h4 name="a03f" id="a03f" class="graf graf--h4 graf-after--mixtapeEmbed">LangChain document loaders</h4><p name="92c9" id="92c9" class="graf graf--p graf-after--h4">First, we must preprocess the company’s resources and store them in a vector database. Luckily, LangChain can help us load external data, calculate text embeddings, and store the documents in a vector database of our choice.</p><p name="3342" id="3342" class="graf graf--p graf-after--p">First, we have to load the text into documents. LangChain offers a variety of <a href="https://python.langchain.com/en/latest/modules/indexes/document_loaders.html?highlight=document%20loaders" data-href="https://python.langchain.com/en/latest/modules/indexes/document_loaders.html?highlight=document%20loaders" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">helper functions that can take various formats and types of data and produce a document output</a>. The helper functions are called Document loaders.</p><p name="af66" id="af66" class="graf graf--p graf-after--p">Neo4j has a lot of its documentation available in GitHub repositories. Conveniently, LangChain provides a document loader that takes a repository URL as input and produces a document for each file in the repository. Additionally, we can use the filter function to ignore files during the loading process if needed.</p><p name="9c48" id="9c48" class="graf graf--p graf-after--p">We will begin by loading the AsciiDoc files from the <a href="https://github.com/neo4j-documentation/knowledge-base" data-href="https://github.com/neo4j-documentation/knowledge-base" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Neo4j’s knowledge base repository</a>.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="caed" id="caed" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Knowledge base</span><br />kb_loader = GitLoader(<br />    clone_url=<span class="hljs-string">&quot;https://github.com/neo4j-documentation/knowledge-base&quot;</span>,<br />    repo_path=<span class="hljs-string">&quot;./repos/kb/&quot;</span>,<br />    branch=<span class="hljs-string">&quot;master&quot;</span>,<br />    file_filter=<span class="hljs-keyword">lambda</span> file_path: file_path.endswith(<span class="hljs-string">&quot;.adoc&quot;</span>)<br />    <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;articles&quot;</span> <span class="hljs-keyword">in</span> file_path,<br />)<br />kb_data = kb_loader.load()<br /><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(kb_data)) <span class="hljs-comment"># 309</span></span></pre><p name="5e2e" id="5e2e" class="graf graf--p graf-after--pre">Wasn’t that easy as a pie? The <code class="markup--code markup--p-code">GitLoader</code> function clones the repository and load relevant files as documents. In this example, we specified that the file must end with <code class="markup--code markup--p-code">.adoc</code> suffix and be a part of the <code class="markup--code markup--p-code">articles</code> folder. In total, 309 articles were loaded. We also have to be mindful of the size of the documents. For example, GPT-3.5-turbo has a token limit of 4000, while GPT-4 allows 8000 tokens in a single request. While number of words is not exactly identical to the number of tokens, it is still a good estimator.</p><p name="6097" id="6097" class="graf graf--p graf-after--p">Next, we will load the documentation of the <a href="https://github.com/neo4j/graph-data-science" data-href="https://github.com/neo4j/graph-data-science" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Graph Data Science repository</a>. Here, we will use a text splitter to make sure none of the documents exceed 2000 words. Again, I know that number of words is not equal to the number of tokens, but it is a good approximation. Defining the threshold number of tokens can significantly affect how the database is found and retrieved. I found a <a href="https://www.pinecone.io/learn/chunking-strategies/" data-href="https://www.pinecone.io/learn/chunking-strategies/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">great article by Pinecone that can help you understand the basics of various chunking strategies</a>.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="cbc1" id="cbc1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Define text chunk strategy</span><br />splitter = CharacterTextSplitter(<br />  chunk_size=2000, <br />  chunk_overlap=50,<br />  separator=<span class="hljs-string">&quot; &quot;</span><br />)<br /><span class="hljs-comment"># GDS guides</span><br />gds_loader = GitLoader(<br />    clone_url=<span class="hljs-string">&quot;https://github.com/neo4j/graph-data-science&quot;</span>,<br />    repo_path=<span class="hljs-string">&quot;./repos/gds/&quot;</span>,<br />    branch=<span class="hljs-string">&quot;master&quot;</span>,<br />    file_filter=lambda file_path: file_path.endswith(<span class="hljs-string">&quot;.adoc&quot;</span>) <br />    and <span class="hljs-string">&quot;pages&quot;</span> in file_path,<br />)<br />gds_data = gds_loader.load()<br /><span class="hljs-comment"># Split documents into chunks</span><br />gds_data_split = splitter.split_documents(gds_data)<br />print(len(gds_data_split)) <span class="hljs-comment">#771</span></span></pre><p name="95df" id="95df" class="graf graf--p graf-after--pre">We could load other Neo4j repositories that contain documentation. However, the idea is to show various data loading methods and not explore all of Neo4j’s repositories containing documentation. Therefore, we will move on and look at how we can load documents from a Pandas Dataframe.</p><p name="b411" id="b411" class="graf graf--p graf-after--p">For example, say that we want to load a YouTube video as a document source for our chatbot. Neo4j has its own YouTube channel and, even I appear in a video or two. Two years ago I presented how to implement an information extraction pipeline.</p><figure name="daf0" id="daf0" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://www.youtube.com/embed/1sRgsEKlUr0?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="500c" id="500c" class="graf graf--p graf-after--figure">With LangChain, we can use the captions of the video and load it as documents with only three lines of code.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="9c49" id="9c49" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">yt_loader = YoutubeLoader(<span class="hljs-string">&quot;1sRgsEKlUr0&quot;</span>)<br />yt_data = yt_loader.load()<br />yt_data_split = splitter.split_documents(yt_data)<br /><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(yt_data_split)) <span class="hljs-comment">#10</span></span></pre><p name="debc" id="debc" class="graf graf--p graf-after--pre">It couldn’t get any easier than this. Next, we will look at loading documents from a Pandas dataframe. A month ago, I retrieved information from Neo4j medium publication for a separate blog post. Since we want to bring external information about Neo4j to the bot, we can also use the content of the medium articles.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="3a2c" id="3a2c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">article_url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/medium/neo4j_articles.csv&quot;</span><br />medium = pd.read_csv(article_url, sep=<span class="hljs-string">&quot;;&quot;</span>)<br />medium[<span class="hljs-string">&quot;source&quot;</span>] = medium[<span class="hljs-string">&quot;url&quot;</span>]<br />medium_loader = DataFrameLoader(<br />    medium[[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;source&quot;</span>]], <br />    page_content_column=<span class="hljs-string">&quot;text&quot;</span>)<br />medium_data = medium_loader.load()<br />medium_data_split = splitter.split_documents(medium_data)<br /><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(medium_data_split)) <span class="hljs-comment">#4254</span></span></pre><p name="bd29" id="bd29" class="graf graf--p graf-after--pre">Here, we used Pandas to load a CSV file from GitHub, renamed one column, and used the <code class="markup--code markup--p-code">DataFrameLoader</code>function to load the articles as documents. Since medium posts could exceed 4000 tokens, we used the text splitter to split the articles into multiple chunks.</p><p name="4bb6" id="4bb6" class="graf graf--p graf-after--p">The last source we will use is the Stack Overflow API. Stack Overflow is a web platform where users help others solve coding problems. Their API does not require any authorization. Therefore, we can use the API to retrieve questions with accepted answers that are tagged with the Neo4j tag.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="e7d4" id="e7d4" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">so_data = []<br /><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>):<br />    <span class="hljs-comment"># Define the Stack Overflow API endpoint and parameters</span><br />    api_url = <span class="hljs-string">&quot;https://api.stackexchange.com/2.3/questions&quot;</span><br />    params = {<br />        <span class="hljs-string">&quot;order&quot;</span>: <span class="hljs-string">&quot;desc&quot;</span>,<br />        <span class="hljs-string">&quot;sort&quot;</span>: <span class="hljs-string">&quot;creation&quot;</span>,<br />        <span class="hljs-string">&quot;filter&quot;</span>: <span class="hljs-string">&quot;!-MBrU_IzpJ5H-AG6Bbzy.X-BYQe(2v-.J&quot;</span>,<br />        <span class="hljs-string">&quot;tagged&quot;</span>: <span class="hljs-string">&quot;neo4j&quot;</span>,<br />        <span class="hljs-string">&quot;site&quot;</span>: <span class="hljs-string">&quot;stackoverflow&quot;</span>,<br />        <span class="hljs-string">&quot;pagesize&quot;</span>: <span class="hljs-number">100</span>,<br />        <span class="hljs-string">&quot;page&quot;</span>: i,<br />    }<br />    <span class="hljs-comment"># Send GET request to Stack Overflow API</span><br />    response = requests.get(api_url, params=params)<br />    data = response.json()<br />    <span class="hljs-comment"># Retrieve the resolved questions</span><br />    resolved_questions = [<br />        question<br />        <span class="hljs-keyword">for</span> question <span class="hljs-keyword">in</span> data[<span class="hljs-string">&quot;items&quot;</span>]<br />        <span class="hljs-keyword">if</span> question[<span class="hljs-string">&quot;is_answered&quot;</span>] <span class="hljs-keyword">and</span> question.get(<span class="hljs-string">&quot;accepted_answer_id&quot;</span>)<br />    ]<br /><br />    <span class="hljs-comment"># Print the resolved questions</span><br />    <span class="hljs-keyword">for</span> question <span class="hljs-keyword">in</span> resolved_questions:<br />        text = (<br />            <span class="hljs-string">&quot;Title:&quot;</span>,<br />            question[<span class="hljs-string">&quot;title&quot;</span>] + <span class="hljs-string">&quot;\n&quot;</span> + <span class="hljs-string">&quot;Question:&quot;</span>,<br />            BeautifulSoup(question[<span class="hljs-string">&quot;body&quot;</span>]).get_text()<br />            + <span class="hljs-string">&quot;\n&quot;</span><br />            + BeautifulSoup(<br />                [x[<span class="hljs-string">&quot;body&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> question[<span class="hljs-string">&quot;answers&quot;</span>] <span class="hljs-keyword">if</span> x[<span class="hljs-string">&quot;is_accepted&quot;</span>]][<span class="hljs-number">0</span>]<br />            ).get_text(),<br />        )<br />        source = question[<span class="hljs-string">&quot;link&quot;</span>]<br />        so_data.append(Document(page_content=<span class="hljs-built_in">str</span>(text), metadata={<span class="hljs-string">&quot;source&quot;</span>: source}))<br /><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(so_data)) <span class="hljs-comment">#777</span></span></pre><p name="4574" id="4574" class="graf graf--p graf-after--pre">Each approved answer and the original question are used to construct a single document. Since most Stack overflow questions and answers do not exceed 4000 tokens, we skipped the text-splitting step.</p><p name="0cc3" id="0cc3" class="graf graf--p graf-after--p">Now that we have loaded the documentation resources as documents, we can move on to the next step.</p><h4 name="136d" id="136d" class="graf graf--h4 graf-after--p">Storing documents in a vector database</h4><p name="3658" id="3658" class="graf graf--p graf-after--h4">A chatbot finds relevant information by comparing the vector embedding of questions with document embeddings. A text embedding is a machine-readable representation of text in the form of a vector or, more plainly, a list of floats. In this example, we will use the <strong class="markup--strong markup--p-strong">ada-002</strong> model provided by OpenAI to embed documents.</p><p name="2954" id="2954" class="graf graf--p graf-after--p">The whole idea behind vector databases is the ability to store vectors and provide fast similarity searches. The vectors are usually compared using cosine similarity. LangChain includes <a href="https://python.langchain.com/en/latest/modules/indexes/vectorstores.html" data-href="https://python.langchain.com/en/latest/modules/indexes/vectorstores.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">integration with a variety of vector databases</a>. To keep things simple, we will use the<a href="https://www.trychroma.com/" data-href="https://www.trychroma.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> Chroma vector database</a>, which can be used as a local in-memory. For a more serious chatbot application, we want to use a persistent database that doesn’t lose data once the script or notebook is closed.</p><p name="3103" id="3103" class="graf graf--p graf-after--p">We will create two collections of documents. The first will be more sales and marketing oriented, containing documents from Medium and YouTube. The second collection focuses more on support use cases and consists of documentation and Stack Overflow documents.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="d7f5" id="d7f5" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># Define embedding model</span><br />OPENAI_API_KEY = <span class="hljs-string">&quot;OPENAI_API_KEY&quot;</span><br />embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)<br /><br />sales_data = medium_data_split + yt_data_split<br />sales_store = Chroma.from_documents(<br />    sales_data, embeddings, collection_name=<span class="hljs-string">&quot;sales&quot;</span><br />)<br /><br />support_data = kb_data + gds_data_split + so_data<br />support_store = Chroma.from_documents(<br />    support_data, embeddings, collection_name=<span class="hljs-string">&quot;support&quot;</span><br />)</span></pre><p name="1978" id="1978" class="graf graf--p graf-after--pre">This script runs each document through OpenAI’s text embedding API and inserts the resulting embedding along with text in the Chroma database. The process of text embedding costs 0.80$, which is a reasonable price.</p><h4 name="255d" id="255d" class="graf graf--h4 graf-after--p">Question answering using external context</h4><p name="d901" id="d901" class="graf graf--p graf-after--h4">The last thing to do is to implement two separate question-answering flow. The first will handle the sales &amp; marketing requests, while the other will handle support. The LangChain library uses LLMs for reasoning and providing answers to the user. Therefore, we start by defining the LLM. Here, we will be using the <strong class="markup--strong markup--p-strong">GPT-3.5-turbo</strong> model from OpenAI.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="fa08" id="fa08" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">llm = ChatOpenAI(<br />    model_name=<span class="hljs-string">&quot;gpt-3.5-turbo&quot;</span>,<br />    temperature=<span class="hljs-number">0</span>,<br />    openai_api_key=OPENAI_API_KEY,<br />    max_tokens=<span class="hljs-number">512</span>,<br />)</span></pre><p name="2435" id="2435" class="graf graf--p graf-after--pre">Implementing a question-answering flow is about as easy as it gets with LangChain. We only need to provide the LLM to be used along with the retriever that is used to fetch relevant documents. Additionally, we have the option to customize the LLM prompt used to answer questions.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="4733" id="4733" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">sales_template = <span class="hljs-string">&quot;&quot;&quot;As a Neo4j marketing bot, your goal is to provide accurate <br />and helpful information about Neo4j, a powerful graph database used for <br />building various applications. You should answer user inquiries based on the <br />context provided and avoid making up answers. If you don&#x27;t know the answer, <br />simply state that you don&#x27;t know. Remember to provide relevant information <br />about Neo4j&#x27;s features, benefits, and use cases to assist the user in <br />understanding its value for application development.<br /><br />{context}<br /><br />Question: {question}&quot;&quot;&quot;</span><br />SALES_PROMPT = PromptTemplate(<br />    template=sales_template, input_variables=[<span class="hljs-string">&quot;context&quot;</span>, <span class="hljs-string">&quot;question&quot;</span>]<br />)<br />sales_qa = RetrievalQA.from_chain_type(<br />    llm=llm,<br />    chain_type=<span class="hljs-string">&quot;stuff&quot;</span>,<br />    retriever=sales_store.as_retriever(),<br />    chain_type_kwargs={<span class="hljs-string">&quot;prompt&quot;</span>: SALES_PROMPT},<br />)</span></pre><p name="c18c" id="c18c" class="graf graf--p graf-after--pre">The most important part of the sales prompt is to prohibit the LLM from basing its responses without relying on official company resources. Remember, LLMs can act very assertively while providing invalid information. However, we would like to avoid that scenario and avoid getting into problems where the bot promised or sold non-existing features. We can test the sales question answering flow by asking the following question:</p><figure name="ee4f" id="ee4f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*d_LSAWzUWZ568ZUGrfg8TQ.png" data-width="1090" data-height="362" src="https://cdn-images-1.medium.com/max/800/1*d_LSAWzUWZ568ZUGrfg8TQ.png"><figcaption class="imageCaption">Sales question-answering. Image by the author.</figcaption></figure><p name="3dae" id="3dae" class="graf graf--p graf-after--figure">The response to the question seems relevant and accurate. Remember, the information to construct this response came from Medium articles.</p><p name="18b6" id="18b6" class="graf graf--p graf-after--p">Next, we will implement the support question-answering flow. Here, we will allow the LLM model to use its knowledge of Cypher and Neo4j to help solve the user’s problem if the context doesn’t provide enough information.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="23fb" id="23fb" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">support_template = <span class="hljs-string">&quot;&quot;&quot;<br />As a Neo4j Customer Support bot, you are here to assist with any issues <br />a user might be facing with their graph database implementation and Cypher statements.<br />Please provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.<br />If the provided context doesn&#x27;t provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.<br /><br />{context}<br /><br />Question: {question}&quot;&quot;&quot;</span><br /><br />SUPPORT_PROMPT = PromptTemplate(<br />    template=support_template, input_variables=[<span class="hljs-string">&quot;context&quot;</span>, <span class="hljs-string">&quot;question&quot;</span>]<br />)<br /><br />support_qa = RetrievalQA.from_chain_type(<br />    llm=llm,<br />    chain_type=<span class="hljs-string">&quot;stuff&quot;</span>,<br />    retriever=support_store.as_retriever(),<br />    chain_type_kwargs={<span class="hljs-string">&quot;prompt&quot;</span>: SUPPORT_PROMPT},<br />)</span></pre><p name="3681" id="3681" class="graf graf--p graf-after--pre">And again, we can test the support question-answering abilities. I took a random question from Neo4j’s discord server.</p><figure name="be8d" id="be8d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BCS_7lgO1Tds214kHs0Z1g.png" data-width="1088" data-height="707" src="https://cdn-images-1.medium.com/max/800/1*BCS_7lgO1Tds214kHs0Z1g.png"><figcaption class="imageCaption">Support question-answering. Image by the author</figcaption></figure><p name="a98c" id="a98c" class="graf graf--p graf-after--figure">The response is quite to the point. Remember, we retrieved the Graph Data Science documentation and are using it as context to form the chatbot questions.</p><h4 name="ddf1" id="ddf1" class="graf graf--h4 graf-after--p">Agent implementation</h4><p name="5e77" id="5e77" class="graf graf--p graf-after--h4">We now have two separate instructions and stores for <strong class="markup--strong markup--p-strong">sales</strong> and <strong class="markup--strong markup--p-strong">support</strong> responses. If we had to put a human in the loop to distinguish between the two, the whole point of the chatbot would be lost. Luckily, we can use a LangChain agent to decide which tool to use based on the user input. First, we need to define the available tools of an agent along with instructions on when and how to use them.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="2dc2" id="2dc2" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">tools = [<br />    Tool(<br />        name=<span class="hljs-string">&quot;sales&quot;</span>,<br />        func=sales_qa.run,<br />        description=<span class="hljs-string">&quot;&quot;&quot;useful for when a user is interested in various Neo4j information, <br />                       use-cases, or applications. A user is not asking for any debugging, but is only<br />                       interested in general advice for integrating and using Neo4j.<br />                       Input should be a fully formed question.&quot;&quot;&quot;</span>,<br />    ),<br />    Tool(<br />        name=<span class="hljs-string">&quot;support&quot;</span>,<br />        func=support_qa.run,<br />        description=<span class="hljs-string">&quot;&quot;&quot;useful for when when a user asks to optimize or debug a Cypher statement or needs<br />                       specific instructions how to accomplish a specified task. <br />                       Input should be a fully formed question.&quot;&quot;&quot;</span>,<br />    ),<br />]</span></pre><p name="d587" id="d587" class="graf graf--p graf-after--pre">The description of a tool is used by an agent to identify when and how to use a tool. For example, the support tool should be used to optimize or debug a Cypher statement and the input to the tool should be a fully formed question.</p><p name="1b62" id="1b62" class="graf graf--p graf-after--p">The last thing we need to do is to initialize the agent.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0b8e" id="0b8e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">agent = initialize_agent(<br />    tools, <br />    llm, <br />    agent=<span class="hljs-string">&quot;zero-shot-react-description&quot;</span>, <br />    verbose=<span class="hljs-literal">True</span><br />)</span></pre><p name="d316" id="d316" class="graf graf--p graf-after--pre">Now we can go ahead and test the agent on a couple of questions.</p><figure name="b3b8" id="b3b8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*rJQ7aFFpo0jWEehRsQYLFg.png" data-width="1209" data-height="837" src="https://cdn-images-1.medium.com/max/800/1*rJQ7aFFpo0jWEehRsQYLFg.png"><figcaption class="imageCaption">Sales agent example. Image by the author.</figcaption></figure><figure name="7a6a" id="7a6a" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*3IizH3SfB2pUZ84s2YTKaw.png" data-width="1209" data-height="825" src="https://cdn-images-1.medium.com/max/800/1*3IizH3SfB2pUZ84s2YTKaw.png"><figcaption class="imageCaption">Support agent example. Image by the author.</figcaption></figure><p name="5df5" id="5df5" class="graf graf--p graf-after--figure">Remember, the main difference between the two QAs beside the context sources is that we allow the support QA to form answers that can’t be found in the provided context. On the other hand, we prohibit the sales QA from doing that to avoid any overpromising statements.</p><h4 name="49ba" id="49ba" class="graf graf--h4 graf-after--p">Summary</h4><p name="d9d0" id="d9d0" class="graf graf--p graf-after--h4">In the era of LLMs, you can develop a chatbot that uses company’s resources to answer questions in a single day thanks to LangChain library as it offers various document loaders as well as integration with popular LLM models. Therefore, the only thing you need to do is to collect company’s resources, import them into a vector database, and you are good to go. Just note that the implementation is not deterministic, which means you can get slightly different results on identical prompts. GPT-4 model is much better for more accurate and consistent responses.</p><p name="14a2" id="14a2" class="graf graf--p graf-after--p graf--trailing">Let me know if you have any ideas or feedback regarding the chatbot implementation. As always, the code is available on <a href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/neo4jdocs/neo4j_support_bot.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub</a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/63c4761193e7"><time class="dt-published" datetime="2023-04-19T15:27:25.992Z">April 19, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/implementing-a-sales-support-agent-with-langchain-63c4761193e7" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>