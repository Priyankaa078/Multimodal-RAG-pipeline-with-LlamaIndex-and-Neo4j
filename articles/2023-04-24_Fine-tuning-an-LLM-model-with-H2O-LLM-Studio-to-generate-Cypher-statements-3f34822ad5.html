<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Fine-tuning an LLM model with H2O LLM Studio to generate Cypher statements</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Fine-tuning an LLM model with H2O LLM Studio to generate Cypher statements</h1>
</header>
<section data-field="subtitle" class="p-summary">
Avoid depending on external and ever changing APIs for your knowledge graph based chatbot
</section>
<section data-field="body" class="e-content">
<section name="4191" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4a23" id="4a23" class="graf graf--h3 graf--leading graf--title">Fine-tuning an LLM model with H2O LLM Studio to generate Cypher statements</h3><h4 name="63d7" id="63d7" class="graf graf--h4 graf-after--h3 graf--subtitle">Avoid depending on external and ever changing APIs for your knowledge graph based chatbot</h4><figure name="7a81" id="7a81" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="0*dN1-bWs7kugu2mIa" data-width="6000" data-height="4000" data-unsplash-photo-id="urTBnMtWWYc" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*dN1-bWs7kugu2mIa"><figcaption class="imageCaption">Photo by <a href="https://unsplash.com/@mikehindle?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@mikehindle?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-creator noopener" target="_blank">Mike Hindle</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--figure-anchor" rel="photo-source noopener" target="_blank">Unsplash</a></figcaption></figure><p name="df76" id="df76" class="graf graf--p graf-after--figure">Large language models like ChatGPT have a knowledge cutoff date beyond which they are not aware of any events that happened later. Instead of fine-tuning models with later information, the trend is to provide additional external context to LLM at query time. I have written a couple of blog posts on implementing a <a href="https://medium.com/neo4j/context-aware-knowledge-graph-chatbot-with-gpt-4-and-neo4j-d3a99e8ae21e" data-href="https://medium.com/neo4j/context-aware-knowledge-graph-chatbot-with-gpt-4-and-neo4j-d3a99e8ae21e" class="markup--anchor markup--p-anchor" target="_blank">context-aware knowledge graph-based bot</a> to a <a href="https://towardsdatascience.com/implementing-a-sales-support-agent-with-langchain-63c4761193e7" data-href="https://towardsdatascience.com/implementing-a-sales-support-agent-with-langchain-63c4761193e7" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">bot that can read through the company’s resources to answer questions</a>. However, I have used OpenAI’s large language models in all of the examples so far</p><p name="1870" id="1870" class="graf graf--p graf-after--p">While OpenAI’s official position is that they don’t use users’ data to improve their models, there are stories like how <a href="https://mashable.com/article/samsung-chatgpt-leak-details" data-href="https://mashable.com/article/samsung-chatgpt-leak-details" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Samsung employees leaked top secret data by inputting it into ChatGPT</a>. If I were dealing with top-secret, proprietary information, I would stay on the safe side and not share that information with OpenAI. Luckily, new open-source LLM models are popping up every day.</p><p name="266c" id="266c" class="graf graf--p graf-after--p">I have tested many open-source LLM models on their ability to generate Cypher statements. Some of them have a basic understanding of Cypher syntax. However, I haven’t found any models reliably generating Cypher statements based on provided examples or graph schema. So, the only solution was to fine-tune an open-sourced LLM model to generate Cypher statements reliably.</p><p name="d9d1" id="d9d1" class="graf graf--p graf-after--p">I have never fine-tuned any NLP model, let alone an LLM. Therefore, I had to find a simple way to get started without first obtaining a Ph.D. in machine learning. Luckily, I stumbled upon H2O’s LLM Studio tool, released just a couple of days ago, which provides a graphical interface for fine-tuning LLM models. I was delighted to discover that fine-tuning an LLM no longer required me to write any code or long bash commands. With just a few mouse clicks, I would be able to complete the task.</p><div name="ccb7" id="ccb7" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/h2oai/h2o-llmstudio" data-href="https://github.com/h2oai/h2o-llmstudio" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/h2oai/h2o-llmstudio"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - h2oai/h2o-llmstudio: H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs</strong><br><em class="markup--em markup--mixtapeEmbed-em">Welcome to H2O LLM Studio, a framework and no-code GUI designed for fine-tuning state-of-the-art large language models…</em>github.com</a><a href="https://github.com/h2oai/h2o-llmstudio" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="412a46411991973b91714ab02d7e379c" data-thumbnail-img-id="0*OWTARa7ZEvT4h9sf" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*OWTARa7ZEvT4h9sf);"></a></div><p name="a9bd" id="a9bd" class="graf graf--p graf-after--mixtapeEmbed">All the code of this blog post is <a href="https://github.com/tomasonjo/blogs/tree/master/h20_llm" data-href="https://github.com/tomasonjo/blogs/tree/master/h20_llm" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">available on GitHub</a>.</p><h4 name="5395" id="5395" class="graf graf--h4 graf-after--p">Preparing a training dataset</h4><p name="709f" id="709f" class="graf graf--p graf-after--h4">First, I had to learn how the training dataset should be structured. I examined their <a href="https://colab.research.google.com/drive/1-OYccyTvmfa3r7cAquw8sioFFPJcn4R9?usp=sharing" data-href="https://colab.research.google.com/drive/1-OYccyTvmfa3r7cAquw8sioFFPJcn4R9?usp=sharing" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">tutorial notebook</a> and discovered that the tool could handle training data provided as a CSV file, where the first column includes user prompts, and the second column contains desired LLM responses.</p><figure name="7fe4" id="7fe4" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/tomasonjo/a635772102162237bbdbf4b91dcbc580.js"></script></figure><p name="9b9b" id="9b9b" class="graf graf--p graf-after--figure">Ok, that’s easy enough. Now I just had to produce the training examples. I decided that 200 is a good number of training examples. However, I am way too lazy to write 200 Cypher statements manually. Therefore, I employed GPT-4 to do the job for me. The code can be found here:</p><div name="23bc" id="23bc" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/tomasonjo/blogs/blob/master/h20_llm/LLM_train_dataset.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/h20_llm/LLM_train_dataset.ipynb" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/tomasonjo/blogs/blob/master/h20_llm/LLM_train_dataset.ipynb"><strong class="markup--strong markup--mixtapeEmbed-strong">blogs/LLM_train_dataset.ipynb at master · tomasonjo/blogs</strong><br><em class="markup--em markup--mixtapeEmbed-em">You can&#39;t perform that action at this time. You signed in with another tab or window. You signed out in another tab or…</em>github.com</a><a href="https://github.com/tomasonjo/blogs/blob/master/h20_llm/LLM_train_dataset.ipynb" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="b02d8456069bd38aa956aed5334763de" data-thumbnail-img-id="0*sH-OGXMfx8JSLnJB" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*sH-OGXMfx8JSLnJB);"></a></div><p name="c9d4" id="c9d4" class="graf graf--p graf-after--mixtapeEmbed">The movie recommendation dataset is baked into GPT-4, so it can generate good enough examples. However, some examples are slightly off and don’t fit the graph schema. So, if I were fine-tuning an LLM for commercial use, I would use GPT-4 to generate Cypher statements and then walk through manually to validate them. Additionally, I would want to ensure that the validation set contains no examples from the training set.</p><p name="b561" id="b561" class="graf graf--p graf-after--p">I have also tested if a prefix “Create a Cypher statement for the following question” is needed for instructions. It seems that some models like <strong class="markup--strong markup--p-strong">EleutherAI/pythia-12b-deduped </strong>need the prefix, otherwise they fail miserably. On the other hand, <strong class="markup--strong markup--p-strong">facebook/opt-13b</strong> did a solid job even without the prefix.</p><figure name="05c0" id="05c0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MkynxzFo-aQbKpDGuFXJBQ.png" data-width="934" data-height="483" src="https://cdn-images-1.medium.com/max/800/1*MkynxzFo-aQbKpDGuFXJBQ.png"><figcaption class="imageCaption">Models trained without or with a prefix in instructions. Image by the author.</figcaption></figure><p name="308f" id="308f" class="graf graf--p graf-after--figure">To be able to compare all models with the same dataset, I used a dataset that adds a prefix “Create a Cypher statement for the following question:” to the instructions section of the dataset.</p><h4 name="05e2" id="05e2" class="graf graf--h4 graf-after--p">H2O LLM Studio installation</h4><p name="e52e" id="e52e" class="graf graf--p graf-after--h4">H2O LLM Studio can be installed in two simple steps. In the first step, we have to install Python 3.10 environment if it is missing. The steps to install Python 3.10 are described in their GitHub repository.</p><div name="d2f5" id="d2f5" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/h2oai/h2o-llmstudio" data-href="https://github.com/h2oai/h2o-llmstudio" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/h2oai/h2o-llmstudio"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - h2oai/h2o-llmstudio: H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs</strong><br><em class="markup--em markup--mixtapeEmbed-em">Welcome to H2O LLM Studio, a framework and no-code GUI designed for fine-tuning state-of-the-art large language models…</em>github.com</a><a href="https://github.com/h2oai/h2o-llmstudio" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="412a46411991973b91714ab02d7e379c" data-thumbnail-img-id="0*OWTARa7ZEvT4h9sf" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*OWTARa7ZEvT4h9sf);"></a></div><p name="f6c5" id="f6c5" class="graf graf--p graf-after--mixtapeEmbed">After we ensure a Python 3.10 environment, we simply clone the repository and install dependencies with the <code class="markup--code markup--p-code">make install</code> command. After the installation, we can run the LLM studio with the <code class="markup--code markup--p-code">make wave</code> command. Now we can open the graphical interface in your favourite browser by opening the <code class="markup--code markup--p-code">localhost:10101</code> website.</p><figure name="996e" id="996e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*FuV4fE2myj_J5wskzrLDTg.png" data-width="1424" data-height="479" src="https://cdn-images-1.medium.com/max/800/1*FuV4fE2myj_J5wskzrLDTg.png"><figcaption class="imageCaption">H2O LLM Studio home page. Image by the author.</figcaption></figure><h4 name="33e8" id="33e8" class="graf graf--h4 graf-after--figure">Import dataset</h4><p name="9373" id="9373" class="graf graf--p graf-after--h4">First, we have to import the dataset to be used to fine-tune an LLM. You can <a href="https://github.com/tomasonjo/blog-datasets/tree/main/llm" data-href="https://github.com/tomasonjo/blog-datasets/tree/main/llm" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">download the one I used</a> if you don’t want to create your dataset. Note that it is not curated, and some examples do not fit the movie recommendation graph schema. However, it is a great start to getting to know the tool. We can import CSV files using the drag&amp;drop interface.</p><figure name="f63b" id="f63b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*dFfw79bABrKLHtrvDQ8KXA.png" data-width="1051" data-height="533" src="https://cdn-images-1.medium.com/max/800/1*dFfw79bABrKLHtrvDQ8KXA.png"><figcaption class="imageCaption">Upload CSV interface. Image by the author.</figcaption></figure><p name="3105" id="3105" class="graf graf--p graf-after--figure">It is a bit counter-intuitive, but we have to upload the training and validation sets separately. Let’s say we first upload the training set. Then, when we upload the validation set, we have to use the merge datasets option so that we have both the training and validation sets in the same dataset.</p><figure name="ecb7" id="ecb7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ES3t4ezTlzEAbLyBtHZ0FQ.png" data-width="237" data-height="532" src="https://cdn-images-1.medium.com/max/800/1*ES3t4ezTlzEAbLyBtHZ0FQ.png"><figcaption class="imageCaption">Imported dataset with both train and validation dataframes. Image by the author.</figcaption></figure><p name="ec14" id="ec14" class="graf graf--p graf-after--figure">The final dataset should have both training and validation dataframes present.</p><p name="edcd" id="edcd" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">I’ve learned you can also upload a ZIP file with both training and validation sets to avoid having to separately upload files.</em></p><h4 name="013a" id="013a" class="graf graf--h4 graf-after--p">Create experiment</h4><p name="71a0" id="71a0" class="graf graf--p graf-after--h4">Now that everything is ready, we can go ahead and fine-tune an LLM model. If we click on the <strong class="markup--strong markup--p-strong">Create Experiment</strong> tab, we will be presented with fine-tuning options. The most important setting to choose are the <strong class="markup--strong markup--p-strong">dataset</strong> used for training, the <strong class="markup--strong markup--p-strong">LLM backbone</strong>, and I have also increased the <strong class="markup--strong markup--p-strong">epochs</strong> count in my experiments. I have left the other parameters default as I have no idea what they do. We can choose from 13 LLM models:</p><figure name="f623" id="f623" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*inVs_pLIGUFeK3bqb3BivA.png" data-width="329" data-height="463" src="https://cdn-images-1.medium.com/max/800/1*inVs_pLIGUFeK3bqb3BivA.png"><figcaption class="imageCaption">Available LLM models. Image by the author.</figcaption></figure><p name="7558" id="7558" class="graf graf--p graf-after--figure">Note that the higher the parameter count, the more GPU RAM we require for finetuning and inference. For example, I ran out of memory using a 40GB GPU when trying to finetune an LLM model with 20B parameters. On the other hand, we expect that the higher the parameter count of an LLM, the better the results. I would say that we require about 5GB of GPU RAM for smaller LLMs like pythia-1b and up to 40GB GPU for opt-13b models. Once we set the desired parameters, we can run the experiment with a single click. For the most part, the finetuning process was relatively fast using an Nvidia A100 40GB.</p><figure name="e39d" id="e39d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nnoZ2b4vH9aBU4tX6aiu9Q.png" data-width="1210" data-height="507" src="https://cdn-images-1.medium.com/max/800/1*nnoZ2b4vH9aBU4tX6aiu9Q.png"><figcaption class="imageCaption">Experiments page. Image by the author.</figcaption></figure><p name="741e" id="741e" class="graf graf--p graf-after--figure">Most models were trained in less than 30 minutes using 15 epochs. The nice thing about the LLM Studio is that it produces a dashboard to inspect the training results.</p><figure name="5c0d" id="5c0d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*8FQXzd5KFiyCLrySCbTQuA.png" data-width="1243" data-height="708" src="https://cdn-images-1.medium.com/max/800/1*8FQXzd5KFiyCLrySCbTQuA.png"><figcaption class="imageCaption">LLM finetuning metrics. Image by the author.</figcaption></figure><p name="d6f4" id="d6f4" class="graf graf--p graf-after--figure">Not only that, but we can also chat with the model in the graphical interface.</p><figure name="ffcb" id="ffcb" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BaZaTvdOBumllWuSCJjkvQ.png" data-width="1281" data-height="464" src="https://cdn-images-1.medium.com/max/800/1*BaZaTvdOBumllWuSCJjkvQ.png"><figcaption class="imageCaption">Chat interface in the LLM Studio. Image by the author.</figcaption></figure><h4 name="be6a" id="be6a" class="graf graf--h4 graf-after--figure">Export models to HuggingFace repository</h4><p name="7e7e" id="7e7e" class="graf graf--p graf-after--h4">It’s as if the H2O LLM Studio wasn’t cool enough, it also allows to export finetuned models to HuggingFace with a single click.</p><figure name="5f69" id="5f69" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZNRDxLgIu2P6Q806XHZMiA.png" data-width="909" data-height="639" src="https://cdn-images-1.medium.com/max/800/1*ZNRDxLgIu2P6Q806XHZMiA.png"><figcaption class="imageCaption">Export models to HuggingFace. Image by the author.</figcaption></figure><p name="d254" id="d254" class="graf graf--p graf-after--figure">The ability to export a model to the HuggingFace repository with a single click allows us to use the model anywhere in our workflows as easily as possible. I have exported a small finetuned pythia-1b model that can run in Google Colab to demonstrate how to use it with the transformers library.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="96ce" id="96ce" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">import</span> torch<br /><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer<br /><br />device = <span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br /><br />tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;tomasonjo/movie-generator-small&quot;</span>)<br />model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;tomasonjo/movie-generator-small&quot;</span>).to(<br />    device<br />)<br /><br />prefix = <span class="hljs-string">&quot;\nCreate a Cypher statement to answer the following question:&quot;</span><br /><br /><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_cypher</span>(<span class="hljs-params">prompt</span>):<br />    inputs = tokenizer(<br />        <span class="hljs-string">f&quot;<span class="hljs-subst">{prefix}</span><span class="hljs-subst">{prompt}</span>&lt;|endoftext|&gt;&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span><br />    ).to(device)<br />    tokens = model.generate(<br />        **inputs,<br />        max_new_tokens=<span class="hljs-number">256</span>,<br />        temperature=<span class="hljs-number">0.3</span>,<br />        repetition_penalty=<span class="hljs-number">1.2</span>,<br />        num_beams=<span class="hljs-number">4</span>,<br />    )[<span class="hljs-number">0</span>]<br />    tokens = tokens[inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape[<span class="hljs-number">1</span>] :]<br />    <span class="hljs-keyword">return</span> tokenizer.decode(tokens, skip_special_tokens=<span class="hljs-literal">True</span>)</span></pre><p name="905b" id="905b" class="graf graf--p graf-after--pre">The LLM Studio uses a special <code class="markup--code markup--p-code">&lt;|endoftext|&gt;</code>character that must be added to the end of the user prompt in order for the model to work correctly. Therefore, we must do the same when using the finetuned model with the transformers library. Other than that, there is nothing really that needs to be done. We can now use the model to generate Cypher statements.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="a093" id="a093" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">generate_cypher(<span class="hljs-string">&quot;How many movies did Tom Hanks appear in?&quot;</span>)<br /><span class="hljs-comment">#MATCH (d:Person {name: &#x27;Tom Hanks&#x27;})-[:ACTED_IN]-&gt;(m:Movie) </span><br /><span class="hljs-comment">#RETURN {movie: m.title} AS result</span><br /><br />generate_cypher(<span class="hljs-string">&quot;When was Toy Story released?&quot;</span>)<br /><span class="hljs-comment">#MATCH (m:Movie {title: &#x27;When&#x27;})-[:IN_GENRE]-&gt;(g:Genre)</span><br /><span class="hljs-comment">#RETURN {genre: g.name} AS result</span></span></pre><p name="30cf" id="30cf" class="graf graf--p graf-after--pre">I deliberately showed one valid and one invalid Cypher statement generated to show that the smaller models might be good enough for demos, where the prompts can be predefined. On the other hand, you probably wouldn’t want to use them in production. However, using bigger models comes with a price. For example, to run models with 12B parameters, we need at least 24 GB GPU, while the 20B parameter models require GPUs with 48 GB.</p><h4 name="40fd" id="40fd" class="graf graf--h4 graf-after--p">Summary</h4><p name="fee6" id="fee6" class="graf graf--p graf-after--h4">Finetuning open-source LLMs allows us to break free of the OpenAI dependency. Although GPT-4 works better, especially in a conversational setting where follow-up questions could be asked, we can still keep our top-secret data to ourselves. I tested multiple models while writing this blog post, except for 20B models, due to GPU memory issues. I can confidently say that you could finetune a model to generate Cypher statements good enough for a production setting. One thing to note is that follow-up questions, where the model has to rely on previous dialogue to understand the context of the question, don’t seem to be functioning at the moment. Therefore, we are limited to single-step queries, where we need to provide the whole context in a single prompt. However, since the development of open-source LLMs is exploding, I am excited about what’s to come next.</p><p name="39f2" id="39f2" class="graf graf--p graf-after--p graf--trailing">Till then, try out the <a href="https://github.com/h2oai/h2o-llmstudio" data-href="https://github.com/h2oai/h2o-llmstudio" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">H2O LLM Studio</a> if you want to finetune an LLM to fit your personal or company’s needs with only a few mouse clicks.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/3f34822ad5"><time class="dt-published" datetime="2023-04-24T15:10:59.259Z">April 24, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/fine-tuning-an-llm-model-with-h2o-llm-studio-to-generate-cypher-statements-3f34822ad5" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>