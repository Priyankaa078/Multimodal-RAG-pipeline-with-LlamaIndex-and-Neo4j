<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Enhancing Word Embedding with Graph Neural Networks</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Enhancing Word Embedding with Graph Neural Networks</h1>
</header>
<section data-field="subtitle" class="p-summary">
Leverage GraphSAGE algorithm in combination with OpenAI word embeddings to  increase downstream document classification accuracy
</section>
<section data-field="body" class="e-content">
<section name="8c1b" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="1029" id="1029" class="graf graf--h3 graf--leading graf--title">Enhancing Word Embedding With Graph Neural Networks</h3><h4 name="b899" id="b899" class="graf graf--h4 graf-after--h3 graf--subtitle">Use GraphSAGE algorithm in combination with OpenAI word embeddings to increase downstream document classification accuracy.</h4><p name="aab0" id="aab0" class="graf graf--p graf-after--h4">Natural Language Processing (NLP) has seen rapid advancements in recent years. One important aspect of this progress has been the use of embeddings, which are numerical representations of words or phrases that capture their meaning and relationships to other words in a language.</p><p name="2969" id="2969" class="graf graf--p graf-after--p">Embeddings can be used in a wide range of NLP tasks, such as document classification, machine translation, sentiment analysis, and named entity recognition. Furthermore, with the availability of large pre-trained language models like GPT-3, embeddings have become even more critical for enabling transfer learning across a range of language tasks and domains. As such, embeddings are closely tied to the rapid advancements in NLP.</p><p name="e702" id="e702" class="graf graf--p graf-after--p">On the other hand, recent advancements in graphs and graph neural networks have led to improved performance on a wide range of tasks, including image recognition, drug discovery, and recommender systems. In particular, graph neural networks have shown great promise in learning representations of graph-structured data, where the relationships between data points provide a signal that improves the accuracy of downstream machine-learning tasks.</p><p name="9652" id="9652" class="graf graf--p graf-after--p">In this blog post, you’ll discover how to harness the power of graph neural networks to capture and encode the relationships between data points and enhance document classification accuracy. Specifically, you will train two models to predict a Medium article’s tags.</p><figure name="04d5" id="04d5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yALRPZD6-IG9qPjheKtvXw.png" data-width="533" data-height="83" src="https://cdn-images-1.medium.com/max/800/1*yALRPZD6-IG9qPjheKtvXw.png"><figcaption class="imageCaption">Example tags from my previous blog post. Image by the author.</figcaption></figure><p name="43ec" id="43ec" class="graf graf--p graf-after--figure">Most edium articles have relevant tags assigned to them by the author for easier discoverability and search performance. Additionally, you can think of these tags as a categorization of articles. Each article can have up to 5 tags or categories it belongs to, as shown in the above image. Therefore you will train two classification models to perform a <strong class="markup--strong markup--p-strong">multi-label classification</strong>, where each article can have one or more tags assigned to them.</p><figure name="06b3" id="06b3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nYljvhaiOPtUUvWTR0JYIA.png" data-width="1186" data-height="583" src="https://cdn-images-1.medium.com/max/800/1*nYljvhaiOPtUUvWTR0JYIA.png"><figcaption class="imageCaption">Multi-label classification of medium article tags. Image by the author.</figcaption></figure><p name="b38e" id="b38e" class="graf graf--p graf-after--figure">The first classification model will use OpenAI’s latest embeddings (text-embedding-ada-002) of the article’s title and subtitle as the input features. This model will provide a baseline accuracy you will try to improve using a graph neural network algorithm called GraphSAGE. Interestingly, word embeddings can be and will be used in this example as input to GraphSAGE. During training, the GraphSAGE algorithm then leverages these word embeddings to iteratively aggregate information from neighboring nodes, resulting in powerful node-level representations that can improve the accuracy of downstream machine-learning tasks like document classification.</p><p name="31be" id="31be" class="graf graf--p graf-after--p">In short, this blog post explores the use of graph neural networks to improve word embeddings by taking into account the relationships between data points. When the relationships between data points are relevant and predictive, graph neural networks can learn more meaningful and accurate representations of text data and, consequently, increase the accuracy of downstream machine learning models.</p><h3 name="868f" id="868f" class="graf graf--h3 graf-after--p">Medium Dataset</h3><p name="a0e2" id="a0e2" class="graf graf--p graf-after--h3">There are a couple of medium article datasets available on Kaggle. However, none of them contain any relationships between articles. What type of relationships between articles would even be predictive for predicting their tags? Medium has added the ability for users to create lists that can help them bookmark and curate the content they have or intend to read.</p><figure name="66fc" id="66fc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*oACu7taRp6hErAh2Y-HghQ.png" data-width="786" data-height="755" src="https://cdn-images-1.medium.com/max/800/1*oACu7taRp6hErAh2Y-HghQ.png"><figcaption class="imageCaption">Curated lists of articles by users on medium. Image by the author.</figcaption></figure><p name="c782" id="c782" class="graf graf--p graf-after--figure">This image presents an example where the user created four lists of articles based on their topics. For example, most articles were grouped under the <strong class="markup--strong markup--p-strong">Data Science</strong> list, while other articles were added to the<strong class="markup--strong markup--p-strong"> Communication</strong>, <strong class="markup--strong markup--p-strong">Maths</strong>, and <strong class="markup--strong markup--p-strong">Design</strong> lists. The idea is that if two articles are in the same list, they are somewhat more similar than if they don’t have any common lists. You can think of medium lists as human-annotated relationships between articles that can help you find and potentially recommend similar articles.</p><p name="d49d" id="d49d" class="graf graf--p graf-after--p">There is one exception to this assumption. Some users create vast reading lists that contain all sorts of articles.</p><figure name="d91b" id="d91b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BlXdb80suTU6sg517p1iOw.png" data-width="737" data-height="197" src="https://cdn-images-1.medium.com/max/800/1*BlXdb80suTU6sg517p1iOw.png"><figcaption class="imageCaption">An example of a reading list that contains 2551 articles. Image by the author.</figcaption></figure><p name="0a82" id="0a82" class="graf graf--p graf-after--figure">Interestingly, most of these lists with a vast amount of articles have an identical <strong class="markup--strong markup--p-strong">Reading list</strong> title. So it has to be some sort of default value by Medium or something, as I have noticed the reading list title with a couple of users.</p><p name="2b0c" id="2b0c" class="graf graf--p graf-after--p">Unfortunately, there are no publicly available datasets with information about the Medium articles as well as the user lists they belong to. Therefore, I had to spend an afternoon parsing the data. I retrieved information about 55 thousand medium articles from 4000 user lists.</p><h4 name="809d" id="809d" class="graf graf--h4 graf-after--p">Preparing Neo4j Environment</h4><p name="2a99" id="2a99" class="graf graf--p graf-after--h4">The graph construction and GraphSAGE training will be executed in Neo4j. I like Neo4j as it offers a nicely designed graph query language called Cypher as well as a Graph Data Science plugin that contains more than 50 graph algorithms that cover most of graph analytics workflow. Therefore, there is no need to use multiple tools to create and analyze the graph.</p><p name="a5a9" id="a5a9" class="graf graf--p graf-after--p">The graph schema of the Medium dataset if the following:</p><figure name="b903" id="b903" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*pE_yquIPRCyRjdDwO0Em3w.png" data-width="964" data-height="609" src="https://cdn-images-1.medium.com/max/800/1*pE_yquIPRCyRjdDwO0Em3w.png"><figcaption class="imageCaption">Graph schema. Image by the author.</figcaption></figure><p name="fc6d" id="fc6d" class="graf graf--p graf-after--figure">The schema revolves around Medium articles. We know the <strong class="markup--strong markup--p-strong">url</strong>, <strong class="markup--strong markup--p-strong">title</strong>, and <strong class="markup--strong markup--p-strong">date</strong> of the article. Additionally, I have calculated the OpenAI’s embeddings using the text-embedding-ada-002 model based on the article title and subtitle and stored them as <strong class="markup--strong markup--p-strong">openaiEmbedding</strong> property. Additionally, we know who wrote the article, which user’s lists it belongs to, and its tags.</p><p name="0287" id="0287" class="graf graf--p graf-after--p">I have prepared two options for you to import the medium dataset into Neo4j database. You can execute the following Jupyter notebook and import the dataset from Python. This option also works with <a href="https://neo4j.com/sandbox/" data-href="https://neo4j.com/sandbox/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Neo4j Sandbox environment</a> <em class="markup--em markup--p-em">(use blank graph data science project).</em></p><div name="e732" id="e732" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/tomasonjo/blogs/blob/master/medium/Import.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/medium/Import.ipynb" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/tomasonjo/blogs/blob/master/medium/Import.ipynb"><strong class="markup--strong markup--mixtapeEmbed-strong">blogs/Import.ipynb at master · tomasonjo/blogs</strong><br><em class="markup--em markup--mixtapeEmbed-em">You can&#39;t perform that action at this time. You signed in with another tab or window. You signed out in another tab or…</em>github.com</a><a href="https://github.com/tomasonjo/blogs/blob/master/medium/Import.ipynb" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="aac5a098da178f96e803ae57f8362329" data-thumbnail-img-id="0*UOW07FLB3sh208-f" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*UOW07FLB3sh208-f);"></a></div><p name="0d50" id="0d50" class="graf graf--p graf-after--mixtapeEmbed">The other option is to restore the Neo4j database dump I have prepared.</p><div name="71a1" id="71a1" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://drive.google.com/file/d/1HLywpCBwEMYs6wZee0WHpZJxw3HtoqNn/view?usp=share_link" data-href="https://drive.google.com/file/d/1HLywpCBwEMYs6wZee0WHpZJxw3HtoqNn/view?usp=share_link" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://drive.google.com/file/d/1HLywpCBwEMYs6wZee0WHpZJxw3HtoqNn/view?usp=share_link"><strong class="markup--strong markup--mixtapeEmbed-strong">medium-dump-v55.dump</strong><br><em class="markup--em markup--mixtapeEmbed-em">Edit description</em>drive.google.com</a><a href="https://drive.google.com/file/d/1HLywpCBwEMYs6wZee0WHpZJxw3HtoqNn/view?usp=share_link" class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="ba365d5ad5a47223211fc709e68053cd"></a></div><p name="d17d" id="d17d" class="graf graf--p graf-after--mixtapeEmbed">The dump has been created with Neo4j version 5.5.0, so make sure to use that version or later. The easiest way to restore the database dump is to use the <a href="https://neo4j.com/download/" data-href="https://neo4j.com/download/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Neo4j Desktop environment</a>. Additionally, you will need to install the <strong class="markup--strong markup--p-strong">APOC</strong> and <strong class="markup--strong markup--p-strong">GDS</strong> libraries if you are using the Neo4j Desktop environment.</p><p name="4492" id="4492" class="graf graf--p graf-after--p">After the database import is finished, you can run the following Cypher statement in Neo4j Browser to verify that the import was successful.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="sql" name="fd4b" id="fd4b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">MATCH</span> p<span class="hljs-operator">=</span>(n:Author)<span class="hljs-operator">-</span>[:WROTE]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>(d)<span class="hljs-operator">-</span>[:IN_LIST]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>(), p1<span class="hljs-operator">=</span>(d)<span class="hljs-operator">-</span>[:HAS_TAG]<span class="hljs-operator">-</span><span class="hljs-operator">&gt;</span>()<br /><span class="hljs-keyword">WHERE</span> n.name <span class="hljs-operator">=</span> &quot;Tomaz Bratanic&quot;<br /><span class="hljs-keyword">RETURN</span> p,p1 LIMIT <span class="hljs-number">25</span></span></pre><p name="6914" id="6914" class="graf graf--p graf-after--pre">The result will contain a couple of articles I have written along with their lists and tags.</p><figure name="4fcd" id="4fcd" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2NSeGr8-0uUkXg7v9ZGZgQ.png" data-width="938" data-height="826" src="https://cdn-images-1.medium.com/max/800/1*2NSeGr8-0uUkXg7v9ZGZgQ.png"><figcaption class="imageCaption">A small subset of the Medium graph. Image by the author.</figcaption></figure><p name="3a2d" id="3a2d" class="graf graf--p graf-after--figure">Now it is time for the practical part of this blog post. All the analysis code is available as a Jupyter Notebook.</p><div name="fa39" id="fa39" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/tomasonjo/blogs/blob/master/medium/Classification%20with%20GraphSAGE.ipynb" data-href="https://github.com/tomasonjo/blogs/blob/master/medium/Classification%20with%20GraphSAGE.ipynb" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/tomasonjo/blogs/blob/master/medium/Classification%20with%20GraphSAGE.ipynb"><strong class="markup--strong markup--mixtapeEmbed-strong">blogs/Classification with GraphSAGE.ipynb at master · tomasonjo/blogs</strong><br><em class="markup--em markup--mixtapeEmbed-em">You can&#39;t perform that action at this time. You signed in with another tab or window. You signed out in another tab or…</em>github.com</a><a href="https://github.com/tomasonjo/blogs/blob/master/medium/Classification%20with%20GraphSAGE.ipynb" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="189edcf5faa7479af6246a4307b44e66" data-thumbnail-img-id="0*LCmcTUDGRt2mjStt" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*LCmcTUDGRt2mjStt);"></a></div><h3 name="ab6e" id="ab6e" class="graf graf--h3 graf-after--mixtapeEmbed">Exploratory Analysis</h3><p name="845a" id="845a" class="graf graf--p graf-after--h3">We will be using the <a href="https://neo4j.com/docs/graph-data-science-client/current/" data-href="https://neo4j.com/docs/graph-data-science-client/current/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Graph Data Science Python Client</a> to interface with Neo4j and its Graph Data Science plugin. It is an excellent addition to the Neo4j ecosystem, allowing us to execute graph algorithms using pure Python code. Check out my<a href="https://medium.com/neo4j/how-to-get-started-with-the-neo4j-graph-data-science-python-client-56209d9b0d0d" data-href="https://medium.com/neo4j/how-to-get-started-with-the-neo4j-graph-data-science-python-client-56209d9b0d0d" class="markup--anchor markup--p-anchor" target="_blank"> introductory blog post</a> for more information.</p><p name="9e33" id="9e33" class="graf graf--p graf-after--p">First, we will evaluate the distribution of tags per medium article.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="6456" id="6456" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">dist_df = gds.run_cypher(<span class="hljs-string">&quot;&quot;&quot;<br />MATCH (a:Article)<br />RETURN count{(a)-[:HAS_TAG]-&gt;()} AS count<br />&quot;&quot;&quot;</span>)<br /><br />sns.displot(dist_df[<span class="hljs-string">&#x27;count&#x27;</span>], height=<span class="hljs-number">6</span>, aspect=<span class="hljs-number">1.5</span>)</span></pre><figure name="c1c9" id="c1c9" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*f1cN54f-Q3Ysco5Ga-Op_w.png" data-width="663" data-height="427" src="https://cdn-images-1.medium.com/max/800/1*f1cN54f-Q3Ysco5Ga-Op_w.png"><figcaption class="imageCaption">Distribution of tags per article. Image by the author.</figcaption></figure><p name="3f12" id="3f12" class="graf graf--p graf-after--figure">Around 50% of articles have no tags present. There are two reasons for that. Either the author did not use any, or the scrapping process failed to retrieve them for various reasons, like medium publications having custom HTML structures. However, it is not a big deal as we still have more than 25 thousand articles with their tags present, allowing us to train and evaluate the multi-label classification model of article tags. Most authors choose to use five tags per article, which is also the upper limit that the Medium platform allows.</p><p name="eec1" id="eec1" class="graf graf--p graf-after--p">Next, we will evaluate if any articles are not part of any user lists.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="0aba" id="0aba" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.run_cypher(<br />    <span class="hljs-string">&quot;&quot;&quot;<br />MATCH (a:Article)<br />RETURN exists {(a)-[:IN_LIST]-()} AS in_list,<br />       count(*) AS count<br />ORDER BY count DESC<br />&quot;&quot;&quot;</span><br />)</span></pre><p name="901a" id="901a" class="graf graf--p graf-after--pre">The results show that all articles belong to at least one list. Identifying isolated nodes (nodes with no connection) is a critical part of any graph analytics workflow, as we have to pay special attention to them while calculating node embeddings. Luckily, this dataset contains no isolated nodes, so we don’t have to worry about that.</p><p name="92a1" id="92a1" class="graf graf--p graf-after--p">In the last part of the exploratory analysis, we will examine the most frequent tags. Here, we will construct a word cloud of tags present in at least 100 articles.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="dd6d" id="dd6d" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">tags = gds.run_cypher(<br />    <span class="hljs-string">&quot;&quot;&quot;<br />MATCH (t:Tag)<br />WITH t, count {(t)&lt;--()} AS size<br />WHERE size &gt; 100<br />RETURN t.name AS tag, size<br />ORDER BY size DESC<br />&quot;&quot;&quot;</span><br />)<br /><br />d = {}<br /><span class="hljs-keyword">for</span> i, row <span class="hljs-keyword">in</span> tags.iterrows():<br />    d[row[<span class="hljs-string">&quot;tag&quot;</span>]] = row[<span class="hljs-string">&quot;size&quot;</span>]<br /><br />wordcloud = WordCloud(<br />    background_color=<span class="hljs-string">&quot;white&quot;</span>, colormap=<span class="hljs-string">&quot;tab20c&quot;</span>, min_font_size=<span class="hljs-number">1</span><br />).generate_from_frequencies(d)<br />plt.figure()<br />plt.imshow(wordcloud)<br />plt.axis(<span class="hljs-string">&quot;off&quot;</span>)<br />plt.show()</span></pre><figure name="6077" id="6077" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*DXS1elnRVhR_9S3JaDqalQ.png" data-width="914" data-height="453" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*DXS1elnRVhR_9S3JaDqalQ.png"><figcaption class="imageCaption">Word cloud of the most frequent tags. Image by the author.</figcaption></figure><p name="e98a" id="e98a" class="graf graf--p graf-after--figure">The most frequent tags are data science, artificial intelligence, programming, and machine learning.</p><h3 name="0237" id="0237" class="graf graf--h3 graf-after--p">Multi-Label Classification</h3><p name="7a63" id="7a63" class="graf graf--p graf-after--h3">As mentioned, we will train a multi-label classification model to predict tags of a Medium article. Therefore, we will use the <a href="http://scikit.ml/" data-href="http://scikit.ml/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">scikit-multilearn</a> library to help with data splitting and model training.</p><p name="786c" id="786c" class="graf graf--p graf-after--p">I noticed that the dataset split with scikit-multilearn library does not provide a random seed parameter, and therefore, the dataset split is not deterministic. For a proper comparison of the baseline model trained on OpenAI’s word embedding and a model based on GraphSAGE embeddings, we will perform a single dataset split so that both model versions use the same training and test examples. Otherwise, there could be some differences between the models’ accuracy based solely on the dataset split.</p><p name="bc5b" id="bc5b" class="graf graf--p graf-after--p">The word embeddings are already stored in the graph, so we only need to calculate the node embeddings using the GraphSAGE algorithm before we can train the classification models.</p><h3 name="06ff" id="06ff" class="graf graf--h3 graf-after--p">GraphSAGE</h3><p name="cc7f" id="cc7f" class="graf graf--p graf-after--h3">GraphSAGE is a convolutional graph neural network algorithm. The key idea behind the algorithm is that we learn a function that generates node embeddings by sampling and aggregating feature information from a node’s local neighborhood. As the GraphSAGE algorithm learns a function that can induce the embedding of a node, it can also be used to induce embeddings of a new node that wasn’t observed during the training phase. This is called inductive learning.</p><figure name="065b" id="065b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*tob6Hr52ClkmpjMr.png" data-width="700" data-height="246" src="https://cdn-images-1.medium.com/max/800/0*tob6Hr52ClkmpjMr.png"></figure><p name="09d7" id="09d7" class="graf graf--p graf-after--figure">Neighborhood exploration and information sharing in GraphSAGE. [1]</p><p name="7af4" id="7af4" class="graf graf--p graf-after--p">If you want to learn more about the training process and the math behind the GraphSAGE algorithm, I suggest you take a look at the <a href="https://towardsdatascience.com/an-intuitive-explanation-of-graphsage-6df9437ee64f" data-href="https://towardsdatascience.com/an-intuitive-explanation-of-graphsage-6df9437ee64f" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">An Intuitive Explanation of GraphSAGE</a> blog post by Rıza Özçelik or the <a href="http://snap.stanford.edu/graphsage/" data-href="http://snap.stanford.edu/graphsage/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">official GraphSAGE site</a>.</p><h3 name="9bfa" id="9bfa" class="graf graf--h3 graf-after--p">Monopartite Projection With Node Similarity Algorithm</h3><p name="580d" id="580d" class="graf graf--p graf-after--h3">GraphSAGE supports graphs with multiple types of nodes, where each type of node has different features representing it. In our example, we have <strong class="markup--strong markup--p-strong">Article</strong> and <strong class="markup--strong markup--p-strong">List</strong> nodes. However, I have decided to simplify the workflow by performing a monopartite projection.</p><figure name="ef1c" id="ef1c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*IcqyaPI7WizUbRBQdxUgRQ.png" data-width="521" data-height="239" src="https://cdn-images-1.medium.com/max/800/1*IcqyaPI7WizUbRBQdxUgRQ.png"><figcaption class="imageCaption">Monopartite projection of articles. There is a relationship between articles if they share a list. Image by the author.</figcaption></figure><p name="112c" id="112c" class="graf graf--p graf-after--figure">Monopartite projection is a frequent step in graph analysis. The idea is to take a bipartite graph (graph with two node types) and output a monopartite graph (graph with only one node type). In this specific example, we can create a relationship between two articles if they are part of the same list. Additionally, the number of shared lists or a normalized value like the Jaccard coefficient can be stored as a relationship property.</p><p name="6da7" id="6da7" class="graf graf--p graf-after--p">Since the monopartite projection is a common step in graph analysis, the Neo4j Graph Data Science library offers a <a href="https://neo4j.com/docs/graph-data-science/current/algorithms/node-similarity/" data-href="https://neo4j.com/docs/graph-data-science/current/algorithms/node-similarity/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Node Similarity algorithm</a> to help us with it.</p><p name="b2d5" id="b2d5" class="graf graf--p graf-after--p">First, we need to project an in-memory graph. We will include the <strong class="markup--strong markup--p-strong">Article</strong> and <strong class="markup--strong markup--p-strong">List</strong> nodes along with the <strong class="markup--strong markup--p-strong">IN_LIST</strong> relationships. Additionally, we will include the <strong class="markup--strong markup--p-strong">openaiEmbedding</strong> node properties.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="f0fd" id="f0fd" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><br />G, metadata = gds.graph.project(<br />    <span class="hljs-string">&quot;articles&quot;</span>, <br />    [<span class="hljs-string">&quot;Article&quot;</span>, <span class="hljs-string">&quot;List&quot;</span>],<br />    <span class="hljs-string">&quot;IN_LIST&quot;</span>, <br />    nodeProperties=[<span class="hljs-string">&quot;openaiEmbedding&quot;</span>]<br />)</span></pre><p name="bcc0" id="bcc0" class="graf graf--p graf-after--pre">Now we can perform the monopartite projection using the Node Similarity algorithm. One thing to note is that the default value of the <strong class="markup--strong markup--p-strong">topK</strong> parameter is 10, meaning that each node will be connected to only its ten most similar nodes. However, in this example, we want to create a relationship between all articles in the user list. Therefore, we will use a relatively high value of the <strong class="markup--strong markup--p-strong">topK</strong> parameter.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4ea0" id="4ea0" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.nodeSimilarity.mutate(<br />    G, topK=<span class="hljs-number">2000</span>, mutateProperty=<span class="hljs-string">&quot;score&quot;</span>, mutateRelationshipType=<span class="hljs-string">&quot;SIMILAR&quot;</span><br />)</span></pre><p name="6f4c" id="6f4c" class="graf graf--p graf-after--pre">We have used the <strong class="markup--strong markup--p-strong">mutate</strong> mode of the algorithm which stores the results back to the in-memory projected graph. The <strong class="markup--strong markup--p-strong">SIMILAR</strong> relationship has been created between all pairs of articles that share at least a single user list.</p><h3 name="2898" id="2898" class="graf graf--h3 graf-after--p">Training the GraphSAGE Model</h3><p name="6297" id="6297" class="graf graf--p graf-after--h3">The GraphSAGE algorithm is inductive, meaning that it can be used to generate embeddings for nodes that were previously unseen during training. The inductive nature allows us to train the GraphSAGE model only on a subset of the graph and then generate the embeddings for all the nodes. Training the GraphSAGE model only on a subset of the graph saves us time and compute power, which is useful when dealing with large graphs. While our graph is not that large, we can use this example to demonstrate how to sample the training subset of the graph efficiently.</p><h4 name="b2ab" id="b2ab" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Random walk with restarts sampling</strong></h4><p name="0ac1" id="0ac1" class="graf graf--p graf-after--h4">The idea behind random walk with restarts sampling is quite simple. The algorithm takes random walks from a set of predefined start nodes. At each step of the walk, there is a probability that the current random walk stops and a new one starts from the set of start nodes. The user can define the start nodes. If no start nodes are defined, the algorithm chooses them uniformly at random.</p><p name="f60b" id="f60b" class="graf graf--p graf-after--p">I thought it would be interesting to show you an example of choosing a start node manually. So we will begin by executing the Weakly Connected Components algorithm to evaluate how connected the graph of articles is. A weakly connected component is a set of nodes within the graph where a path exists between all nodes in the set if the direction of relationships is ignored.<br>A weakly connected component can be considered an island that nodes from other components cannot reach.<br>While the algorithm identifies connected sets of nodes, its output can help you evaluate how disconnected the overall graph is.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="5912" id="5912" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">wcc = gds.wcc.stream(G)<br />wcc_grouped = (<br />    wcc.groupby(<span class="hljs-string">&quot;componentId&quot;</span>)<br />    .size()<br />    .to_frame(<span class="hljs-string">&quot;componentSize&quot;</span>)<br />    .reset_index()<br />    .sort_values(<span class="hljs-string">&quot;componentSize&quot;</span>, ascending=<span class="hljs-literal">False</span>)<br />    .reset_index()<br />)<br /><span class="hljs-built_in">print</span>(wcc_grouped)</span></pre><figure name="ed5b" id="ed5b" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*U8ljDQT0n_nWgdHVzlBG1w.png" data-width="298" data-height="380" src="https://cdn-images-1.medium.com/max/800/1*U8ljDQT0n_nWgdHVzlBG1w.png"></figure><p name="5c19" id="5c19" class="graf graf--p graf-after--figure">There is a total of 604 connected components in our graph. The largest component contains 98% of all nodes, while the other ones are smaller, with many containing only two nodes. If a component contains only two nodes, it means that we have a medium user list that has only two articles in it, and those two articles are not part of any other lists.</p><p name="c64b" id="c64b" class="graf graf--p graf-after--p">We executed the Weakly Connected Component algorithm to identify a node that belongs to a large connected component and, therefore, can be used as a starting node of the sampling algorithm. For example, if we used a node with only one neighbor, the sampling algorithm couldn’t perform longer walks to subsample the graph efficiently.</p><p name="7776" id="7776" class="graf graf--p graf-after--p">Fortunately, the sampling algorithm is implemented to automatically expand the set of start nodes if the random walks do not visit any new nodes. However, as we have used a start node from the largest connected component with 98% of all nodes, the algorithm won’t have to expand the set of start nodes automatically.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="d7c4" id="d7c4" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">largest_component = wcc_grouped[<span class="hljs-string">&quot;componentId&quot;</span>][<span class="hljs-number">0</span>]<br />start_node = wcc[wcc[<span class="hljs-string">&quot;componentId&quot;</span>] == largest_component][<span class="hljs-string">&quot;nodeId&quot;</span>][<span class="hljs-number">0</span>]<br /><br />trainG, metadata = gds.alpha.graph.sample.rwr(<br />    <span class="hljs-string">&quot;trainGraph&quot;</span>,<br />    G,<br />    samplingRatio=<span class="hljs-number">0.20</span>,<br />    startNodes=[<span class="hljs-built_in">int</span>(start_node)],<br />    nodeLabels=[<span class="hljs-string">&quot;Article&quot;</span>],<br />    relationshipTypes=[<span class="hljs-string">&quot;SIMILAR&quot;</span>],<br />)</span></pre><p name="b060" id="b060" class="graf graf--p graf-after--pre">The sampling ratio parameter defines the fraction of nodes in the original graph to be sampled. For example, when using the value 0.20 for the sampling ratio, the sampled subgraph will be 20% the size of the original graph. Additionally, we need to define that the random walks can only visit <strong class="markup--strong markup--p-strong">Article</strong> nodes through <strong class="markup--strong markup--p-strong">SIMILAR</strong> relationships by using the <strong class="markup--strong markup--p-strong">nodeLabels</strong> and <strong class="markup--strong markup--p-strong">relationshipTypes</strong> parameters.</p><h4 name="5471" id="5471" class="graf graf--h4 graf-after--p">GraphSAGE Training</h4><p name="a5dd" id="a5dd" class="graf graf--p graf-after--h4">Finally, we can go ahead and train the GraphSAGE model on the sampled subgraph.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="8636" id="8636" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.beta.graphSage.train(<br />    trainG,<br />    modelName=<span class="hljs-string">&quot;articleModel&quot;</span>,<br />    embeddingDimension=<span class="hljs-number">256</span>,<br />    sampleSizes=[<span class="hljs-number">10</span>, <span class="hljs-number">10</span>],<br />    searchDepth=<span class="hljs-number">15</span>,<br />    epochs=<span class="hljs-number">20</span>,<br />    learningRate=<span class="hljs-number">0.0001</span>,<br />    activationFunction=<span class="hljs-string">&quot;RELU&quot;</span>,<br />    aggregator=<span class="hljs-string">&quot;MEAN&quot;</span>,<br />    featureProperties=[<span class="hljs-string">&quot;openaiEmbedding&quot;</span>],<br />    batchSize=<span class="hljs-number">10</span>,<br />)</span></pre><p name="7d98" id="7d98" class="graf graf--p graf-after--pre">The GraphSAGE algorithm will use the <strong class="markup--strong markup--p-strong">openaiEmbedding</strong> node property as input features. The GraphSAGE embeddings will have a dimension of 256 (vector size). While I have played around with hyper-parameter optimization for this blog, I have noticed that the learning rate and activation function are the most impactful parameters.</p><h4 name="490d" id="490d" class="graf graf--h4 graf-after--p">Generate Embeddings</h4><p name="1edd" id="1edd" class="graf graf--p graf-after--h4">After the GraphSAGE model has been trained, we can use it to calculate the node embeddings for all the <strong class="markup--strong markup--p-strong">Article</strong> nodes in the original larger projected graph and consider only the <strong class="markup--strong markup--p-strong">SIMILAR</strong> relationships.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0b7c" id="0b7c" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.beta.graphSage.write(<br />    G,<br />    modelName=<span class="hljs-string">&quot;articleModel&quot;</span>,<br />    nodeLabels=[<span class="hljs-string">&quot;Article&quot;</span>],<br />    writeProperty=<span class="hljs-string">&quot;graphSAGE&quot;</span>,<br />    relationshipTypes=[<span class="hljs-string">&quot;SIMILAR&quot;</span>],<br />)</span></pre><p name="8c5a" id="8c5a" class="graf graf--p graf-after--pre">This time, we used the <strong class="markup--strong markup--p-strong">write</strong> mode to store the GraphSAGE embeddings as node properties in the database.</p><h3 name="0ee6" id="0ee6" class="graf graf--h3 graf-after--p">Classification model</h3><p name="6bc9" id="6bc9" class="graf graf--p graf-after--h3">We have prepared both the OpenAI and GraphSAGE embeddings. The only thing left is to train the models and compare their performance.</p><p name="15b7" id="15b7" class="graf graf--p graf-after--p">First, we will label the article tags we want to predict. I arbitrarily decided to only include tags that are present in at least 100 articles. The target tags will be labeled with a secondary <strong class="markup--strong markup--p-strong">Target</strong> label.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="5dc1" id="5dc1" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">gds.run_cypher(<br />    <span class="hljs-string">&quot;&quot;&quot;<br />MATCH (t:Tag)<br />WHERE count{(t)&lt;--()} &gt; 100<br />SET t:Target<br />RETURN count(*) AS count<br />&quot;&quot;&quot;</span><br />)</span></pre><p name="f73e" id="f73e" class="graf graf--p graf-after--pre">We have labeled 161 tags we want to predict. Remember, the word cloud visualization above took the same 161 tags and visualized them according to their frequencies.</p><p name="52e9" id="52e9" class="graf graf--p graf-after--p">As we will use the scikit-multilearn library, we need to export the relevant information from Neo4j.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="1a6e" id="1a6e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">data = gds.run_cypher(<br />    <span class="hljs-string">&quot;&quot;&quot;<br />MATCH (a:Article)-[:HAS_TAG]-&gt;(tag:Target)<br />RETURN a.url AS article,<br />        a.openaiEmbedding AS openai,<br />        a.graphSAGE AS graphSAGE,<br />        collect(tag.name) AS tags<br />&quot;&quot;&quot;</span><br />)</span></pre><p name="76a0" id="76a0" class="graf graf--p graf-after--pre">Next, we need to construct a binary matrix that indicates the presence of tags for a given article. Essentially, you can think of it as one-hot-encoding of tags per article. So, we can utilize the MultiLabelBinarizer procedure to achieve this.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="68e9" id="68e9" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">mlb = MultiLabelBinarizer()<br />tags_mlb = mlb.fit_transform(data[<span class="hljs-string">&quot;tags&quot;</span>])<br />data[<span class="hljs-string">&quot;target&quot;</span>] = <span class="hljs-built_in">list</span>(tags_mlb)</span></pre><p name="4a3c" id="4a3c" class="graf graf--p graf-after--pre">The scikit-multilearn library offers an improved dataset split for multi-label prediction tasks. However, it does not allow a deterministic approach with a random seed parameter. Therefore, we will perform the dataset split only once for both the word and GraphSAGE embeddings and then train the two models accordingly.</p><p name="0448" id="0448" class="graf graf--p graf-after--p">The following function takes in a data frame and the columns that should be separately used as input features to a multi-label classification model and returns the best-performing model while printing the weighted macro and weighted precisions. Here, we use the <a href="http://scikit.ml/api/skmultilearn.problem_transform.lp.html#:~:text=Label%20Powerset%20is%20a%20problem,found%20in%20the%20training%20data." data-href="http://scikit.ml/api/skmultilearn.problem_transform.lp.html#:~:text=Label%20Powerset%20is%20a%20problem,found%20in%20the%20training%20data." class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">LabelPowerset approach</a> to multi-label classification.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="3f25" id="3f25" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_and_evaluate</span>(<span class="hljs-params">df, input_columns</span>):<br />    max_weighted_precision = <span class="hljs-number">0</span><br />    best_input = <span class="hljs-string">&quot;&quot;</span><br />    <span class="hljs-comment"># Single split data</span><br />    X = data[input_columns].values<br />    y = np.array(data[<span class="hljs-string">&quot;target&quot;</span>].to_list())<br />    x_train_all, y_train, x_test_all, y_test = iterative_train_test_split(<br />        X, y, test_size=<span class="hljs-number">0.2</span><br />    )<br />    <span class="hljs-comment"># Train a model for each input option</span><br />    <span class="hljs-keyword">for</span> i, input_column <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(input_columns):<br />        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Training a model based on <span class="hljs-subst">{input_column}</span> column&quot;</span>)<br />        x_train = np.array([x[i] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> x_train_all])<br />        x_test = np.array([x[i] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> x_test_all])<br /><br />        <span class="hljs-comment"># train</span><br />        classifier = LabelPowerset(LogisticRegression())<br />        classifier.fit(x_train, y_train)<br />        <span class="hljs-comment"># predict</span><br />        predictions = classifier.predict(x_test)<br />        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Test accuracy is {}&quot;</span>.<span class="hljs-built_in">format</span>(accuracy_score(y_test, predictions)))<br />        <span class="hljs-built_in">print</span>(<br />            <span class="hljs-string">&quot;Macro Precision: {:.2f}&quot;</span>.<span class="hljs-built_in">format</span>(<br />                get_macro_precision(mlb.classes_, y_test, predictions)<br />            )<br />        )<br />        weighted_precision = get_weighted_precision(mlb.classes_, y_test, predictions)<br />        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Weighted Precision: {:.2f}&quot;</span>.<span class="hljs-built_in">format</span>(weighted_precision))<br />        <span class="hljs-keyword">if</span> weighted_precision &gt; max_weighted_precision:<br />            max_weighted_precision = weighted_precision<br />            best_classifier = classifier<br />            best_input = input_column<br /><br />    <span class="hljs-keyword">return</span> best_classifier, best_input</span></pre><p name="ffa5" id="ffa5" class="graf graf--p graf-after--pre">With everything prepared, we can go ahead and train the models based on word and graphSAGE embeddings and compare their performance.</p><p name="5eb2" id="5eb2" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">p.s. If you are using Google Colab, you might run into OOM problems using the openai embeddings</em></p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="029a" id="029a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">classifier, best_input = train_and_evaluate(data, [<span class="hljs-string">&quot;openai&quot;</span>, <span class="hljs-string">&quot;graphSAGE&quot;</span>])</span></pre><p name="7e23" id="7e23" class="graf graf--p graf-after--pre">The results are the following:</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="sql" name="7278" id="7278" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">Training a model based <span class="hljs-keyword">on</span> openai <span class="hljs-keyword">column</span><br />Test accuracy <span class="hljs-keyword">is</span> <span class="hljs-number">0.055443548387096774</span><br />Macro <span class="hljs-keyword">Precision</span>: <span class="hljs-number">0.20</span><br />Weighted <span class="hljs-keyword">Precision</span>: <span class="hljs-number">0.36</span><br />Training a model based <span class="hljs-keyword">on</span> graphSAGE <span class="hljs-keyword">column</span><br />Test accuracy <span class="hljs-keyword">is</span> <span class="hljs-number">0.05584677419354839</span><br />Macro <span class="hljs-keyword">Precision</span>: <span class="hljs-number">0.30</span><br />Weighted <span class="hljs-keyword">Precision</span>: <span class="hljs-number">0.41</span></span></pre><p name="71b5" id="71b5" class="graf graf--p graf-after--pre">Although the embeddings of the title and subtitle provide some information about their tags, they may not be the most efficient. This could be due to clickbait-style titles that prioritize grabbing attention over accurately describing the content. Furthermore, authors may have different preferences for tagging identical content with varying labels. Despite these challenges, our model predicts 161 labels, many of which have few examples, yielding acceptable results. To further improve accuracy, we can embed the entire article text and evaluate its performance.</p><p name="b77a" id="b77a" class="graf graf--p graf-after--p">Interestingly, using GraphSAGE embeddings enhances classification precision by considering the relationships between articles. Our model’s macro precision improves by ten percentage points, while the weighted precision improves by five. These outcomes demonstrate that GraphSAGE embeddings help identify infrequent tags more effectively. Unlike standard word embedding models, graph neural networks enable us to encode additional relationships between data points, thereby enhancing downstream machine learning models. We have also performed a dimensionality reduction from 1536 to 256 while increasing the performance, which is a great outcome.</p><h3 name="c747" id="c747" class="graf graf--h3 graf-after--p">Test Predictions</h3><p name="6372" id="6372" class="graf graf--p graf-after--h3">There are almost 50% of articles without any tags in our database. We can test the model on several and manually evaluate the results.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="python" name="6d3a" id="6d3a" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">example = gds.run_cypher(<br />    <span class="hljs-string">&quot;&quot;&quot;<br />MATCH (a:Article)<br />WHERE NOT EXISTS {(a)-[:HAS_TAG]-&gt;()}<br />RETURN a.title AS title,<br />       a.openaiEmbedding AS openai,<br />       a.graphSAGE AS graphSAGE<br />LIMIT 15<br />&quot;&quot;&quot;</span><br />)<br /><br />tags_predicted = classifier.predict(np.array(example[best_input].to_list()))<br />example[<span class="hljs-string">&quot;tags&quot;</span>] = [<span class="hljs-built_in">list</span>(mlb.inverse_transform(x)[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tags_predicted]<br />example[[<span class="hljs-string">&quot;title&quot;</span>, <span class="hljs-string">&quot;tags&quot;</span>]]</span></pre><p name="4be6" id="4be6" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Results</em></p><figure name="7a8e" id="7a8e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Iio-EECpO6X5iEaaD8ceBw.png" data-width="1006" data-height="454" src="https://cdn-images-1.medium.com/max/800/1*Iio-EECpO6X5iEaaD8ceBw.png"></figure><p name="7617" id="7617" class="graf graf--p graf-after--figure">Interestingly, the model mostly assigns one or two labels per article, when most real-world articles have five tags. This is probably one cause for the values of precision scores. Other than that, the results look promising judging by this small sample.</p><h3 name="a662" id="a662" class="graf graf--h3 graf-after--p">Summary</h3><p name="c8b3" id="c8b3" class="graf graf--p graf-after--h3">Traditional word embedding models like word2vec focus on encoding the co-occurrence statistics of words. However, they entirely ignore any other relationships that can be found between data points. For instance, we had users annotate similar articles by placing them in various reading lists. Luckily, graph neural networks offer a bridge between traditional word embeddings and graph embeddings as they allow us to build on top of word embeddings and encode additional information derived from relationships between data points. Therefore, the graph neural networks do not have to start from scratch but can be used to enhance state-of-the-art word or document embeddings.</p><p name="84e4" id="84e4" class="graf graf--p graf-after--p">To learn more about this topic, join me at NODES 2023, a free online global conference about graph technologies. The CFP is open now until June 30. <a href="https://dev.neo4j.com/nodes23" data-href="https://dev.neo4j.com/nodes23" class="markup--anchor markup--p-anchor" rel="noopener noreferrer noopener" target="_blank">https://dev.neo4j.com/nodes23</a></p><h4 name="e4ff" id="e4ff" class="graf graf--h4 graf-after--p">References</h4><p name="42ac" id="42ac" class="graf graf--p graf-after--h4 graf--trailing">[1] <a href="http://snap.stanford.edu/graphsage/" data-href="http://snap.stanford.edu/graphsage/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Hamilton, Will, Zhitao Ying, and Jure Leskovec. “Inductive representation learning on large graphs.” Advances in Neural Information Processing Systems. 2017.</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@bratanic-tomaz" class="p-author h-card">Tomaz Bratanic</a> on <a href="https://medium.com/p/c26d8e54fe4a"><time class="dt-published" datetime="2023-03-21T16:28:58.485Z">March 21, 2023</time></a>.</p><p><a href="https://medium.com/@bratanic-tomaz/enhancing-word-embedding-with-graph-neural-networks-c26d8e54fe4a" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on November 5, 2023.</p></footer></article></body></html>